{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61aa59d-49b0-41cb-bfa5-733997ae81f9",
   "metadata": {},
   "source": [
    "# Automatic Annotation of Environmental Sentences\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Background and Purpose\n",
    "\n",
    "This notebook applies a rule-based method to annotate environmental text with named entities. It is the third stage in a pipeline for building a domain-specific Named Entity Recognition (NER) dataset. Earlier stages involved sentence segmentation and vocabulary construction.\n",
    "\n",
    "The goal here is to automatically assign entity labels to pre-segmented sentences using curated vocabulary lists. Each vocabulary term is matched directly in text, and the matching span is annotated with its corresponding category. This produces a labelled dataset that can be used to train downstream NER models.\n",
    "\n",
    "The method prioritises speed, scale, and reproducibility. It requires no manual annotation or machine learning during this stage.\n",
    "\n",
    "### 1.2 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "- Apply each vocabulary category (TAXONOMY, HABITAT, ENV_PROCESS, POLLUTANT, MEASUREMENT) to a large sentence corpus\n",
    "- Construct an Aho-Corasick trie per category for efficient multi-pattern matching\n",
    "- Handle overlapping and nested matches through precedence and merging rules\n",
    "- Apply custom logic for MEASUREMENT entities, ensuring numbers and units are annotated together\n",
    "- Generate output in a SpaCy-compatible .jsonl format for downstream model training\n",
    "- Validate annotations using programmatic checks and spot-checking of random samples\n",
    "\n",
    "### 1.3 Challenges in Rule-Based Annotation\n",
    "\n",
    "Rule-based annotation offers simplicity and transparency but presents several challenges that must be addressed carefully:\n",
    "\n",
    "**Overlapping entities across categories**  \n",
    "Certain terms may appear in multiple categories or as part of longer expressions. For example, the word \"forest\" may be listed as a HABITAT but also occur in the species name \"African forest elephant\" (TAXONOMY). Overlapping matches must be resolved by choosing the longest span or prioritising specific categories.\n",
    "\n",
    "**Lack of context awareness**  \n",
    "Exact term matching does not account for semantic context. This can result in false positives where a term has different meanings (e.g. “Amazon” referring to a company vs. a rainforest).\n",
    "\n",
    "**Span boundaries and formatting**  \n",
    "Care must be taken to ensure that only the correct characters are included in each entity span. Matches must be aligned to full words and should not include punctuation, whitespace, or extraneous tokens.\n",
    "\n",
    "**Special treatment for measurement expressions**  \n",
    "Measurement entities such as “20 kg” or “<10 µg/L” require combined annotation of numbers and units. These must be identified and merged into single spans even when not contiguous in the vocabulary list.\n",
    "\n",
    "**Vocabulary coverage and noise**  \n",
    "Some terms may not be present in the vocabulary and will be missed. Others may be too generic and match unintended text. This stage must balance recall with precision and include mechanisms for iterative refinement of the vocab lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79eadc0-8c7b-4732-b8fa-10b82a855104",
   "metadata": {},
   "source": [
    "## 2. Method Overview\n",
    "\n",
    "### 2.1 What Is Aho-Corasick Matching?\n",
    "The Aho-Corasick algorithm is a fast and efficient method for matching many string patterns at once. It builds a data structure called a trie, which allows it to search for all vocabulary terms in a sentence in a single pass.\n",
    "\n",
    "This makes it well-suited for Named Entity Recognition tasks where the goal is to find known phrases (e.g. \"climate change\", \"acid rain\") across a large body of text. Unlike regular expressions or repeated substring searches, Aho-Corasick performs in linear time with respect to the input length.\n",
    "\n",
    "In this notebook, one Aho-Corasick matcher is created for each entity category (e.g. TAXONOMY, HABITAT). Each match includes the matched text, its character span, and its associated category label.\n",
    "\n",
    "### 2.2 Why Weak Labelling?\n",
    "Weak labelling refers to the automatic assignment of labels using predefined rules or resources, rather than manual annotation. In this case, the labels are derived from vocabulary lists matched against the text.\n",
    "\n",
    "This approach is commonly used when:\n",
    "- Manual annotation is too costly or time-consuming\n",
    "- Domain expertise is required to label data accurately\n",
    "- A large amount of unlabelled text is available\n",
    "\n",
    "Weak labelling allows researchers to generate useful training data without human effort, using only heuristics or dictionaries. It is particularly effective in structured domains like environmental science, where many terms are standardised.\n",
    "\n",
    "### 2.3 Benefits and Limitations\n",
    "#### Benefits:\n",
    "- Fast and scalable to large corpora\n",
    "- Easy to understand and reproduce\n",
    "- High recall for known vocabulary terms\n",
    "- Suitable for domains with well-defined terminology\n",
    "\n",
    "#### Limitations:\n",
    "- No understanding of context or meaning\n",
    "- Fails to detect entities not in the vocabulary\n",
    "- Can produce false positives (e.g. \"Amazon\" as rainforest vs company)\n",
    "- Requires careful span handling to avoid overlap or misalignment\n",
    "\n",
    "This method is not a replacement for human-labelled data, but it provides a strong starting point. The resulting annotations can be used to train statistical models or refine vocabularies through iteration and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238b0e0-248a-4714-b96d-985ce27f397c",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5879dba7-8207-4b7e-9b15-506c4ea30b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import ahocorasick\n",
    "\n",
    "SEGMENTED_PATH = Path(\"../data/segmented_text\")\n",
    "VOCAB_PATH = Path(\"../vocabs/final\")\n",
    "OUTPUT_PATH = Path(\"../data/json\")\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "351b2667-fced-43ca-968a-1658837c5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,748,655 sentences\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []\n",
    "\n",
    "for file_path in SEGMENTED_PATH.rglob(\"*.txt\"):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        all_sentences.extend(lines)\n",
    "\n",
    "print(f\"Loaded {len(all_sentences):,} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ec60420-6d41-4ee7-af4c-758b5e7e73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,250 terms for POLLUTANT\n",
      "Loaded 1,303 terms for ENV_PROCESS\n",
      "Loaded 1,170 terms for MEASUREMENT\n",
      "Loaded 574 terms for HABITAT\n",
      "Loaded 129,452 terms for TAXONOMY\n"
     ]
    }
   ],
   "source": [
    "matchers = {}\n",
    "\n",
    "for vocab_file in VOCAB_PATH.glob(\"*.txt\"):\n",
    "    label = vocab_file.stem.upper()\n",
    "\n",
    "    with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "        terms = [line.strip().lower() for line in f if line.strip()]\n",
    "    \n",
    "    automaton = ahocorasick.Automaton()\n",
    "    for term in terms:\n",
    "        automaton.add_word(term, (term, label))\n",
    "    automaton.make_automaton()\n",
    "\n",
    "    matchers[label] = automaton\n",
    "    print(f\"Loaded {len(terms):,} terms for {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bf060f9-60d4-44a6-85df-d27f5288204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Hyphenated-word boundary check --------\n",
    "def is_inside_hyphenated_word(text: str, start: int, end: int) -> bool:\n",
    "    \"\"\"True if the span touches a '-' that is part of a larger token.\"\"\"\n",
    "    return (start > 0 and text[start-1] == '-') or (end < len(text) and text[end] == '-')\n",
    "\n",
    "\n",
    "# -------- Whole-word Aho-Corasick matching with filtering --------\n",
    "def annotate_text_with_vocab(text: str, automaton, label: str):\n",
    "    \"\"\"\n",
    "    Return a list of [start, end, label] spans matched in *text*\n",
    "    using a case-insensitive automaton, respecting word boundaries.\n",
    "    \"\"\"\n",
    "    lowered = text.lower()\n",
    "    text_len = len(text)\n",
    "    spans = []\n",
    "\n",
    "    for end_idx, (term, _) in automaton.iter(lowered):\n",
    "        start_idx = end_idx - len(term) + 1\n",
    "\n",
    "        # Word-boundary guards\n",
    "        before_ok = start_idx == 0 or not text[start_idx-1].isalnum()\n",
    "        after_ok  = end_idx + 1 == text_len or not text[end_idx+1].isalnum()\n",
    "\n",
    "        if before_ok and after_ok and not is_inside_hyphenated_word(text, start_idx, end_idx+1):\n",
    "            spans.append([start_idx, end_idx+1, label])\n",
    "\n",
    "    # Sort by start-pos then (implicitly) by length (longer last → will be dropped on overlap)\n",
    "    spans.sort(key=lambda s: (s[0], s[1]-s[0]))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb46abf8-60e0-4c34-a679-96adbf02f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated 100,000/2,748,655 sentences\n",
      "Annotated 200,000/2,748,655 sentences\n",
      "Annotated 300,000/2,748,655 sentences\n",
      "Annotated 400,000/2,748,655 sentences\n",
      "Annotated 500,000/2,748,655 sentences\n",
      "Annotated 600,000/2,748,655 sentences\n",
      "Annotated 700,000/2,748,655 sentences\n",
      "Annotated 800,000/2,748,655 sentences\n",
      "Annotated 900,000/2,748,655 sentences\n",
      "Annotated 1,000,000/2,748,655 sentences\n",
      "Annotated 1,100,000/2,748,655 sentences\n",
      "Annotated 1,200,000/2,748,655 sentences\n",
      "Annotated 1,300,000/2,748,655 sentences\n",
      "Annotated 1,400,000/2,748,655 sentences\n",
      "Annotated 1,500,000/2,748,655 sentences\n",
      "Annotated 1,600,000/2,748,655 sentences\n",
      "Annotated 1,700,000/2,748,655 sentences\n",
      "Annotated 1,800,000/2,748,655 sentences\n",
      "Annotated 1,900,000/2,748,655 sentences\n",
      "Annotated 2,000,000/2,748,655 sentences\n",
      "Annotated 2,100,000/2,748,655 sentences\n",
      "Annotated 2,200,000/2,748,655 sentences\n",
      "Annotated 2,300,000/2,748,655 sentences\n",
      "Annotated 2,400,000/2,748,655 sentences\n",
      "Annotated 2,500,000/2,748,655 sentences\n",
      "Annotated 2,600,000/2,748,655 sentences\n",
      "Annotated 2,700,000/2,748,655 sentences\n",
      "Annotated 735,542 sentences with at least one entity\n"
     ]
    }
   ],
   "source": [
    "# -------- Rebuild raw annotations using strict matching --------\n",
    "raw_annotations = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(all_sentences):\n",
    "    for label, automaton in matchers.items():\n",
    "        matches = annotate_text_with_vocab(sentence, automaton, label)\n",
    "        if matches:\n",
    "            raw_annotations[sentence].extend(matches)\n",
    "\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(f\"Annotated {i + 1:,}/{len(all_sentences):,} sentences\")\n",
    "\n",
    "print(f\"Annotated {len(raw_annotations):,} sentences with at least one entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e056f-99c6-4962-8ef5-a7902fd691c6",
   "metadata": {},
   "source": [
    "So a total of 2,037,619 of them has at least one entities and the rest of 711026 were discarded. This is a good thing or bad thing because...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e765f02-a57e-4d93-9b99-c17278121183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences with overlapping entity spans:\n",
      "\n",
      "Sentence: During sediment collection samplers were removed from the metal uprights secured to the river bed and the contents emptied into 5-L containers.\n",
      "Spans: [[88, 93, 'HABITAT'], [88, 97, 'HABITAT']]\n",
      "\n",
      "Sentence: All samples were lightly hand ground, pressed and then measured under a He atmosphere under combined Pd and Co excitation radiation and using a high resolution, low spectral interference silicon drift detector.\n",
      "Spans: [[122, 131, 'POLLUTANT'], [122, 131, 'POLLUTANT']]\n",
      "\n",
      "Sentence: Results are the average of three repeats after elimination of outliers, a process that minimises intra-sample noise in the laser granulometry.\n",
      "Spans: [[110, 115, 'POLLUTANT'], [110, 115, 'POLLUTANT']]\n",
      "\n",
      "Sentence: Earth Surface Processes and Landforms 26: 1237 1248.\n",
      "Spans: [[28, 37, 'HABITAT'], [28, 37, 'HABITAT']]\n",
      "\n",
      "Sentence: Springer; 373 390 Folk RL, Ward WC. 1957.\n",
      "Spans: [[0, 8, 'TAXONOMY'], [0, 8, 'TAXONOMY']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans):\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(len(spans)-1):\n",
    "        if spans[i][1] > spans[i+1][0]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"\\nSentences with overlapping entity spans:\\n\")\n",
    "count = 0\n",
    "for sent, spans in raw_annotations.items():\n",
    "    if has_overlap(spans):\n",
    "        print(f\"Sentence: {sent}\")\n",
    "        print(f\"Spans: {spans}\\n\")\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "if count == 0:\n",
    "    print(\"No overlapping spans found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fd525-8003-41a2-9855-5ad3e9b864ec",
   "metadata": {},
   "source": [
    "When two entity spans overlap, only the longer one is kept. This is intentional.\n",
    "\n",
    "Longer spans often provide more precise meaning in context. For example:\n",
    "\n",
    "- We prefer “acidic rain” over “rain”\n",
    "- We keep “coastal salt marsh” instead of just “salt” or “marsh”\n",
    "\n",
    "Shorter spans are typically part of larger phrases and can lead to misleading annotations if extracted alone. By keeping the longest available match, we reduce ambiguity and improve the quality of weak labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4ef1c8c-a970-4e9f-a931-62cc62ccc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(spans):\n",
    "    \"\"\"\n",
    "    Keep longest span when overlaps occur (longest-match-wins).\n",
    "    Spans must be [start, end, label].\n",
    "    \"\"\"\n",
    "    spans = sorted(spans, key=lambda s: (s[0], -(s[1]-s[0])))\n",
    "    resolved, occupied = [], set()\n",
    "    for s, e, lbl in spans:\n",
    "        if not any(pos in occupied for pos in range(s, e)):\n",
    "            resolved.append([s, e, lbl])\n",
    "            occupied.update(range(s, e))\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffa05fc5-d899-480b-98a8-e66e9f564303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned annotations for 735,542 sentences\n"
     ]
    }
   ],
   "source": [
    "clean_annotations = {}\n",
    "\n",
    "for sent, spans in raw_annotations.items():\n",
    "    deduped = list({(s, e, l) for s, e, l in spans})  # remove exact duplicates\n",
    "    resolved = resolve_overlaps(deduped)\n",
    "    if resolved:\n",
    "        clean_annotations[sent] = resolved\n",
    "\n",
    "print(f\"Cleaned annotations for {len(clean_annotations):,} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ed92378-f8fe-4f01-948e-2f487cff8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with overlapping spans after resolution: 0\n"
     ]
    }
   ],
   "source": [
    "overlap_count = 0\n",
    "for spans in clean_annotations.values():\n",
    "    if has_overlap(spans):\n",
    "        overlap_count += 1\n",
    "\n",
    "print(f\"Sentences with overlapping spans after resolution: {overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b824478-610e-45e6-bf6b-2adcf84c16f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He has also spread [red squirrels|TAXONOMY] across the Highlands and established [goldeneye|TAXONOMY] [ducks|TAXONOMY] as a breeding species.\n",
      "\n",
      "In 2022, a unit of the CDC reported that out of 2,310 [urine|POLLUTANT] samples collected, more than 80% were laced with detectable traces of [glyphosate|POLLUTANT].\n",
      "\n",
      "Marine [protected areas|HABITAT] ([MPAs|MEASUREMENT]) require ecologically meaningful designs capable of taking into account the particularities of the species under consideration, the dynamic nature of the [marine environment|HABITAT], and the multiplicity of [anthropogenic|ENV_PROCESS] impacts.\n",
      "\n",
      "'War on plastic' could strand oil industry's £300bn investment | Major oil firms plan to grow plastic supply to counter impact of shift against fossil fuels | The war on plastic waste could scupper the oil industry’s multi-billion dollar bet that the world will continue to need more fossil fuels to help make the petrochemicals used in [plastics|POLLUTANT], according to a new report.\n",
      "\n",
      "Country diary: [nuthatches|TAXONOMY] continue their northerly advance | Backstone Bank [Wood|HABITAT], Weardale: The common sight of this once rare [bird|TAXONOMY] is a welcome reversal of the trend of decline |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def mark_entities(text, spans):\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    marked = \"\"\n",
    "    last = 0\n",
    "    for start, end, label in spans:\n",
    "        marked += text[last:start]\n",
    "        marked += f\"[{text[start:end]}|{label}]\"\n",
    "        last = end\n",
    "    marked += text[last:]\n",
    "    return marked\n",
    "\n",
    "samples = random.sample(list(clean_annotations.items()), 5)\n",
    "\n",
    "for sent, spans in samples:\n",
    "    print(mark_entities(sent, spans))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a312c199-46df-47c1-8c4e-fd960ebcc513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved → ../data/json/training_data.jsonl  (735,542 lines)\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = OUTPUT_PATH / \"training_data.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for sent, spans in clean_annotations.items():\n",
    "        json.dump({\"text\": sent, \"label\": spans}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"\\nSaved → {jsonl_path}  ({len(clean_annotations):,} lines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1bd3190-44cd-40ba-9cad-d5466141c603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts by category\n",
      "\n",
      "HABITAT         : 373,060\n",
      "ENV_PROCESS     : 365,599\n",
      "TAXONOMY        : 258,948\n",
      "MEASUREMENT     : 111,187\n",
      "POLLUTANT       : 96,938\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Count entities per category -----------------\n",
    "from collections import Counter\n",
    "\n",
    "label_counter = Counter()\n",
    "\n",
    "for spans in clean_annotations.values():\n",
    "    for _, _, lbl in spans:\n",
    "        label_counter[lbl] += 1\n",
    "\n",
    "print(\"Entity counts by category\\n\")\n",
    "for lbl, n in label_counter.most_common():\n",
    "    print(f\"{lbl:<15} : {n:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab92c335-4bd9-4f97-aa16-86d3b1359d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── HABITAT ──\n",
      "habitat                        32782\n",
      "forest                         25871\n",
      "ecosystem                      21801\n",
      "habitats                       21766\n",
      "river                          20572\n",
      "ecosystems                     14783\n",
      "landscape                      12460\n",
      "forests                        9274\n",
      "lake                           8154\n",
      "island                         8084\n",
      "rivers                         7568\n",
      "coast                          7220\n",
      "wood                           6070\n",
      "grassland                      5635\n",
      "islands                        4730\n",
      "landscapes                     4280\n",
      "wetland                        4225\n",
      "garden                         4171\n",
      "reef                           4166\n",
      "wetlands                       4118\n",
      "lakes                          3976\n",
      "bay                            3952\n",
      "beach                          3886\n",
      "valley                         3793\n",
      "national park                  3591\n",
      "mountain                       3502\n",
      "hill                           2882\n",
      "territory                      2742\n",
      "protected areas                2617\n",
      "grasslands                     2583\n",
      "\n",
      "── MEASUREMENT ──\n",
      "temperature                    17616\n",
      "temperatures                   10364\n",
      "ph                             5949\n",
      "km                             4684\n",
      "cm                             4422\n",
      "mg                             4380\n",
      "pa                             2868\n",
      "kg                             2702\n",
      "°c                             2453\n",
      "pm                             2020\n",
      "salinity                       2016\n",
      "ml                             1848\n",
      "mg/l                           1818\n",
      "min                            1813\n",
      "soil moisture                  1743\n",
      "pt                             1556\n",
      "ns                             1533\n",
      "humidity                       1453\n",
      "fl                             1427\n",
      "nm                             1215\n",
      "m2                             1185\n",
      "m3                             1168\n",
      "ng                             1073\n",
      "cl                             1019\n",
      "ev                             1019\n",
      "carbon footprint               965\n",
      "water level                    883\n",
      "nl                             799\n",
      "relative humidity              774\n",
      "evapotranspiration             765\n",
      "\n",
      "── POLLUTANT ──\n",
      "wastewater                     8350\n",
      "sewage                         4254\n",
      "noise                          4035\n",
      "pb                             3500\n",
      "methane                        3156\n",
      "carbon dioxide                 3098\n",
      "litter                         3077\n",
      "plastics                       3066\n",
      "pesticides                     2636\n",
      "heavy metals                   2609\n",
      "mercury                        2494\n",
      "smoke                          2398\n",
      "microplastics                  2367\n",
      "radiation                      2262\n",
      "phosphorus                     2000\n",
      "arsenic                        1591\n",
      "pesticide                      1445\n",
      "nitrate                        1321\n",
      "ozone                          1178\n",
      "ammonia                        1151\n",
      "antibiotics                    1108\n",
      "microplastic                   1035\n",
      "chromium                       1023\n",
      "pcbs                           966\n",
      "particulate matter             954\n",
      "manure                         928\n",
      "fertiliser                     879\n",
      "cadmium                        874\n",
      "antibiotic                     838\n",
      "zinc                           837\n",
      "\n",
      "── TAXONOMY ──\n",
      "fish                           18185\n",
      "birds                          11967\n",
      "bird                           7561\n",
      "bacteria                       4482\n",
      "cattle                         3164\n",
      "cod                            2930\n",
      "bees                           2239\n",
      "sheep                          2211\n",
      "mammals                        2208\n",
      "turkey                         2021\n",
      "dog                            1888\n",
      "whales                         1830\n",
      "shark                          1706\n",
      "butterfly                      1683\n",
      "salmon                         1682\n",
      "whale                          1550\n",
      "bee                            1511\n",
      "bear                           1504\n",
      "butterflies                    1459\n",
      "sharks                         1417\n",
      "cows                           1410\n",
      "pigs                           1383\n",
      "dogs                           1360\n",
      "deer                           1252\n",
      "bats                           1229\n",
      "rocky                          1134\n",
      "mammal                         1130\n",
      "ant                            1114\n",
      "wolves                         1092\n",
      "pig                            1057\n",
      "\n",
      "── ENV_PROCESS ──\n",
      "climate                        69779\n",
      "climate change                 33526\n",
      "drought                        27453\n",
      "emissions                      26285\n",
      "flood                          14253\n",
      "pollution                      13537\n",
      "weather                        10309\n",
      "season                         8100\n",
      "wind                           7724\n",
      "restoration                    7501\n",
      "air pollution                  5812\n",
      "rain                           5259\n",
      "degradation                    4881\n",
      "anthropogenic                  4623\n",
      "contamination                  4522\n",
      "deforestation                  3842\n",
      "effluent                       3834\n",
      "storm                          3766\n",
      "droughts                       3634\n",
      "greenhouse gas emissions       3506\n",
      "carbon emissions               3433\n",
      "snow                           3329\n",
      "global warming                 3138\n",
      "dam                            2959\n",
      "adsorption                     2836\n",
      "seasons                        2610\n",
      "storms                         2454\n",
      "emission                       2242\n",
      "deposition                     2106\n",
      "remote sensing                 2017\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Top 30 frequent entity texts per label (lowercased)\n",
    "for lbl in label_counter:\n",
    "    c = Counter(\n",
    "        text[start:end].lower()\n",
    "        for text, spans in clean_annotations.items()\n",
    "        for start, end, l in spans\n",
    "        if l == lbl\n",
    "    )\n",
    "    print(f\"\\n── {lbl} ──\")\n",
    "    for token, n in c.most_common(30):\n",
    "        print(f\"{token:<30} {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a731c-a2cd-4008-8489-893d63007873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba3496-20d6-4388-ba49-be5b85a1ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc21cb6-ad95-4241-97b8-b1b79b646d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d578e-5111-42e1-94cd-19a76d77ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd1621-49e2-4869-aaff-855dedc98eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518316c-f1f3-400d-9eee-c74d12b75fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461fd4e-d734-48a7-bb1c-4c69e4fcebe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab61b8-3235-4321-9a27-2525d995541d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12610a63-4529-4a43-b0ac-9abaae9896df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d7d44-73dc-4e3a-9946-746dab6ff2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617815f8-1cbf-43c7-a345-0abdd392ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6b0d1-c146-4485-8c8f-2eb23c8b99bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c9e7aa-69c2-484d-a277-f39810c290a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded 0 total sentences from ../data/sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.tokens import DocBin\n",
    "import ahocorasick\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"sentences\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"json\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the input text data\n",
    "preprocessed_texts = []\n",
    "for file in BASE_DIR.rglob(\"*.txt\"):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        preprocessed_texts.extend(lines)\n",
    "print(f\"📄 Loaded {len(preprocessed_texts):,} total sentences from {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762c957d-87ca-431d-9973-b299897a04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_spaces(text):\n",
    "    return re.sub(r'(?<=[a-zA-Z0-9])(?=[.?!])(?=[^\\s])', r'\\g<0> ', text)\n",
    "\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [\n",
    "        fix_missing_spaces(line.strip().lower())\n",
    "        for line in f if line.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f36e09-77a0-4cde-8563-9dc518cdd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_automaton(vocab_terms):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for term in vocab_terms:\n",
    "        A.add_word(term, term)\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "def is_inside_hyphenated_word(text, start, end):\n",
    "    # Check if the match is attached to another token via a hyphen\n",
    "    if start > 0 and text[start - 1] == '-':\n",
    "        return True\n",
    "    if end < len(text) and text[end] == '-':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def annotate_text_with_vocab(text, automaton, label):\n",
    "    text_length = len(text)\n",
    "    matches = []\n",
    "\n",
    "    # Iterate using the automaton\n",
    "    for end_index, term in automaton.iter(text):\n",
    "        start_index = end_index - len(term) + 1\n",
    "\n",
    "        # Whole word check\n",
    "        if (start_index == 0 or not text[start_index - 1].isalnum()) and (\n",
    "            end_index + 1 == text_length or not text[end_index + 1].isalnum()\n",
    "        ):\n",
    "            if not is_inside_hyphenated_word(text, start_index, end_index + 1):\n",
    "                matches.append([start_index, end_index + 1, label])\n",
    "\n",
    "    # Sort by start index, then longer spans first\n",
    "    matches.sort(key=lambda x: (x[0], x[1] - x[0]), reverse=False)\n",
    "    \n",
    "    # Don't block overlaps — just collect all clean, whole-word matches\n",
    "    annotations = [[start, end, label] for start, end, label in matches]\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce544cf6-8463-40bc-8060-595fec6162c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_files = [\n",
    "    \"measurement.txt\", \n",
    "    \"pollutant.txt\", \n",
    "    \"env_process.txt\", \n",
    "    \"habitat.txt\", \n",
    "    \"taxonomy.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bb4d33-16a5-4724-a604-e26bcb514854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10977cae-7209-444a-947e-a58c0c1d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Annotating MEASUREMENT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating POLLUTANT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating ENV_PROCESS\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating HABITAT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating TAXONOMY\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "✅ Done! Annotated 622,647 unique texts.\n",
      "📁 Saved to: ../data/json/training_data.jsonl\n",
      "⏱ Total time: 63.30 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "text_to_annotations = defaultdict(list)\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_name.upper()\n",
    "    print(f\"🔍 Annotating {label}\")\n",
    "\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    automaton = build_automaton(vocab_terms)\n",
    "\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        annotations = annotate_text_with_vocab(text, automaton, label)\n",
    "        if annotations:\n",
    "            text_to_annotations[text].extend(annotations)\n",
    "            text_to_annotations[text] = resolve_overlaps(text_to_annotations[text])\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Processed {i + 1:,}/{len(preprocessed_texts):,} texts...\")\n",
    "\n",
    "# Save final output\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, annotations in text_to_annotations.items():\n",
    "        json.dump({\"text\": text, \"label\": annotations}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ Done! Annotated {len(text_to_annotations):,} unique texts.\")\n",
    "print(f\"📁 Saved to: {annotated_path}\")\n",
    "print(f\"⏱ Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d597e0b9-38c3-4034-ba60-15b1b39187cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampled 1000 items to ../data/json/sample_for_manual_testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"training_data.jsonl\"\n",
    "OUTPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"sample_for_manual_testing.jsonl\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load safely, skip blank or bad lines\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"⚠️ Skipped malformed line\")\n",
    "\n",
    "# Sample\n",
    "sampled = random.sample(all_data, min(1000, len(all_data)))\n",
    "\n",
    "# Save sample\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Sampled {len(sampled)} items to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fae8-2e5f-4296-ba5b-2c17ef3cf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbec58a-70f6-40c3-ad62-b5564c3db6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8c12-6fa7-4dc8-983f-74db7550db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12450218-9566-41d7-b94a-f4f9aec934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932c0f9-7127-45f0-8468-015b23677b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a067e-c779-45cb-aee7-34c4660ca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Directory Setup ---\n",
    "BASE_DIR = Path(\"data\") / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"data\") / \"processed_data\"\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "# --- SpaCy Setup ---\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# --- Load Annotated Data ---\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# --- Save Cleaned Data ---\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned {len(valid_data)} valid samples.\")\n",
    "print(f\"Skipped {len(invalid_data)} invalid samples.\")\n",
    "print(f\"Saved cleaned annotations to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2395795-d5b6-4e1b-95cc-e7898a446e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 4961 terms down to 4554 unique ones.\n",
      "Saved to: ..\\vocabularies\\taxonomy_lemmatized.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15b779-938f-4870-a8eb-8ece8d98893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0094bf-9ed0-4798-8fac-4869b3cf4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
