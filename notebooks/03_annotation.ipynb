{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61aa59d-49b0-41cb-bfa5-733997ae81f9",
   "metadata": {},
   "source": [
    "# Automatic Annotation of Environmental Sentences\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook applies rule-based methods to annotate environmental text with named entity labels. Pre-segmented sentences from earlier stages are processed using exact string matching against curated vocabulary lists. Each match is labelled with one of five categories: TAXONOMY, HABITAT, ENV_PROCESS, POLLUTANT, or MEASUREMENT.\n",
    "\n",
    "The Aho-Corasick algorithm is used for efficient, case-insensitive pattern matching. Matched spans are formatted as training data for a SpaCy NER model. For MEASUREMENT entities, numeric values and their associated units are combined into single entity spans. This annotation stage provides high-recall training data for downstream machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c9e7aa-69c2-484d-a277-f39810c290a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Loaded 0 total sentences from ../data/sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.tokens import DocBin\n",
    "import ahocorasick\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"sentences\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"json\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the input text data\n",
    "preprocessed_texts = []\n",
    "for file in BASE_DIR.rglob(\"*.txt\"):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        preprocessed_texts.extend(lines)\n",
    "print(f\"ðŸ“„ Loaded {len(preprocessed_texts):,} total sentences from {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762c957d-87ca-431d-9973-b299897a04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_spaces(text):\n",
    "    return re.sub(r'(?<=[a-zA-Z0-9])(?=[.?!])(?=[^\\s])', r'\\g<0> ', text)\n",
    "\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [\n",
    "        fix_missing_spaces(line.strip().lower())\n",
    "        for line in f if line.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f36e09-77a0-4cde-8563-9dc518cdd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_automaton(vocab_terms):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for term in vocab_terms:\n",
    "        A.add_word(term, term)\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "def is_inside_hyphenated_word(text, start, end):\n",
    "    # Check if the match is attached to another token via a hyphen\n",
    "    if start > 0 and text[start - 1] == '-':\n",
    "        return True\n",
    "    if end < len(text) and text[end] == '-':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def annotate_text_with_vocab(text, automaton, label):\n",
    "    text_length = len(text)\n",
    "    matches = []\n",
    "\n",
    "    # Iterate using the automaton\n",
    "    for end_index, term in automaton.iter(text):\n",
    "        start_index = end_index - len(term) + 1\n",
    "\n",
    "        # Whole word check\n",
    "        if (start_index == 0 or not text[start_index - 1].isalnum()) and (\n",
    "            end_index + 1 == text_length or not text[end_index + 1].isalnum()\n",
    "        ):\n",
    "            if not is_inside_hyphenated_word(text, start_index, end_index + 1):\n",
    "                matches.append([start_index, end_index + 1, label])\n",
    "\n",
    "    # Sort by start index, then longer spans first\n",
    "    matches.sort(key=lambda x: (x[0], x[1] - x[0]), reverse=False)\n",
    "    \n",
    "    # Don't block overlaps â€” just collect all clean, whole-word matches\n",
    "    annotations = [[start, end, label] for start, end, label in matches]\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3606d0f6-c26d-44e0-bfa3-3ed860fe7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Expanded 196 terms to 312 (with plurals added).\n",
      "ðŸ“ Saved to: ..\\vocabularies\\meas_with_plurals.txt\n"
     ]
    }
   ],
   "source": [
    "# import inflect\n",
    "# from pathlib import Path\n",
    "\n",
    "# p = inflect.engine()\n",
    "\n",
    "# # Load vocab file\n",
    "# VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "# input_path = VOCAB_DIR / \"pollutant.txt\"\n",
    "# output_path = VOCAB_DIR / \"pollutant_with_plurals.txt\"\n",
    "\n",
    "# # Read and pluralise\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     original_terms = {line.strip().lower() for line in f if line.strip()}\n",
    "\n",
    "# expanded_terms = set()\n",
    "\n",
    "# for term in original_terms:\n",
    "#     expanded_terms.add(term)\n",
    "#     # Try to pluralise single words\n",
    "#     if len(term.split()) == 1:\n",
    "#         plural = p.plural(term)\n",
    "#         if plural and plural != term:\n",
    "#             expanded_terms.add(plural)\n",
    "\n",
    "# # Write expanded vocab\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for term in sorted(expanded_terms):\n",
    "#         f.write(term + \"\\n\")\n",
    "\n",
    "# print(f\"âœ… Expanded {len(original_terms)} terms to {len(expanded_terms)} (with plurals added).\")\n",
    "# print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce544cf6-8463-40bc-8060-595fec6162c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_files = [\n",
    "    \"measurement.txt\", \n",
    "    \"pollutant.txt\", \n",
    "    \"env_process.txt\", \n",
    "    \"habitat.txt\", \n",
    "    \"taxonomy.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bb4d33-16a5-4724-a604-e26bcb514854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10977cae-7209-444a-947e-a58c0c1d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Annotating MEASUREMENT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "ðŸ” Annotating POLLUTANT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "ðŸ” Annotating ENV_PROCESS\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "ðŸ” Annotating HABITAT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "ðŸ” Annotating TAXONOMY\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "âœ… Done! Annotated 622,647 unique texts.\n",
      "ðŸ“ Saved to: ../data/json/training_data.jsonl\n",
      "â± Total time: 63.30 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "text_to_annotations = defaultdict(list)\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_name.upper()\n",
    "    print(f\"ðŸ” Annotating {label}\")\n",
    "\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    automaton = build_automaton(vocab_terms)\n",
    "\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        annotations = annotate_text_with_vocab(text, automaton, label)\n",
    "        if annotations:\n",
    "            text_to_annotations[text].extend(annotations)\n",
    "            text_to_annotations[text] = resolve_overlaps(text_to_annotations[text])\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Processed {i + 1:,}/{len(preprocessed_texts):,} texts...\")\n",
    "\n",
    "# Save final output\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, annotations in text_to_annotations.items():\n",
    "        json.dump({\"text\": text, \"label\": annotations}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Done! Annotated {len(text_to_annotations):,} unique texts.\")\n",
    "print(f\"ðŸ“ Saved to: {annotated_path}\")\n",
    "print(f\"â± Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d597e0b9-38c3-4034-ba60-15b1b39187cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampled 1000 items to ../data/json/sample_for_manual_testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"training_data.jsonl\"\n",
    "OUTPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"sample_for_manual_testing.jsonl\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load safely, skip blank or bad lines\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš ï¸ Skipped malformed line\")\n",
    "\n",
    "# Sample\n",
    "sampled = random.sample(all_data, min(1000, len(all_data)))\n",
    "\n",
    "# Save sample\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… Sampled {len(sampled)} items to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fae8-2e5f-4296-ba5b-2c17ef3cf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbec58a-70f6-40c3-ad62-b5564c3db6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8c12-6fa7-4dc8-983f-74db7550db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12450218-9566-41d7-b94a-f4f9aec934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932c0f9-7127-45f0-8468-015b23677b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a067e-c779-45cb-aee7-34c4660ca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Directory Setup ---\n",
    "BASE_DIR = Path(\"data\") / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"data\") / \"processed_data\"\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "# --- SpaCy Setup ---\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# --- Load Annotated Data ---\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# --- Save Cleaned Data ---\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned {len(valid_data)} valid samples.\")\n",
    "print(f\"Skipped {len(invalid_data)} invalid samples.\")\n",
    "print(f\"Saved cleaned annotations to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2395795-d5b6-4e1b-95cc-e7898a446e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 4961 terms down to 4554 unique ones.\n",
      "Saved to: ..\\vocabularies\\taxonomy_lemmatized.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15b779-938f-4870-a8eb-8ece8d98893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0094bf-9ed0-4798-8fac-4869b3cf4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
