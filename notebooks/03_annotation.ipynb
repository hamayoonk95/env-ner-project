{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61aa59d-49b0-41cb-bfa5-733997ae81f9",
   "metadata": {},
   "source": [
    "# Automatic Annotation of Environmental Sentences\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Background and Purpose\n",
    "\n",
    "This notebook applies a rule-based method to annotate environmental text with named entities. It is the third stage in a pipeline for building a domain-specific Named Entity Recognition (NER) dataset. Earlier stages involved sentence segmentation and vocabulary construction.\n",
    "\n",
    "The goal here is to automatically assign entity labels to pre-segmented sentences using curated vocabulary lists. Each vocabulary term is matched directly in text, and the matching span is annotated with its corresponding category. This produces a labelled dataset that can be used to train NER models.\n",
    "\n",
    "The method prioritises speed, scale, and reproducibility. It requires no manual annotation or machine learning during this stage.\n",
    "\n",
    "### 1.2 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "- Apply each vocabulary category (TAXONOMY, HABITAT, ENV_PROCESS, POLLUTANT, MEASUREMENT) to a large sentence corpus\n",
    "- Construct an Aho-Corasick trie per category for efficient multi-pattern matching\n",
    "- Handle overlapping and nested matches through precedence and merging rules\n",
    "- Apply custom logic for MEASUREMENT entities, ensuring numbers and units are annotated together\n",
    "- Generate output in a SpaCy-compatible .jsonl format for model training\n",
    "- Validate annotations using programmatic checks and spot-checking of random samples\n",
    "\n",
    "### 1.3 Challenges in Rule-Based Annotation\n",
    "\n",
    "Rule-based annotation offers simplicity and transparency but presents several challenges that must be addressed carefully:\n",
    "\n",
    "**Overlapping entities across categories**  \n",
    "Certain terms may appear in multiple categories or as part of longer expressions. For example, the word \"forest\" may be listed as a HABITAT but also occur in the species name \"African forest elephant\" (TAXONOMY). Overlapping matches must be resolved by choosing the longest span or prioritising specific categories.\n",
    "\n",
    "**Lack of context awareness**  \n",
    "Exact term matching does not account for semantic context. This can result in false positives where a term has different meanings (e.g. “Amazon” referring to a company vs. a rainforest).\n",
    "\n",
    "**Span boundaries and formatting**  \n",
    "Care must be taken to ensure that only the correct characters are included in each entity span. Matches must be aligned to full words and should not include punctuation, whitespace, or extraneous tokens.\n",
    "\n",
    "**Special treatment for measurement expressions**  \n",
    "Measurement entities such as “20 kg” or “<10 µg/L” require combined annotation of numbers and units. These must be identified and merged into single spans even when not contiguous in the vocabulary list.\n",
    "\n",
    "**Vocabulary coverage and noise**  \n",
    "Some terms may not be present in the vocabulary and will be missed. Others may be too generic and match unintended text. This stage must balance recall with precision and include mechanisms for iterative refinement of the vocab lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79eadc0-8c7b-4732-b8fa-10b82a855104",
   "metadata": {},
   "source": [
    "## 2. Method Overview\n",
    "\n",
    "### 2.1 What Is Aho-Corasick Matching?\n",
    "The Aho-Corasick algorithm is a fast and efficient method for matching many string patterns at once. It builds a data structure called a trie, which allows it to search for all vocabulary terms in a sentence in a single pass.\n",
    "\n",
    "This makes it well-suited for Named Entity Recognition tasks where the goal is to find known phrases (e.g. \"climate change\", \"acid rain\") across a large body of text. Unlike regular expressions or repeated substring searches, Aho-Corasick performs in linear time with respect to the input length.\n",
    "\n",
    "In this notebook, one Aho-Corasick matcher is created for each entity category (e.g. TAXONOMY, HABITAT). Each match includes the matched text, its character span, and its associated category label.\n",
    "\n",
    "### 2.2 Why Weak Labelling?\n",
    "Weak labelling refers to the automatic assignment of labels using predefined rules or resources, rather than manual annotation. In this case, the labels are derived from vocabulary lists matched against the text.\n",
    "\n",
    "This approach is commonly used when:\n",
    "- Manual annotation is too costly or time-consuming\n",
    "- Domain expertise is required to label data accurately\n",
    "- A large amount of unlabelled text is available\n",
    "\n",
    "Weak labelling allows researchers to generate useful training data without human effort, using only heuristics or dictionaries. It is particularly effective in structured domains like environmental science, where many terms are standardised.\n",
    "\n",
    "### 2.3 Benefits and Limitations\n",
    "#### Benefits:\n",
    "- Fast and scalable to large corpora\n",
    "- Easy to understand and reproduce\n",
    "- High recall for known vocabulary terms\n",
    "- Suitable for domains with well-defined terminology\n",
    "\n",
    "#### Limitations:\n",
    "- No understanding of context or meaning\n",
    "- Fails to detect entities not in the vocabulary\n",
    "- Can produce false positives (e.g. \"Amazon\" as rainforest vs company)\n",
    "- Requires careful span handling to avoid overlap or misalignment\n",
    "\n",
    "This method is not a replacement for human-labelled data, but it provides a strong starting point. The resulting annotations can be used to train statistical models or refine vocabularies through iteration and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238b0e0-248a-4714-b96d-985ce27f397c",
   "metadata": {},
   "source": [
    "## 3. Annotation Process\n",
    "In this section, the annotation process is initialised using the Aho-Corasick algorithm for each entity category. A case-insensitive matcher is built for every vocabulary, and core matching functionality is defined to identify valid entity spans within each sentence. This enables efficient detection of known terms while filtering out partial or incorrect matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5879dba7-8207-4b7e-9b15-506c4ea30b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import ahocorasick\n",
    "\n",
    "SEGMENTED_PATH = Path(\"../data/segmented_text\")\n",
    "VOCAB_PATH = Path(\"../vocabs/final\")\n",
    "OUTPUT_PATH = Path(\"../data/json\")\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425ff9b-f059-4c8f-9e1b-f6fd044cdf51",
   "metadata": {},
   "source": [
    "### 3.1 Load all sentences\n",
    "All pre-segmented sentences are loaded from the corpus folder. This creates a single list containing every sentence to be annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "351b2667-fced-43ca-968a-1658837c5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,748,655 sentences\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []\n",
    "\n",
    "for file_path in SEGMENTED_PATH.rglob(\"*.txt\"):\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        all_sentences.extend(lines)\n",
    "\n",
    "print(f\"Loaded {len(all_sentences):,} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813e1b3-f92f-4d79-a233-cec1531388d8",
   "metadata": {},
   "source": [
    "The total number of sentences is 2,748,655. Only those containing at least one matched entity will be retained in the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662740a3-57d4-4cb8-baeb-e88d5dbe6de4",
   "metadata": {},
   "source": [
    "### 3.2 Load vocabulary and build automata\n",
    "Each vocabulary file is read and converted into an Aho-Corasick automaton for fast multi-pattern matching. One matcher is created per entity category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ec60420-6d41-4ee7-af4c-758b5e7e73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,250 terms for POLLUTANT\n",
      "Loaded 1,303 terms for ENV_PROCESS\n",
      "Loaded 1,170 terms for MEASUREMENT\n",
      "Loaded 574 terms for HABITAT\n",
      "Loaded 129,452 terms for TAXONOMY\n"
     ]
    }
   ],
   "source": [
    "matchers = {}\n",
    "\n",
    "for vocab_file in VOCAB_PATH.glob(\"*.txt\"):\n",
    "    label = vocab_file.stem.upper()\n",
    "\n",
    "    with open(vocab_file, encoding=\"utf-8\") as f:\n",
    "        terms = [line.strip().lower() for line in f if line.strip()]\n",
    "    \n",
    "    automaton = ahocorasick.Automaton()\n",
    "    for term in terms:\n",
    "        automaton.add_word(term, (term, label))\n",
    "    automaton.make_automaton()\n",
    "\n",
    "    matchers[label] = automaton\n",
    "    print(f\"Loaded {len(terms):,} terms for {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53b1b1-50e0-4435-b0a3-b9b756b21c92",
   "metadata": {},
   "source": [
    "This step loads over 130,000 taxonomy terms and around 500–1,300 terms for each of the other four categories. These matchers will be used to scan each sentence for known entity terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cefb02-9df0-4037-913e-56a9d6a3c875",
   "metadata": {},
   "source": [
    "### 3.3 Define span-matching logic\n",
    "This step defines the core function used to detect vocabulary matches within a sentence. Matches are returned as character spans only if they respect full-word boundaries and are not part of hyphenated words. This helps avoid partial or incorrect matches (e.g. skipping \"rain\" in \"acid-rainfall\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bf060f9-60d4-44a6-85df-d27f5288204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inside_hyphenated_word(text: str, start: int, end: int) -> bool:\n",
    "    \"\"\"True if the span touches a hyphen that is part of a larger word\"\"\"\n",
    "    return (start > 0 and text[start-1] == '-') or (end < len(text) and text[end] == '-')\n",
    "\n",
    "\n",
    "def annotate_text_with_vocab(text: str, automaton, label: str):\n",
    "    \"\"\"Return a list of [start, end, label] spans matched in input text\"\"\"\n",
    "    lowered = text.lower()\n",
    "    text_len = len(text)\n",
    "    spans = []\n",
    "\n",
    "    for end_idx, (term, _) in automaton.iter(lowered):\n",
    "        start_idx = end_idx - len(term) + 1\n",
    "\n",
    "        before_ok = start_idx == 0 or not text[start_idx-1].isalnum()\n",
    "        after_ok  = end_idx + 1 == text_len or not text[end_idx+1].isalnum()\n",
    "\n",
    "        if before_ok and after_ok and not is_inside_hyphenated_word(text, start_idx, end_idx+1):\n",
    "            spans.append([start_idx, end_idx+1, label])\n",
    "\n",
    "    spans.sort(key=lambda s: (s[0], s[1]-s[0]))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b7d5d-74ff-4892-9a1c-536290cff006",
   "metadata": {},
   "source": [
    "### 3.4 Annotating the data\n",
    "Each sentence is passed through all five Aho-Corasick matchers. Matching spans are stored per sentence if they meet the boundary rules defined earlier. Sentences without any entity match are ignored at this stage.\n",
    "\n",
    "This approach is fast and scalable. However, it produces weak labels since matches are based only on exact vocabulary terms, without any use of surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb46abf8-60e0-4c34-a679-96adbf02f488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated 100,000/2,748,655 sentences\n",
      "Annotated 200,000/2,748,655 sentences\n",
      "Annotated 300,000/2,748,655 sentences\n",
      "Annotated 400,000/2,748,655 sentences\n",
      "Annotated 500,000/2,748,655 sentences\n",
      "Annotated 600,000/2,748,655 sentences\n",
      "Annotated 700,000/2,748,655 sentences\n",
      "Annotated 800,000/2,748,655 sentences\n",
      "Annotated 900,000/2,748,655 sentences\n",
      "Annotated 1,000,000/2,748,655 sentences\n",
      "Annotated 1,100,000/2,748,655 sentences\n",
      "Annotated 1,200,000/2,748,655 sentences\n",
      "Annotated 1,300,000/2,748,655 sentences\n",
      "Annotated 1,400,000/2,748,655 sentences\n",
      "Annotated 1,500,000/2,748,655 sentences\n",
      "Annotated 1,600,000/2,748,655 sentences\n",
      "Annotated 1,700,000/2,748,655 sentences\n",
      "Annotated 1,800,000/2,748,655 sentences\n",
      "Annotated 1,900,000/2,748,655 sentences\n",
      "Annotated 2,000,000/2,748,655 sentences\n",
      "Annotated 2,100,000/2,748,655 sentences\n",
      "Annotated 2,200,000/2,748,655 sentences\n",
      "Annotated 2,300,000/2,748,655 sentences\n",
      "Annotated 2,400,000/2,748,655 sentences\n",
      "Annotated 2,500,000/2,748,655 sentences\n",
      "Annotated 2,600,000/2,748,655 sentences\n",
      "Annotated 2,700,000/2,748,655 sentences\n",
      "Annotated 735,542 sentences with at least one entity\n"
     ]
    }
   ],
   "source": [
    "raw_annotations = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(all_sentences):\n",
    "    for label, automaton in matchers.items():\n",
    "        matches = annotate_text_with_vocab(sentence, automaton, label)\n",
    "        if matches:\n",
    "            raw_annotations[sentence].extend(matches)\n",
    "\n",
    "    if (i + 1) % 100000 == 0:\n",
    "        print(f\"Annotated {i + 1:,}/{len(all_sentences):,} sentences\")\n",
    "\n",
    "print(f\"Annotated {len(raw_annotations):,} sentences with at least one entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e056f-99c6-4962-8ef5-a7902fd691c6",
   "metadata": {},
   "source": [
    "This step annotated 735,542 out of 2,748,655 sentences, meaning around 2 million sentences were excluded due to no matches.\n",
    "\n",
    "This is a normal outcome in weak labelling. Removing unmatched sentences improves overall label precision by avoiding empty or noisy examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f1c8c-9687-42fe-b882-0b43d6ff480f",
   "metadata": {},
   "source": [
    "### 3.5 removing overlaping spans\n",
    "Some sentences contain overlapping entity matches. These occur when multiple vocabulary terms match substrings that share characters. For example, both “salt” and “salt marsh” may appear in the same span.\n",
    "\n",
    "In such cases, only the longest span is retained. This helps preserve contextual clarity and reduces noise from nested or partial terms.\n",
    "\n",
    "The following code checks the annotated data for overlaps and prints a few examples for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e765f02-a57e-4d93-9b99-c17278121183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences with overlapping entity spans:\n",
      "\n",
      "Sentence: During sediment collection samplers were removed from the metal uprights secured to the river bed and the contents emptied into 5-L containers.\n",
      "Spans: [[88, 93, 'HABITAT'], [88, 97, 'HABITAT']]\n",
      "\n",
      "Sentence: All samples were lightly hand ground, pressed and then measured under a He atmosphere under combined Pd and Co excitation radiation and using a high resolution, low spectral interference silicon drift detector.\n",
      "Spans: [[122, 131, 'POLLUTANT'], [122, 131, 'POLLUTANT']]\n",
      "\n",
      "Sentence: Results are the average of three repeats after elimination of outliers, a process that minimises intra-sample noise in the laser granulometry.\n",
      "Spans: [[110, 115, 'POLLUTANT'], [110, 115, 'POLLUTANT']]\n",
      "\n",
      "Sentence: Earth Surface Processes and Landforms 26: 1237 1248.\n",
      "Spans: [[28, 37, 'HABITAT'], [28, 37, 'HABITAT']]\n",
      "\n",
      "Sentence: Springer; 373 390 Folk RL, Ward WC. 1957.\n",
      "Spans: [[0, 8, 'TAXONOMY'], [0, 8, 'TAXONOMY']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def has_overlap(spans):\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    for i in range(len(spans)-1):\n",
    "        if spans[i][1] > spans[i+1][0]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"\\nSentences with overlapping entity spans:\\n\")\n",
    "count = 0\n",
    "for sent, spans in raw_annotations.items():\n",
    "    if has_overlap(spans):\n",
    "        print(f\"Sentence: {sent}\")\n",
    "        print(f\"Spans: {spans}\\n\")\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "\n",
    "if count == 0:\n",
    "    print(\"No overlapping spans found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fd525-8003-41a2-9855-5ad3e9b864ec",
   "metadata": {},
   "source": [
    "To ensure annotation consistency, overlapping spans must be removed. When two or more entity matches share characters, the longest match is retained. This reduces ambiguity and helps maintain clean training data.\n",
    "\n",
    "The function below implements a “longest-match-wins” strategy by sorting spans by start position and length, and then keeping only non-overlapping ones. Exact duplicate spans are also removed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4ef1c8c-a970-4e9f-a931-62cc62ccc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(spans):\n",
    "    \"\"\"Keep longest span when overlaps occur\"\"\"\n",
    "    spans = sorted(spans, key=lambda s: (s[0], -(s[1]-s[0])))\n",
    "    resolved, occupied = [], set()\n",
    "    for s, e, lbl in spans:\n",
    "        if not any(pos in occupied for pos in range(s, e)):\n",
    "            resolved.append([s, e, lbl])\n",
    "            occupied.update(range(s, e))\n",
    "    return resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffa05fc5-d899-480b-98a8-e66e9f564303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned annotations for 735,542 sentences\n"
     ]
    }
   ],
   "source": [
    "clean_annotations = {}\n",
    "\n",
    "for sent, spans in raw_annotations.items():\n",
    "    deduped = list({(s, e, l) for s, e, l in spans})  # remove exact duplicates\n",
    "    resolved = resolve_overlaps(deduped)\n",
    "    if resolved:\n",
    "        clean_annotations[sent] = resolved\n",
    "\n",
    "print(f\"Cleaned annotations for {len(clean_annotations):,} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ed92378-f8fe-4f01-948e-2f487cff8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with overlapping spans after resolution: 0\n"
     ]
    }
   ],
   "source": [
    "overlap_count = 0\n",
    "for spans in clean_annotations.values():\n",
    "    if has_overlap(spans):\n",
    "        overlap_count += 1\n",
    "\n",
    "print(f\"Sentences with overlapping spans after resolution: {overlap_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec88948-b5c2-4c3a-ad0c-bc542144e097",
   "metadata": {},
   "source": [
    "The overlap resolution process cleaned the annotations for 735,542 sentences. A final validation shows that 0 sentences contain overlapping spans, confirming the success of the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e836049-1fc6-43a0-83b3-b2ffd64c73bf",
   "metadata": {},
   "source": [
    "### 3.6 Spot-checking samples\n",
    "To qualitatively inspect the annotation quality, a small random sample of annotated sentences is printed below. Entities are marked inline in the format [text|LABEL] for readability.\n",
    "\n",
    "This step helps verify that entity boundaries are accurate and that labels align with expectations. It also highlights whether multi-word terms are correctly captured and whether irrelevant matches are slipping through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b824478-610e-45e6-bf6b-2adcf84c16f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He has also spread [red squirrels|TAXONOMY] across the Highlands and established [goldeneye|TAXONOMY] [ducks|TAXONOMY] as a breeding species.\n",
      "\n",
      "In 2022, a unit of the CDC reported that out of 2,310 [urine|POLLUTANT] samples collected, more than 80% were laced with detectable traces of [glyphosate|POLLUTANT].\n",
      "\n",
      "Marine [protected areas|HABITAT] ([MPAs|MEASUREMENT]) require ecologically meaningful designs capable of taking into account the particularities of the species under consideration, the dynamic nature of the [marine environment|HABITAT], and the multiplicity of [anthropogenic|ENV_PROCESS] impacts.\n",
      "\n",
      "'War on plastic' could strand oil industry's £300bn investment | Major oil firms plan to grow plastic supply to counter impact of shift against fossil fuels | The war on plastic waste could scupper the oil industry’s multi-billion dollar bet that the world will continue to need more fossil fuels to help make the petrochemicals used in [plastics|POLLUTANT], according to a new report.\n",
      "\n",
      "Country diary: [nuthatches|TAXONOMY] continue their northerly advance | Backstone Bank [Wood|HABITAT], Weardale: The common sight of this once rare [bird|TAXONOMY] is a welcome reversal of the trend of decline |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def mark_entities(text, spans):\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    marked = \"\"\n",
    "    last = 0\n",
    "    for start, end, label in spans:\n",
    "        marked += text[last:start]\n",
    "        marked += f\"[{text[start:end]}|{label}]\"\n",
    "        last = end\n",
    "    marked += text[last:]\n",
    "    return marked\n",
    "\n",
    "samples = random.sample(list(clean_annotations.items()), 5)\n",
    "\n",
    "for sent, spans in samples:\n",
    "    print(mark_entities(sent, spans))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a6bf9-0c25-44ae-b1d8-6fe70361fb5d",
   "metadata": {},
   "source": [
    "The printed examples demonstrate that the annotation logic performs well:\n",
    "\n",
    "- Multi-word entities such as \"protected areas\", \"marine environment\", and \"red squirrels\" are correctly labelled.\n",
    "- Category assignments appear contextually appropriate. For instance, \"glyphosate\" is labelled as POLLUTANT, while \"nuthatches\" and \"goldeneye ducks\" are labelled as TAXONOMY.\n",
    "- Nested and adjacent entities are cleanly separated, with no sign of overlap or misaligned spans.\n",
    "\n",
    "Overall, the spot checks indicate that the annotation pipeline is producing high-quality weak labels suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb07f5d-b239-45a6-bd7e-2f48c027ad46",
   "metadata": {},
   "source": [
    "### 3.7 Save annotated data to JSONL\n",
    "The final cleaned annotations are written to a .jsonl file in SpaCy-compatible format. Each line contains a dictionary with the sentence text and a list of entity spans.\n",
    "\n",
    "This format will be used for training a NER model in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a312c199-46df-47c1-8c4e-fd960ebcc513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved → ../data/json/training_data.jsonl  (735,542 lines)\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = OUTPUT_PATH / \"training_data.jsonl\"\n",
    "with jsonl_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for sent, spans in clean_annotations.items():\n",
    "        json.dump({\"text\": sent, \"label\": spans}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"\\nSaved → {jsonl_path}  ({len(clean_annotations):,} lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f291255-3a12-43f8-95c9-9a7d0f575472",
   "metadata": {},
   "source": [
    "## 4. Analysing Annotated Entities\n",
    "This section runs basic checks to better understand the output of the rule-based annotation. It includes simple counts and summaries that help validate entity coverage across categories.\n",
    "\n",
    "The aim is to identify whether any categories dominate or are underrepresented, and to check that the vocabulary terms are being used as expected. These diagnostics provide a useful baseline before moving on to model training or further data refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a25e4e-e41d-46e5-a167-918d0e2bfcf0",
   "metadata": {},
   "source": [
    "### 4.1 Entity Counts by Category\n",
    "This section calculates the total number of entities annotated for each vocabulary category. It provides a high-level overview of entity distribution across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1bd3190-44cd-40ba-9cad-d5466141c603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts by category\n",
      "\n",
      "HABITAT         : 373,060\n",
      "ENV_PROCESS     : 365,599\n",
      "TAXONOMY        : 258,948\n",
      "MEASUREMENT     : 111,187\n",
      "POLLUTANT       : 96,938\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counter = Counter()\n",
    "\n",
    "for spans in clean_annotations.values():\n",
    "    for _, _, lbl in spans:\n",
    "        label_counter[lbl] += 1\n",
    "\n",
    "print(\"Entity counts by category\\n\")\n",
    "for lbl, n in label_counter.most_common():\n",
    "    print(f\"{lbl:<15} : {n:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc0473-0f78-4266-b0eb-8315d37ad9b6",
   "metadata": {},
   "source": [
    "HABITAT and ENV_PROCESS entities are the most frequently matched categories, each with over 350,000 annotations. This reflects the relatively high coverage and generality of terms in these vocabularies. TAXONOMY is also well represented, though lower in volume due to the more specific nature of species and organism names. MEASUREMENT and POLLUTANT categories are less frequent, likely due to narrower term lists and stricter matching criteria (e.g. for units and numerical formats). This distribution may influence model performance and suggests potential areas for further vocabulary expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b313509-925f-4655-9e86-1695c0b231df",
   "metadata": {},
   "source": [
    "### 4.2 Most Frequent Entities by Category\n",
    "This section prints the 30 most frequently matched entities for each vocabulary category. This helps to understand which terms are most dominant in the dataset and whether they align with expectations. It also highlights common patterns, imbalances, or potential sources of noise across different entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab92c335-4bd9-4f97-aa16-86d3b1359d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── HABITAT ──\n",
      "habitat                        32782\n",
      "forest                         25871\n",
      "ecosystem                      21801\n",
      "habitats                       21766\n",
      "river                          20572\n",
      "ecosystems                     14783\n",
      "landscape                      12460\n",
      "forests                        9274\n",
      "lake                           8154\n",
      "island                         8084\n",
      "rivers                         7568\n",
      "coast                          7220\n",
      "wood                           6070\n",
      "grassland                      5635\n",
      "islands                        4730\n",
      "landscapes                     4280\n",
      "wetland                        4225\n",
      "garden                         4171\n",
      "reef                           4166\n",
      "wetlands                       4118\n",
      "lakes                          3976\n",
      "bay                            3952\n",
      "beach                          3886\n",
      "valley                         3793\n",
      "national park                  3591\n",
      "mountain                       3502\n",
      "hill                           2882\n",
      "territory                      2742\n",
      "protected areas                2617\n",
      "grasslands                     2583\n",
      "\n",
      "── MEASUREMENT ──\n",
      "temperature                    17616\n",
      "temperatures                   10364\n",
      "ph                             5949\n",
      "km                             4684\n",
      "cm                             4422\n",
      "mg                             4380\n",
      "pa                             2868\n",
      "kg                             2702\n",
      "°c                             2453\n",
      "pm                             2020\n",
      "salinity                       2016\n",
      "ml                             1848\n",
      "mg/l                           1818\n",
      "min                            1813\n",
      "soil moisture                  1743\n",
      "pt                             1556\n",
      "ns                             1533\n",
      "humidity                       1453\n",
      "fl                             1427\n",
      "nm                             1215\n",
      "m2                             1185\n",
      "m3                             1168\n",
      "ng                             1073\n",
      "cl                             1019\n",
      "ev                             1019\n",
      "carbon footprint               965\n",
      "water level                    883\n",
      "nl                             799\n",
      "relative humidity              774\n",
      "evapotranspiration             765\n",
      "\n",
      "── POLLUTANT ──\n",
      "wastewater                     8350\n",
      "sewage                         4254\n",
      "noise                          4035\n",
      "pb                             3500\n",
      "methane                        3156\n",
      "carbon dioxide                 3098\n",
      "litter                         3077\n",
      "plastics                       3066\n",
      "pesticides                     2636\n",
      "heavy metals                   2609\n",
      "mercury                        2494\n",
      "smoke                          2398\n",
      "microplastics                  2367\n",
      "radiation                      2262\n",
      "phosphorus                     2000\n",
      "arsenic                        1591\n",
      "pesticide                      1445\n",
      "nitrate                        1321\n",
      "ozone                          1178\n",
      "ammonia                        1151\n",
      "antibiotics                    1108\n",
      "microplastic                   1035\n",
      "chromium                       1023\n",
      "pcbs                           966\n",
      "particulate matter             954\n",
      "manure                         928\n",
      "fertiliser                     879\n",
      "cadmium                        874\n",
      "antibiotic                     838\n",
      "zinc                           837\n",
      "\n",
      "── TAXONOMY ──\n",
      "fish                           18185\n",
      "birds                          11967\n",
      "bird                           7561\n",
      "bacteria                       4482\n",
      "cattle                         3164\n",
      "cod                            2930\n",
      "bees                           2239\n",
      "sheep                          2211\n",
      "mammals                        2208\n",
      "turkey                         2021\n",
      "dog                            1888\n",
      "whales                         1830\n",
      "shark                          1706\n",
      "butterfly                      1683\n",
      "salmon                         1682\n",
      "whale                          1550\n",
      "bee                            1511\n",
      "bear                           1504\n",
      "butterflies                    1459\n",
      "sharks                         1417\n",
      "cows                           1410\n",
      "pigs                           1383\n",
      "dogs                           1360\n",
      "deer                           1252\n",
      "bats                           1229\n",
      "rocky                          1134\n",
      "mammal                         1130\n",
      "ant                            1114\n",
      "wolves                         1092\n",
      "pig                            1057\n",
      "\n",
      "── ENV_PROCESS ──\n",
      "climate                        69779\n",
      "climate change                 33526\n",
      "drought                        27453\n",
      "emissions                      26285\n",
      "flood                          14253\n",
      "pollution                      13537\n",
      "weather                        10309\n",
      "season                         8100\n",
      "wind                           7724\n",
      "restoration                    7501\n",
      "air pollution                  5812\n",
      "rain                           5259\n",
      "degradation                    4881\n",
      "anthropogenic                  4623\n",
      "contamination                  4522\n",
      "deforestation                  3842\n",
      "effluent                       3834\n",
      "storm                          3766\n",
      "droughts                       3634\n",
      "greenhouse gas emissions       3506\n",
      "carbon emissions               3433\n",
      "snow                           3329\n",
      "global warming                 3138\n",
      "dam                            2959\n",
      "adsorption                     2836\n",
      "seasons                        2610\n",
      "storms                         2454\n",
      "emission                       2242\n",
      "deposition                     2106\n",
      "remote sensing                 2017\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Top 30 frequent entity texts per label (lowercased)\n",
    "for lbl in label_counter:\n",
    "    c = Counter(\n",
    "        text[start:end].lower()\n",
    "        for text, spans in clean_annotations.items()\n",
    "        for start, end, l in spans\n",
    "        if l == lbl\n",
    "    )\n",
    "    print(f\"\\n── {lbl} ──\")\n",
    "    for token, n in c.most_common(30):\n",
    "        print(f\"{token:<30} {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f2e7c-a7d3-4633-a3fc-63ead71b080c",
   "metadata": {},
   "source": [
    "The most common entities reflect both vocabulary design and the nature of environmental text:\n",
    "\n",
    "- HABITAT is dominated by general ecological terms such as habitat, forest, ecosystem, and river. This shows strong representation of common landscape features.\n",
    "- MEASUREMENT includes units (mg, kg, km) and environmental indicators (temperature, pH, salinity). These often appear alongside quantitative observations.\n",
    "- POLLUTANT terms like wastewater, sewage, noise, methane, and carbon dioxide reflect typical environmental hazards. Some highly specific chemicals (pb, arsenic, pcbs) are also well represented.\n",
    "- TAXONOMY covers high-level terms like fish, birds, and bacteria, as well as species and common animals (whale, salmon, pig). A few odd entries like rocky or ant may require review.\n",
    "- ENV_PROCESS is led by climate, climate change, and drought. These results confirm that this category captures broader environmental phenomena and stressors.\n",
    "\n",
    "The data confirms that entity matching works reliably for frequent, well-represented terms. It may also highlight vocabulary areas to refine or extend in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f68561-6c0f-48f4-8887-3038f9873d3a",
   "metadata": {},
   "source": [
    "### 4.3 Entity Distribution per Sentence\n",
    "This section examines how many entities appear in each annotated sentence. Understanding this distribution helps assess the density of entity labels across the dataset.\n",
    "\n",
    "Sentences with very few entities may provide limited training signal, while those with many entities might overwhelm a model or indicate noisy matches. A balanced distribution is generally preferable, especially for training Named Entity Recognition systems.\n",
    "\n",
    "The analysis includes summary statistics and a histogram to visualise the number of entities per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ecc21cb6-ad95-4241-97b8-b1b79b646d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotated sentences     : 735,542\n",
      "Average entities per sentence : 1.64\n",
      "Max entities in a sentence    : 53\n",
      "Sentences with only 1 entity  : 449,228\n",
      "Sentences with >10 entities   : 441\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "entity_counts = [len(spans) for spans in clean_annotations.values()]\n",
    "\n",
    "print(f\"Total annotated sentences     : {len(entity_counts):,}\")\n",
    "print(f\"Average entities per sentence : {sum(entity_counts)/len(entity_counts):.2f}\")\n",
    "print(f\"Max entities in a sentence    : {max(entity_counts)}\")\n",
    "print(f\"Sentences with only 1 entity  : {(entity_counts.count(1)):,}\")\n",
    "print(f\"Sentences with >10 entities   : {sum(1 for c in entity_counts if c > 10):,}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(entity_counts, bins=range(1, 16), edgecolor='black', align='left')\n",
    "plt.title(\"Distribution of Entity Counts per Sentence\")\n",
    "plt.xlabel(\"Number of Entities\")\n",
    "plt.ylabel(\"Number of Sentences\")\n",
    "plt.xticks(range(1, 16))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8faf7-5c16-49bc-9388-f388ef547fed",
   "metadata": {},
   "source": [
    "The majority of annotated sentences contain only a small number of entities. Specifically:\n",
    "\n",
    "- Average entities per sentence: 1.64\n",
    "- Sentences with exactly one entity: 449,228\n",
    "- Sentences with more than 10 entities: 441\n",
    "- Maximum number of entities in a sentence: 53\n",
    "\n",
    "The distribution is heavily right-skewed, with most sentences containing 1–3 entities. This is expected, as environmental sentences tend to mention a limited number of domain-specific terms per context.\n",
    "\n",
    "This distribution is favourable for training, as it provides many examples with single or paired entities, helping the model learn isolated and contextual patterns. However, a small number of complex sentences with dense annotations may be useful for testing the model’s ability to generalise in high-entity settings.\n",
    "\n",
    "Sentences with a very high number of entities (e.g. >10) may arise from:\n",
    "\n",
    "- Long compound sentences\n",
    "- Tabular or list-like structures\n",
    "- Noise or overly permissive matching\n",
    "\n",
    "These should be reviewed separately if annotation noise becomes an issue during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a651ed-d0d5-4704-b943-27b1a1815c47",
   "metadata": {},
   "source": [
    "### 4.4 Reviewing High-Entity Sentences\n",
    "While most sentences contain only a few entities, a small number include unusually high counts. This section reviews a sample of these outliers to assess whether they represent meaningful, well-structured data or noisy artefacts.\n",
    "\n",
    "Inspecting these cases helps identify patterns such as list-like structures, overly technical language, or annotation artefacts caused by aggressive matching. If needed, such sentences can be filtered or downweighted during model training to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2bd1621-49e2-4869-aaff-855dedc98eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_annotations = {\n",
    "    sent: spans for sent, spans in clean_annotations.items()\n",
    "    if len(spans) <= 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5518316c-f1f3-400d-9eee-c74d12b75fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Here we use subtidal surface sediment surveys and sediment cores to explore the effects of the 2011 Brisbane River flood on trace metals zinc (Zn), lead (Pb), copper (Cu), nickel (Ni), chromium (Cr), manganese (Mn), and phosphorus (P) deposition in Moreton Bay, a shallow subtropical bay in eastern Australia.',\n",
       "  [[109, 114, 'HABITAT'],\n",
       "   [115, 120, 'ENV_PROCESS'],\n",
       "   [137, 141, 'POLLUTANT'],\n",
       "   [154, 156, 'POLLUTANT'],\n",
       "   [159, 165, 'POLLUTANT'],\n",
       "   [185, 193, 'POLLUTANT'],\n",
       "   [200, 209, 'POLLUTANT'],\n",
       "   [220, 230, 'POLLUTANT'],\n",
       "   [235, 245, 'ENV_PROCESS'],\n",
       "   [257, 260, 'HABITAT'],\n",
       "   [284, 287, 'HABITAT']]),\n",
       " ('Variable List (i.e. column description): Region= UK region where fish site is located Catchment= the river catchment/basin location of each fish site used in the Chempop project River= the river/watercourse location of each fish site used in the Chempop project SiteID= the unique code for each fish site used in the Chempop project SiteName= the name of each fish site used in the Chempop project NGR= fish site NGR location SurveyEasting= the easting of each fish site used in the Chempop project** SurveyNorthing= the northing of each fish site used in the Chempop project** SurveyArea= area of fish site surveyed (m-2)',\n",
       "  [[65, 69, 'TAXONOMY'],\n",
       "   [101, 106, 'HABITAT'],\n",
       "   [140, 144, 'TAXONOMY'],\n",
       "   [178, 183, 'HABITAT'],\n",
       "   [189, 194, 'HABITAT'],\n",
       "   [195, 206, 'HABITAT'],\n",
       "   [224, 228, 'TAXONOMY'],\n",
       "   [295, 299, 'TAXONOMY'],\n",
       "   [360, 364, 'TAXONOMY'],\n",
       "   [403, 407, 'TAXONOMY'],\n",
       "   [461, 465, 'TAXONOMY'],\n",
       "   [538, 542, 'TAXONOMY'],\n",
       "   [598, 602, 'TAXONOMY']]),\n",
       " ('=Upper Hore Y Fe ug/l RDA Fe ug/l Y Fe ug/l RDA Fe ug/l Y Fe ug/l RDA Fe ug/l Y Fe ug/l RDA Fe ug/l Y Fe ug/l RDA Fe ug/l Y Fe ug/l RDA Fe ug/l Fe: Just a few iron flyers and drop-outs were culled.',\n",
       "  [[17, 21, 'MEASUREMENT'],\n",
       "   [29, 33, 'MEASUREMENT'],\n",
       "   [39, 43, 'MEASUREMENT'],\n",
       "   [51, 55, 'MEASUREMENT'],\n",
       "   [61, 65, 'MEASUREMENT'],\n",
       "   [73, 77, 'MEASUREMENT'],\n",
       "   [83, 87, 'MEASUREMENT'],\n",
       "   [95, 99, 'MEASUREMENT'],\n",
       "   [105, 109, 'MEASUREMENT'],\n",
       "   [117, 121, 'MEASUREMENT'],\n",
       "   [127, 131, 'MEASUREMENT'],\n",
       "   [139, 143, 'MEASUREMENT']]),\n",
       " ('HQA_Score = the River Habitat Survey Habitat Quality Assessment score (covariate; habitat quality) HMS_Score = the River Habitat Survey Habitat Modification Score (covariate; habitat quality) Fish_Altitude = the terrain altitude (m above sea level) of each fish site used in the Chempop project (covariate) (meters above sea level) LC_Woodland_Per = the percentage of the catchment upstream of each fish site used in the Chempop project characterised as woodland (covariate; land use) (%) LC_Arable_Per = the percentage of the catchment upstream of each fish site used in the Chempop project characterised as arable (covariate; land use) (%) LC_Seminatural_Per= the percentage of the catchment upstream of each fish site used in the Chempop project characterised as semi-natural (covariate; land use) (%)',\n",
       "  [[16, 21, 'HABITAT'],\n",
       "   [22, 29, 'HABITAT'],\n",
       "   [37, 44, 'HABITAT'],\n",
       "   [82, 89, 'HABITAT'],\n",
       "   [115, 120, 'HABITAT'],\n",
       "   [121, 128, 'HABITAT'],\n",
       "   [136, 143, 'HABITAT'],\n",
       "   [175, 182, 'HABITAT'],\n",
       "   [192, 196, 'TAXONOMY'],\n",
       "   [197, 205, 'HABITAT'],\n",
       "   [220, 228, 'HABITAT'],\n",
       "   [238, 247, 'ENV_PROCESS'],\n",
       "   [257, 261, 'TAXONOMY'],\n",
       "   [321, 330, 'ENV_PROCESS'],\n",
       "   [399, 403, 'TAXONOMY'],\n",
       "   [554, 558, 'TAXONOMY'],\n",
       "   [711, 715, 'TAXONOMY']]),\n",
       " ('The greenhouse gas (GHG) emission represent the amount of greenhouse gases (nitrous oxide, methane and carbon dioxide) emissions associated with the annual production of 172 crops and 7 livestock commodities, considering the following processes emitting GHGs: (i) for crops, peatland drainage, rice cultivation, manure and synthetic fertiliser application to cropland, and (ii) for livestock, enteric fermentation, manure deposition on grassland, manure management on farm; and feed-related emissions due to application of manure on feed crops and application of synthetic fertiliser on grassland and feed crops, plus other cropland emissions for feed crops (e.g. from rice cultivation and peatland drainage).',\n",
       "  [[4, 18, 'ENV_PROCESS'],\n",
       "   [25, 33, 'ENV_PROCESS'],\n",
       "   [58, 74, 'ENV_PROCESS'],\n",
       "   [91, 98, 'POLLUTANT'],\n",
       "   [103, 117, 'POLLUTANT'],\n",
       "   [119, 128, 'ENV_PROCESS'],\n",
       "   [312, 318, 'POLLUTANT'],\n",
       "   [333, 343, 'POLLUTANT'],\n",
       "   [415, 421, 'POLLUTANT'],\n",
       "   [422, 432, 'ENV_PROCESS'],\n",
       "   [436, 445, 'HABITAT'],\n",
       "   [447, 453, 'POLLUTANT'],\n",
       "   [491, 500, 'ENV_PROCESS'],\n",
       "   [523, 529, 'POLLUTANT'],\n",
       "   [573, 583, 'POLLUTANT'],\n",
       "   [587, 596, 'HABITAT'],\n",
       "   [633, 642, 'ENV_PROCESS']])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_entity = [(s, v) for s, v in clean_annotations.items() if len(v) > 10]\n",
    "random.sample(high_entity, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb594556-187b-4a15-b234-e89727a7d4a0",
   "metadata": {},
   "source": [
    "The sample reveals a variety of cases where high entity counts are valid but come with important caveats:\n",
    "\n",
    "- Scientific reporting style: Many sentences come from academic writing with dense terminology. For example, lists of pollutants (\"zinc (Zn), lead (Pb), copper (Cu)...\") or measurement units (\"Fe ug/l\", \"mg/L\") are common in environmental science texts. These are annotated correctly but may challenge models during training due to their structural complexity.\n",
    "- Tabular or metadata-like text: Some sentences resemble structured metadata fields or headers (e.g. “Region= UK region where fish site is located...”). These are often not natural language and could reduce model generalisability if over-represented. They might be excluded or separated into a different training tier.\n",
    "- Overlapping or stacked mentions: In certain cases, such as compound noun phrases or repeated entity mentions (“manure management on farm; and feed-related emissions…”), the model may need to resolve nested relationships or semantic dependencies. These are not incorrect but are more difficult to learn from.\n",
    "- Measurement-heavy strings: Sentences such as the one containing a dozen variants of “Fe ug/l” show how repetitive, pattern-like data can dominate. These may inflate frequency counts for certain entity types (like MEASUREMENT) and should be reviewed to ensure diversity in training data.\n",
    "\n",
    "Overall, while many high-entity sentences are valid and domain-relevant, they highlight the importance of data curation. If these types dominate the dataset, they may bias the model. Sampling or weighting strategies can be considered during training to ensure balanced learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284d3f3-8279-4c8a-b972-c0c160329e3b",
   "metadata": {},
   "source": [
    "### 4.6 Span Length Analysis\n",
    "This step measures the average and maximum span length (in characters) for each entity category. It provides insight into how concise or descriptive the annotations are. For instance, longer spans may indicate compound expressions like “coastal salt marsh”, while very short spans may suggest generic terms like “fish”.\n",
    "\n",
    "Monitoring span lengths helps validate the quality of boundaries and informs later decisions, such as truncation limits or maximum sequence lengths in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12610a63-4529-4a43-b0ac-9abaae9896df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label            Avg. Length   Max Length\n",
      "HABITAT                 7.66           35\n",
      "MEASUREMENT             6.46           29\n",
      "POLLUTANT               8.56           37\n",
      "TAXONOMY                6.52           23\n",
      "ENV_PROCESS             9.86           36\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "lengths_by_label = defaultdict(list)\n",
    "\n",
    "for text, spans in clean_annotations.items():\n",
    "    for start, end, label in spans:\n",
    "        span_len = end - start\n",
    "        lengths_by_label[label].append(span_len)\n",
    "\n",
    "print(f\"{'Label':<15} {'Avg. Length':>12} {'Max Length':>12}\")\n",
    "for label, lengths in lengths_by_label.items():\n",
    "    avg = np.mean(lengths)\n",
    "    max_len = max(lengths)\n",
    "    print(f\"{label:<15} {avg:12.2f} {max_len:12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9412050-e02a-4b44-9867-7a73ccdfd52d",
   "metadata": {},
   "source": [
    "The average span lengths are generally within a reasonable range across all categories, suggesting that entity boundaries have been captured accurately without excessive overreach or under-segmentation.\n",
    "\n",
    "- ENV_PROCESS (avg 9.86) has the longest spans on average. This is expected since many environmental processes involve compound phrases like “climate change” or “greenhouse gas emissions”.\n",
    "- POLLUTANT (avg 8.56) also shows a relatively high average. Entities such as “particulate matter” or “carbon dioxide” contribute to this.\n",
    "- HABITAT (avg 7.66) has moderately long spans, consistent with terms like “wetland ecosystem” or “national park”.\n",
    "- TAXONOMY (avg 6.52) and MEASUREMENT (avg 6.46) show shorter average lengths, which aligns with expectations since many species names and units are concise (e.g. “fish”, “mg/l”, “bee”).\n",
    "\n",
    "The maximum lengths (ranging from 23 to 37 characters) are not excessive and suggest that longer descriptive entities are rare but valid.\n",
    "\n",
    "No immediate action is needed, but it's good to be aware that models trained on this data may need appropriate token and span length limits to accommodate longer phrases, especially in ENV_PROCESS and POLLUTANT categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca90d7f-058e-477c-8fa3-b052258a0696",
   "metadata": {},
   "source": [
    "### 4.8 Validating Exported Annotations\n",
    "This section loads the exported .jsonl file and performs a final validation on all entity annotations. The purpose is to confirm that span boundaries are correct, entities are well-formed, and no overlaps remain. These checks ensure the data is ready for SpaCy model training without format or logic issues.\n",
    "\n",
    "Validation includes:\n",
    "- Span boundaries are within the sentence text length\n",
    "- Start character is strictly less than end character\n",
    "- No overlapping spans in the same sentence\n",
    "- Consistent record formatting across all lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation summary:\n",
      "  Total lines checked         : 735,542\n",
      "  Invalid format lines        : 0\n",
      "  Spans with reversed bounds  : 0\n",
      "  Spans out of sentence bounds: 0\n",
      "  Sentences with overlaps     : 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def validate_jsonl_annotations(path):\n",
    "    \"\"\"Validate .jsonl annotations for format, span bounds, and overlaps\"\"\"\n",
    "    def has_overlap(spans):\n",
    "        spans = sorted(spans, key=lambda x: x[0])\n",
    "        for i in range(len(spans) - 1):\n",
    "            if spans[i][1] > spans[i+1][0]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    invalid_format = 0\n",
    "    out_of_bounds = 0\n",
    "    reversed_spans = 0\n",
    "    overlapping = 0\n",
    "    total = 0\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                text = record[\"text\"]\n",
    "                spans = record[\"label\"]\n",
    "            except Exception:\n",
    "                invalid_format += 1\n",
    "                continue\n",
    "\n",
    "            text_len = len(text)\n",
    "\n",
    "            for start, end, _ in spans:\n",
    "                if start >= end:\n",
    "                    reversed_spans += 1\n",
    "                if not (0 <= start < end <= text_len):\n",
    "                    out_of_bounds += 1\n",
    "\n",
    "            if has_overlap(spans):\n",
    "                overlapping += 1\n",
    "\n",
    "    print(\"Validation summary:\")\n",
    "    print(f\"  Total lines checked         : {total:,}\")\n",
    "    print(f\"  Invalid format lines        : {invalid_format}\")\n",
    "    print(f\"  Spans with reversed bounds  : {reversed_spans}\")\n",
    "    print(f\"  Spans out of sentence bounds: {out_of_bounds}\")\n",
    "    print(f\"  Sentences with overlaps     : {overlapping}\")\n",
    "\n",
    "jsonl_path = OUTPUT_PATH / \"training_data.jsonl\"\n",
    "validate_jsonl_annotations(jsonl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde72e0-7fd6-4b51-8a24-d5bb3644f8fe",
   "metadata": {},
   "source": [
    "All 735,542 annotated sentences passed the validation checks. There were no format errors, no reversed or out-of-bounds spans, and no overlaps. This confirms that the exported .jsonl file is correctly structured and suitable for use in NER model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981bce9-cc89-46b3-a58b-602f256653ad",
   "metadata": {},
   "source": [
    "## 5. Final Summary\n",
    "\n",
    "### 5.1 Summary of the Annotation Process\n",
    "This section applied a rule-based method to assign entity labels to environmental text using precompiled vocabulary lists. Over 2.7 million pre-segmented sentences were processed using an Aho-Corasick matcher for each entity category. Matches were filtered to ensure full-word boundaries and remove partial tokens from hyphenated words.\n",
    "\n",
    "Overlapping annotations were resolved by retaining only the longest span in each case. The result was a clean and consistent set of annotations, exported in SpaCy’s .jsonl format and validated for correctness.\n",
    "\n",
    "Key results:\n",
    "\n",
    "- Sentences with entities: 735,542\n",
    "- Total entity spans: 1,205,732\n",
    "- Average entities per sentence: 1.64\n",
    "- Top categories: HABITAT and ENV_PROCESS\n",
    "- Validation outcome: No overlapping spans or boundary issues in the final file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922dc89-b7aa-44ae-aba3-7cdc55824e70",
   "metadata": {},
   "source": [
    "### 5.2 Next Steps\n",
    "The next stage will focus on evaluating how well this annotated dataset performs when used to train Named Entity Recognition (NER) models. This will involve:\n",
    "\n",
    "- Training a baseline model using a Conditional Random Field (CRF)\n",
    "- Training a SpaCy pipeline using its default convolutional neural network (CNN) architecture\n",
    "- Training a SpaCy pipeline with transformer-based embeddings for improved contextual understanding\n",
    "\n",
    "Each model will be assessed on performance, generalisation, and error patterns. Based on the results, further refinement of vocabulary, annotation logic, or synthetic data generation may be considered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
