{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e8adcb-b9c2-4a43-a525-5fe328177ac8",
   "metadata": {},
   "source": [
    "# SpaCy CNN Model for Environmental NER\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Background and Purpose\n",
    "This notebook continues the model training phase of the environmental Named Entity Recognition (NER) pipeline. Following the baseline evaluation using Conditional Random Fields (CRFs), we now transition to training a neural model using SpaCy’s convolutional neural network (CNN) architecture.\n",
    "\n",
    "SpaCy’s built-in `tok2vec` + CNN pipeline offers a fast and compact neural alternative to classical sequence models. While less complex than transformer-based architectures, it retains the ability to model contextual features using a combination of static embeddings and convolutional filters. This makes it a practical next step in assessing how well a lightweight neural approach can generalise from rule-based annotations to unseen text.\n",
    "\n",
    "The same annotated dataset is used here, comprising 735,542 sentences and over 1.2 million entity spans, labelled with five domain-specific entity types: TAXONOMY, HABITAT, ENV_PROCESS, POLLUTANT, and MEASUREMENT. The labels were generated using exact string matching against curated environmental vocabularies and follow the SpaCy `doc.ents` format rather than BIO tagging.\n",
    "\n",
    "This stage will help evaluate whether a neural network trained end-to-end on the raw text and entity spans can learn contextual patterns beyond surface string matches. It also establishes a reference point for comparing more advanced transformer-based models later.\n",
    "\n",
    "### 1.2 Objectives\n",
    "This notebook aims to train and evaluate a series of SpaCy CNN-based NER models, each with progressively stronger hyperparameters. These models are intended to:\n",
    "\n",
    "- Serve as neural baselines that are more expressive than CRFs but faster than transformers.\n",
    "- Test whether convolutional features can generalise entity recognition beyond the rule-based matches.\n",
    "- Explore the impact of hyperparameters such as batch size, dropout, and training steps.\n",
    "- Identify how early SpaCy models perform on different entity types.\n",
    "- Prepare for subsequent experiments involving transformer-based architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9efd3c-fd20-4ef7-951f-265c0e7e128d",
   "metadata": {},
   "source": [
    "## 2. Preparing the Dataset for SpaCy CNN\n",
    "\n",
    "The SpaCy CNN model expects training and evaluation data in its native binary format (`.spacy`), where each example contains raw text and associated entity spans stored in `doc.ents`. The dataset generated from the rule-based annotation pipeline was originally stored in `.jsonl` format, with each entry containing a sentence and a list of character-offset entity spans under the keys `\"text\"` and `\"label\"`.\n",
    "\n",
    "To prepare this data for SpaCy’s training pipeline, the following steps are performed:\n",
    "\n",
    "1. Load the annotated `.jsonl` file into memory.\n",
    "2. Split the dataset into training, validation, and test sets using a 70/15/15 ratio.\n",
    "3. Convert each list of records into `DocBin` objects using `nlp.make_doc(...)` and character-span alignment.\n",
    "4. Save the converted data to `.spacy` files, which are required for use with SpaCy’s CLI training interface.\n",
    "\n",
    "Unlike the CRF model, there is no need to convert to BIO tags explicitly, as SpaCy internally handles alignment between raw text and annotated entity spans during training. This makes the pipeline more streamlined while maintaining compatibility with downstream neural models.\n",
    "\n",
    "This step ensures the data is formatted efficiently for high-throughput model training and consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9ee7fa-821f-4e49-a42a-742ea39cf780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "TRAINING_DATA_PATH = Path(\"../data/json/training_data.jsonl\")\n",
    "SPACY_MODEL_PATH = Path(\"../models/spaCy\")\n",
    "SPACY_DATA_PATH = Path(\"../data/spaCy\")\n",
    "CONFIG_PATH = Path(\"./spaCy_configs/cnn\")\n",
    "CONFIG_PATH_TRANSFORMER = Path(\"./spaCy_configs/transformer\")\n",
    "\n",
    "SPACY_MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "SPACY_DATA_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76234923-4591-4b4d-829b-43f3e05bb6ee",
   "metadata": {},
   "source": [
    "### 2.1 Load and Inspect Annotated Data\n",
    "The annotated dataset is stored in `.jsonl` format, with each line containing a sentence `text` and a list of entity spans under the key `label`. Each span is defined using character offsets and a corresponding entity label, reflecting the results of the earlier rule-based annotation stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b167a46-768f-4012-8f37-5f56b5b71d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c585b5a3-201b-4881-977e-faf48ae5f4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "735479"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = load_jsonl(TRAINING_DATA_PATH)\n",
    "\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8486101-c486-4523-9bc7-3bb432dffbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Biodiverse, multitrophic communities are increasingly recognised as important promoters of species persistence and resilience under environmental change.\n",
      " → environmental change [ENV_PROCESS]\n",
      "\n",
      "Text: Environmental groups have long campaigned to protect the area, which sustains millions of migrating birds and is home to a major population of endangered Iberian lynxes, pointing out that the illegal wells sunk to feed the region’s numerous soft fruit farms are stressing the aquifer.\n",
      " → birds [TAXONOMY]\n",
      " → Iberian lynxes [TAXONOMY]\n",
      "\n",
      "Text: world is not on track to prevent catastrophic warming, to keep temperatures from increasing more than 2C (3.6F), Patricia Espinosa, the executive secretary of the United Nations framework convention on climate change, said earlier this month.\n",
      " → temperatures [MEASUREMENT]\n",
      " → climate change [ENV_PROCESS]\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "for ex in sample(training_data, 3):\n",
    "    text = ex[\"text\"]\n",
    "    entities = ex[\"label\"]\n",
    "    print(\"\\nText:\", text)\n",
    "    for start, end, label in entities:\n",
    "        print(f\" → {text[start:end]} [{label}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1edef-1226-43bd-ba7a-0274be8d3430",
   "metadata": {},
   "source": [
    "A random sample of entries confirms that the spans have been accurately mapped to their corresponding substrings in the text. Entities such as *environmental change* and *climate change* are correctly identified as `ENV_PROCESS`, while biological mentions like *birds* and *Iberian lynxes* are classified as `TAXONOMY`. Quantitative references such as *temperatures* are assigned the `MEASUREMENT` label.\n",
    "\n",
    "This inspection demonstrates both the structural integrity of the span annotations and the diversity of entity types captured by the rule-based method. The presence of both single-token and multi-token entities, along with varying sentence structures, supports the suitability of the data for training a generalisable NER model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37a185-20fc-4780-b762-0c06af0cfe93",
   "metadata": {},
   "source": [
    "### 2.2 Train–Validation–Test Split\n",
    "Before converting the data into SpaCy’s training format, the dataset must be partitioned into separate subsets for training, validation, and testing. This allows for fair evaluation of the model’s ability to generalise beyond its training data.\n",
    "\n",
    "The dataset is randomly split into 70% for training, 15% for validation, and 15% for testing. The validation set is used during model training to monitor performance and prevent overfitting, while the test set is held out entirely for final evaluation.\n",
    "\n",
    "Unlike standard classification tasks, stratified splitting is not used here because the entity labels exist as spans within text rather than as discrete document-level categories. The large size of the dataset ensures sufficient representation of each entity type across all three subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9aafe29-aebf-448b-aa5a-7b937bf3fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 514835\n",
      "Validation size: 110322\n",
      "Test size: 110322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "train_data, temp_data = train_test_split(training_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: 15% val, 15% test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc5499-733e-4127-8352-2ed4906dcb08",
   "metadata": {},
   "source": [
    "The split results in 514,835 training examples and 110,322 each for validation and test. These proportions provide a strong foundation for both model optimisation and evaluation. Entity span diversity is preserved across subsets due to the randomised sampling, making the split suitable for training a general-purpose neural NER model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2247ee7-064f-4b0e-a21b-906d645cb23c",
   "metadata": {},
   "source": [
    "### 2.3 Convert Records to SpaCy DocBin Format\n",
    "\n",
    "SpaCy's training pipeline requires data to be serialised into its binary `.spacy` format, which is optimised for speed and memory efficiency. To achieve this, each sentence and its associated entity spans are converted into a `Doc` object and then stored in a `DocBin` container.\n",
    "\n",
    "The `char_span` method is used to align each entity span (stored as character offsets) with the correct token boundaries within the `Doc`. This is necessary because entity spans must match valid token boundaries for SpaCy to process them correctly. In cases where the span cannot be aligned (due to tokenisation mismatch or annotation error), the resulting `None` values are filtered out before assigning to `doc.ents`.\n",
    "\n",
    "This process ensures that only well-formed entity spans are used in training, maintaining data integrity and preventing runtime errors during model fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2235fe35-db30-4730-9d64-81185a1c1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_records_to_docbin(records, nlp):\n",
    "    doc_bin = DocBin()\n",
    "    for record in records:\n",
    "        text = record[\"text\"]\n",
    "        entities = record[\"label\"]\n",
    "        doc = nlp.make_doc(text)\n",
    "        spans = [doc.char_span(start, end, label=label) for start, end, label in entities]\n",
    "        spans = [span for span in spans if span is not None]\n",
    "        doc.ents = spans\n",
    "        doc_bin.add(doc)\n",
    "    return doc_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e05f33ff-dffc-47e2-9189-2dcea2efd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 514835\n",
      "Val: 110322\n",
      "Test: 110322\n"
     ]
    }
   ],
   "source": [
    "nlp_blank = spacy.blank(\"en\")\n",
    "\n",
    "train_docbin = convert_json_records_to_docbin(train_data, nlp_blank)\n",
    "val_docbin = convert_json_records_to_docbin(val_data, nlp_blank)\n",
    "test_docbin = convert_json_records_to_docbin(test_data, nlp_blank)\n",
    "\n",
    "print(f\"Train: {len(list(train_docbin.get_docs(nlp_blank.vocab)))}\")\n",
    "print(f\"Val: {len(list(val_docbin.get_docs(nlp_blank.vocab)))}\")\n",
    "print(f\"Test: {len(list(test_docbin.get_docs(nlp_blank.vocab)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf9048-fa2b-4081-8cc6-2e38a13cb311",
   "metadata": {},
   "source": [
    "The output confirms that each subset has been successfully converted into a `DocBin` object, with valid entity spans attached to each `Doc`. The number of `Doc` objects in each bin matches the counts from the previous data split, confirming that no data was lost during conversion.\n",
    "\n",
    "By filtering out `None` spans, the function ensures that only valid, token-aligned entity annotations are retained, which is essential for error-free training using SpaCy’s NER component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564bf92-9e21-44e5-bc57-901bf5c02e0c",
   "metadata": {},
   "source": [
    "### 2.4 Prepare `Example` Objects for Evaluation and Fine-Grained Control\n",
    "While SpaCy's CLI training interface operates directly on `.spacy` files, evaluation and diagnostics often require the use of `Example` objects. These allow more granular inspection of model predictions and facilitate custom scoring, visualisation, and error analysis.\n",
    "\n",
    "Each `Example` pairs a `Doc` object with its annotated entity spans, allowing functions like `evaluate_ner_model()` to compare predictions against the ground truth at a span level.\n",
    "\n",
    "This step converts the `DocBin` datasets into in-memory `Example` lists for the training, validation, and test sets. Although not strictly necessary for model training via the CLI, these objects provide flexibility for analysis and are essential for consistent evaluation across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35015da2-11e2-4d90-b0a0-4036f1286fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples_from_docbin(docbin, vocab):\n",
    "    docs = list(docbin.get_docs(vocab))\n",
    "    examples = [\n",
    "        Example.from_dict(doc, {\n",
    "            \"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        }) for doc in docs\n",
    "    ]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c09914d-31a1-4331-96cf-5093257a86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = prepare_examples_from_docbin(train_docbin, nlp_blank.vocab)\n",
    "val_examples = prepare_examples_from_docbin(val_docbin, nlp_blank.vocab)\n",
    "test_examples = prepare_examples_from_docbin(test_docbin, nlp_blank.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1415511d-c933-442d-b41a-39263c4f9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Inland waters are unique ecosystems offering services and habitat resources upon which many species depend.\n",
      " → Inland waters [HABITAT]\n",
      " → ecosystems [HABITAT]\n",
      " → habitat [HABITAT]\n"
     ]
    }
   ],
   "source": [
    "sample_example = random.choice(train_examples)\n",
    "print(\"Text:\", sample_example.reference.text)\n",
    "for ent in sample_example.reference.ents:\n",
    "    print(f\" → {ent.text} [{ent.label_}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7309c496-e37b-4856-b0ec-b03c736c19cb",
   "metadata": {},
   "source": [
    "The sample output confirms that entity spans from the original annotations have been correctly mapped to their corresponding substrings in the `Doc` objects. Terms such as *Inland waters*, *ecosystems*, and *habitat* are properly recognised as `HABITAT`, demonstrating that the conversion to `Example` objects retains both the structure and semantics needed for reliable evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab28c7-a2f3-4f45-869f-32df9c97c211",
   "metadata": {},
   "source": [
    "### 2.5 Save Dataset in `.spacy` Format\n",
    "To enable efficient training using SpaCy’s CLI interface, the processed datasets must be saved in SpaCy’s binary `.spacy` format. This format stores tokenised `Doc` objects and their associated entity spans in a compact, serialised form optimised for fast loading during training.\n",
    "\n",
    "The `save_examples_to_spacy_file` function writes each list of `Example` objects to a file by serialising the underlying `Doc` objects (i.e., `example.reference`). These files are then referenced by the training configuration to load the data in a format that is fully compatible with SpaCy's pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b215d1f8-2aa2-4499-987e-1cc5f1f8cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "\n",
    "def save_examples_to_spacy_file(examples, nlp, output_path):\n",
    "    doc_bin = DocBin()\n",
    "    for example in examples:\n",
    "        doc_bin.add(example.reference)\n",
    "    doc_bin.to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eaff8e31-cb04-4caa-975f-bb4f6ab93ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_examples_to_spacy_file(train_examples, nlp_blank, SPACY_DATA_PATH / \"train.spacy\")\n",
    "save_examples_to_spacy_file(val_examples, nlp_blank, SPACY_DATA_PATH / \"val.spacy\")\n",
    "save_examples_to_spacy_file(test_examples, nlp_blank, SPACY_DATA_PATH / \"test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861492d-4b5f-4c39-b9f3-26a1b710f77f",
   "metadata": {},
   "source": [
    "## 3. Training SpaCy CNN Models\n",
    "This section introduces the training of neural models using SpaCy’s built-in convolutional neural network (CNN) pipeline for NER. Unlike classical models such as CRFs, SpaCy models operate directly on raw text and learn features automatically through a `tok2vec` embedding layer followed by convolutional filters that encode local context.\n",
    "\n",
    "The `tok2vec` component maps each token into a dense vector representation by combining word embeddings (e.g. hash-based, static, or pretrained) with contextual information from neighbouring tokens. This forms the input to the CNN encoder, which applies multiple convolutional filters across sliding windows of tokens.\n",
    "\n",
    "For instance, given the phrase *\"30 kg of nitrogen\"*, a convolutional window might span tokens like *\"30\"*, *\"kg\"*, and *\"of\"*. A filter trained to detect measurements would recognise this pattern and help classify the span as a `MEASUREMENT` entity.\n",
    "\n",
    "These filters function similarly to edge detectors in image processing: they scan over the input and activate when local patterns are detected (e.g. a numeric unit or organism mention). By stacking multiple filters and layers, the model captures increasingly complex structures such as noun phrases or taxonomic clusters.\n",
    "\n",
    "Training is conducted using SpaCy’s CLI interface, which relies on a configuration file to define all components of the pipeline, including architecture, hyperparameters, and data paths. Each model is trained with different settings to explore the impact of batch size, dropout, encoder depth, and training duration.\n",
    "\n",
    "The following sections describe each configuration, along with its rationale and the resulting model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8125c6-ce4b-40a8-b497-9eec4b2d507f",
   "metadata": {},
   "source": [
    "### 3.1 Training the Baseline Model\n",
    "The first model in this notebook serves as a neural baseline for comparison against future CNN and transformer models. It uses a standard SpaCy CNN pipeline generated via the `init config` command. The configuration is optimised for accuracy and does not include any pre-trained embeddings or vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8387a0ee-e13a-4475-8415-1d31e4c8be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/cnn/config0.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config0.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/cnn/config0.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8538d62-1afd-4cc2-a51d-558690c3cd95",
   "metadata": {},
   "source": [
    "After generating the default `config0.cfg`, key parameters were manually adjusted to define a lightweight yet expressive model suitable for the size and structure of the training dataset. This model uses SpaCy's `tok2vec` and `ner` components without any pre-initialised weights. The aim is to assess how well the model can learn purely from the rule-based annotations.\n",
    "\n",
    "The table below summarises the main configuration choices:\n",
    "\n",
    "| Parameter                     | Value       | Purpose                                                                 |\n",
    "|------------------------------|-------------|-------------------------------------------------------------------------|\n",
    "| `vectors`                    | `null`      | No pre-trained vectors; model learns embeddings from scratch.          |\n",
    "| `init_tok2vec`               | `null`      | No initial weights; embeddings are trained directly.                   |\n",
    "| `batch_size`                 | `1000`      | Word-based batch size; tuned for speed and stability.                  |\n",
    "| `encoder architecture`       | `MaxoutWindowEncoder.v2` | Standard CNN encoder in SpaCy.                 |\n",
    "| `encoder width`              | `128`       | Moderate channel size; balances capacity and speed.                    |\n",
    "| `encoder depth`              | `4`         | Four convolutional layers; enough to capture local patterns.           |\n",
    "| `dropout`                    | `0.5`       | Regularisation to reduce overfitting.                                  |\n",
    "| `max_steps`                  | `2000`      | Maximum training steps.                                                |\n",
    "| `patience`                   | `1000`      | Early stopping if validation score does not improve.                   |\n",
    "| `learn_rate`                 | `0.001`     | Default learning rate for Adam optimiser.                              |\n",
    "| `eval_frequency`             | `500`       | Evaluate every 500 steps on the validation set.                        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b150b8-a2bb-4b5c-9da0-49729cb2e770",
   "metadata": {},
   "source": [
    "With the configuration file prepared and the dataset saved in SpaCy’s `.spacy` format, the model is trained using SpaCy’s command-line interface. The training command specifies the configuration file, input and output directories, and GPU usage. This setup allows reproducible training without managing components manually in code.\n",
    "\n",
    "During training, progress is printed after each evaluation interval. The reported metrics include:\n",
    "\n",
    "- `LOSS TOK2VEC`: Loss from the token-to-vector embedding layer. This shows how well the model is learning word-level representations.\n",
    "- `LOSS NER`: Loss from the named entity recognition component. This reflects how accurately the model predicts entity spans.\n",
    "- `ENTS_P`: Precision, or the percentage of predicted entities that are correct.\n",
    "- `ENTS_R`: Recall, or the percentage of actual entities that the model successfully identifies.\n",
    "- `ENTS_F`: F1-score, which balances precision and recall in a single value.\n",
    "- `SCORE`: The overall performance score used to decide whether training is improving.\n",
    "\n",
    "These metrics are calculated on the validation set. They are used to monitor learning and trigger early stopping if the model stops improving.\n",
    "\n",
    "When training ends, the best-performing model is saved in the `model-best` directory within the output folder. This version includes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9115e15-c135-48f5-b947-544d52635a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 21:41:31,308] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory: ../models/spaCy/cnn_0\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/cnn_0\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-06-30 21:41:31,657] [INFO] Set up nlp object from config\n",
      "[2025-06-30 21:41:31,666] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-06-30 21:41:31,668] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-06-30 21:41:31,668] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2025-06-30 21:41:31,671] [INFO] Created vocabulary\n",
      "[2025-06-30 21:41:31,671] [INFO] Finished initializing nlp object\n",
      "[2025-06-30 21:45:54,488] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-06-30 21:48:09,060] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-06-30 21:48:09,069] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-06-30 21:48:09,070] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     38.00    0.07    0.10    0.05    0.00\n",
      "  0     500        149.42   5448.22   40.08   52.21   32.52    0.40             \n",
      "  0    1000        950.60   5195.24   68.42   79.36   60.13    0.68             \n",
      "  0    1500       1079.23   6426.94   81.64   87.79   76.30    0.82             \n",
      "  0    2000       2275.93   8088.55   86.03   89.68   82.66    0.86             \n",
      "Epoch 1:   0%|                                          | 0/500 [00:00<?, ?it/s]\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/cnn_0/model-last\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "!python -m spacy train {CONFIG_PATH / \"config0.cfg\"} \\\n",
    "  --output {SPACY_MODEL_PATH / \"cnn_0\"} \\\n",
    "  --paths.train {SPACY_DATA_PATH / \"train.spacy\"} \\\n",
    "  --paths.dev {SPACY_DATA_PATH / \"val.spacy\"} \\\n",
    "  --gpu-id 0 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef207b-73c5-4f0f-bb3d-a6f550d92fac",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model   | Dropout | Batch Size | Max Steps | Learn Rate | Loss TOK2VEC | Loss NER | F1   |\n",
    "|---------|---------|------------|-----------|------------|---------------|----------|------|\n",
    "| cnn_0   | 0.5     | 1000       | 2000      | 0.001      | 2275.93       | 8088.55  | 0.86 |\n",
    "\n",
    "The baseline CNN model was trained using a batch size of 1000 and a dropout rate of 0.5. No pretrained word vectors or initial weights were used. The model learned all embeddings and parameters from scratch using a fixed learning rate of 0.001 over 2000 update steps.\n",
    "\n",
    "Training began with minimal signal (`F1 = 0.07` at step 0), indicating no prior knowledge of the data. Over time, the validation metrics improved steadily. By step 2000, the model achieved an F1-score of 86.03%, with precision at 89.68% and recall at 82.66%. This performance suggests the model was able to generalise well from rule-based annotations to diverse sentence structures.\n",
    "\n",
    "`LOSS TOK2VEC` reflects how well the model learned internal token embeddings. This value started low and increased as the model built more meaningful feature representations. A rise in `tok2vec` loss is expected when embeddings shift to encode more complex contextual patterns, especially in multi-entity sentences.\n",
    "\n",
    "`LOSS NER` measures the model's ability to classify entity spans correctly. This value increased over time as the model encountered more difficult examples and adjusted its decision boundaries. Importantly, this rise in raw loss was accompanied by improved accuracy, showing that the model continued to learn even as the training examples became harder to predict.\n",
    "\n",
    "This model establishes a strong baseline. It shows that SpaCy's CNN pipeline, even without pretrained features, can learn useful representations from environmental data with structured but weak supervision. It provides a useful reference point for evaluating future improvements using alternative configurations or model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9703b5-a522-4b09-b362-7763210cda05",
   "metadata": {},
   "source": [
    "### 3.2 SpaCy Model 2: Lower Capacity, Longer Training\n",
    "The second model, `cnn_1`, is designed to test a smaller NER classification head over a longer training schedule. It keeps the same CNN encoder from the baseline (`MaxoutWindowEncoder.v2` with width 128 and depth 4), but reduces the size of the NER head by setting `hidden_width` to 32 and `maxout_pieces` to 2. This lowers the number of parameters in the classification layer, helping evaluate whether a lightweight model can still generalise well.\n",
    "\n",
    "The `dropout` remains at 0.5 to prevent overfitting, but the number of training steps is increased to 20,000. Early stopping with a patience of 1000 steps is enabled to stop training if validation performance stops improving. The learning rate remains unchanged at 0.001.\n",
    "\n",
    "To support more frequent updates, the batch size is reduced to 500. Smaller batches introduce more noise during training, which can improve generalisation in lower-capacity models.\n",
    "\n",
    "| Parameter                     | Value       |\n",
    "|------------------------------|-------------|\n",
    "| `vectors`                    | `null`      |\n",
    "| `init_tok2vec`               | `null`      |\n",
    "| `batch_size`                 | `500`       |\n",
    "| `encoder architecture`       | `MaxoutWindowEncoder.v2` |\n",
    "| `encoder width`              | `128`       |\n",
    "| `encoder depth`              | `4`         |\n",
    "| `dropout`                    | `0.5`       |\n",
    "| `max_steps`                  | `20000`     |\n",
    "| `patience`                   | `1000`      |\n",
    "| `learn_rate`                 | `0.001`     |\n",
    "| `eval_frequency`             | `200`       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765cffd2-4e36-40bb-aea7-409f5e4fd244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/cnn/config1.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config1.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/cnn/config1.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02bdb9fd-fd08-41e5-b858-1a6482adb82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-30 22:40:35,084] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/cnn_1\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-06-30 22:40:35,418] [INFO] Set up nlp object from config\n",
      "[2025-06-30 22:40:35,428] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-06-30 22:40:35,429] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-06-30 22:40:35,429] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2025-06-30 22:40:35,432] [INFO] Created vocabulary\n",
      "[2025-06-30 22:40:35,432] [INFO] Finished initializing nlp object\n",
      "[2025-06-30 22:44:44,003] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-06-30 22:46:51,709] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-06-30 22:46:51,718] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-06-30 22:46:51,719] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-06-30 22:46:51,721] [DEBUG] Removed existing output directory: ../models/spaCy/cnn_1/model-best\n",
      "[2025-06-30 22:46:51,723] [DEBUG] Removed existing output directory: ../models/spaCy/cnn_1/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     38.00    0.07    0.11    0.05    0.00\n",
      "  0     200         65.98   2808.82    4.54   74.34    2.34    0.05             \n",
      "  0     400         55.57   1722.18   35.31   48.07   27.91    0.35             \n",
      "  0     600         58.44   1800.61   53.08   65.60   44.57    0.53             \n",
      "  0     800         84.45   2029.67   63.57   75.11   55.10    0.64             \n",
      "  0    1000        325.50   2238.38   67.96   79.10   59.57    0.68             \n",
      "  0    1200        130.34   2443.74   74.50   85.09   66.25    0.74             \n",
      "  0    1400        149.64   2585.69   79.75   87.44   73.29    0.80             \n",
      "  0    1600        203.05   2782.33   82.77   88.08   78.06    0.83             \n",
      "  0    1800       2229.63   3237.28   84.23   88.76   80.13    0.84             \n",
      "  0    2000        270.43   3505.27   86.01   89.35   82.91    0.86             \n",
      "  0    2200        334.90   4044.25   88.02   91.03   85.20    0.88             \n",
      "  0    2400        372.01   4387.80   89.23   91.96   86.66    0.89             \n",
      "  0    2600        360.68   4082.71   89.94   91.51   88.42    0.90             \n",
      "  0    2800        431.88   3887.96   90.77   92.56   89.05    0.91             \n",
      "  0    3000       1246.43   3763.77   91.33   93.37   89.37    0.91             \n",
      "  0    3200        356.56   3543.15   90.82   92.66   89.06    0.91             \n",
      "  0    3400        364.03   3523.86   91.27   91.76   90.78    0.91             \n",
      "  0    3600        415.97   3215.70   91.65   92.53   90.80    0.92             \n",
      "  0    3800        358.90   3157.40   92.00   92.84   91.16    0.92             \n",
      "  0    4000        348.60   3157.47   92.48   92.97   91.99    0.92             \n",
      "  0    4200        397.15   3034.93   92.31   92.59   92.04    0.92             \n",
      "  0    4400        343.57   2892.27   93.03   93.59   92.47    0.93             \n",
      "  0    4600        424.27   2891.51   92.88   94.79   91.05    0.93             \n",
      "  0    4800        328.41   2620.16   93.17   94.02   92.34    0.93             \n",
      "  0    5000        347.72   2753.39   93.06   93.68   92.45    0.93             \n",
      "  0    5200        352.15   2632.74   93.55   94.12   92.98    0.94             \n",
      "  0    5400        347.25   2545.47   93.00   93.32   92.67    0.93             \n",
      "  0    5600        388.01   2572.57   93.61   94.11   93.11    0.94             \n",
      "  0    5800        346.62   2465.19   94.08   95.23   92.95    0.94             \n",
      "  0    6000        366.71   2491.33   93.95   94.21   93.68    0.94             \n",
      "  0    6200        373.66   2359.99   94.36   95.14   93.58    0.94             \n",
      "  0    6400        389.97   2415.41   94.27   94.70   93.84    0.94             \n",
      "  0    6600        342.99   2314.85   94.16   94.17   94.16    0.94             \n",
      "  0    6800        350.19   2316.82   94.60   95.03   94.17    0.95             \n",
      "  0    7000        352.45   2204.24   94.59   95.20   93.99    0.95             \n",
      "  0    7200        377.97   2274.56   94.68   95.34   94.02    0.95             \n",
      "  0    7400        337.59   2161.16   94.79   95.00   94.58    0.95             \n",
      "  0    7600        388.63   2127.26   94.55   94.58   94.51    0.95             \n",
      "  0    7800        338.89   2020.42   94.75   94.95   94.55    0.95             \n",
      "  0    8000        370.26   2096.07   95.05   95.61   94.50    0.95             \n",
      "  0    8200        365.72   2155.59   95.06   95.55   94.57    0.95             \n",
      "  0    8400        354.49   2108.03   94.70   94.91   94.49    0.95             \n",
      "  0    8600        406.68   1984.01   94.99   95.14   94.84    0.95             \n",
      "  0    8800        370.07   2035.81   95.16   95.39   94.92    0.95             \n",
      "  0    9000        361.40   1977.33   94.80   95.23   94.38    0.95             \n",
      "  0    9200        348.18   1940.14   95.29   95.98   94.61    0.95             \n",
      "  0    9400        362.10   1879.48   95.10   95.04   95.15    0.95             \n",
      "  0    9600        386.80   1862.42   95.19   95.38   95.00    0.95             \n",
      "  0    9800        408.95   1908.14   95.27   95.60   94.95    0.95             \n",
      "  0   10000        407.60   1907.92   95.40   95.67   95.12    0.95             \n",
      "  0   10200        351.67   1824.61   95.14   94.88   95.40    0.95             \n",
      "  0   10400        407.79   1882.81   94.65   94.17   95.13    0.95             \n",
      "  0   10600        397.75   1856.66   95.35   95.37   95.33    0.95             \n",
      "  0   10800        409.30   1806.87   95.70   96.57   94.84    0.96             \n",
      "  0   11000        377.69   1782.45   95.84   96.20   95.48    0.96             \n",
      "  0   11200        416.20   1873.27   95.71   95.64   95.78    0.96             \n",
      "  0   11400        448.67   1691.54   95.60   96.14   95.05    0.96             \n",
      "  0   11600        520.44   1729.74   95.75   95.98   95.51    0.96             \n",
      "  0   11800        418.40   1597.04   95.60   96.03   95.18    0.96             \n",
      "  0   12000        437.11   1739.76   95.77   96.19   95.35    0.96             \n",
      "Epoch 1:   0%|                                          | 0/200 [00:00<?, ?it/s]\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/cnn_1/model-last\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "!python -m spacy train {CONFIG_PATH / \"config1.cfg\"} \\\n",
    "  --output {SPACY_MODEL_PATH / \"cnn_1\"} \\\n",
    "  --paths.train {SPACY_DATA_PATH / \"train.spacy\"} \\\n",
    "  --paths.dev {SPACY_DATA_PATH / \"val.spacy\"} \\\n",
    "  --gpu-id 0  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b5a5e-0cc1-422c-9697-2de9ed2e4d43",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model   | Dropout | Batch Size | Max Steps | Learn Rate | Loss TOK2VEC | Loss NER | F1   |\n",
    "|---------|---------|------------|-----------|------------|---------------|----------|------|\n",
    "| cnn_0   | 0.5     | 1000       | 2000      | 0.001      | 2275.93       | 8088.55  | 0.86 |\n",
    "| cnn_1   | 0.5     | 500        | 20000     | 0.001      | 437.11        | 1739.76  | 0.96 |\n",
    "\n",
    "The second CNN model (`cnn_1`) was trained using the same encoder architecture as the baseline but with a longer training schedule and smaller batch size. The aim was to give the model more update opportunities and allow it to converge more gradually.\n",
    "\n",
    "Training continued up to step 12000, where early stopping was triggered. By this point, the model had reached a validation F1-score of 95.77%, with precision at 96.19% and recall at 95.35%. This is a clear improvement over the baseline model, which plateaued at 86.03% F1.\n",
    "\n",
    "`LOSS NER` dropped from over 8000 to 1739.76, indicating that the model became much more confident and accurate in its span predictions. `LOSS TOK2VEC` also reduced significantly to 437.11, suggesting that the token embeddings learned more stable and informative representations over time.\n",
    "\n",
    "The smaller batch size (500 vs 1000) allowed for more frequent parameter updates. This helped the model adapt quickly during training, while the longer training horizon gave it time to refine internal representations. The high dropout rate (0.5) helped prevent overfitting even as model performance increased.\n",
    "\n",
    "This result shows that careful tuning of batch size and training schedule can significantly improve model quality, even without changing the encoder architecture or introducing external features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5fcf63-0c26-4331-9472-633c782840ef",
   "metadata": {},
   "source": [
    "### 3.3 SpaCy Model 3: Shallower Encoder with Lower Dropout\n",
    "The third model (`cnn_2`) tests how well a simpler encoder performs when trained over a longer schedule. The number of convolutional layers in the `tok2vec` encoder is reduced from 4 to 2. This change makes the model faster to train and may encourage it to focus on simpler features.\n",
    "\n",
    "The dropout rate is also lowered from 0.5 to 0.3. Since the encoder is smaller, applying strong regularisation could lead to underfitting. A moderate dropout allows the model to retain more training signal while still reducing the risk of overfitting.\n",
    "\n",
    "All other settings are the same as in the previous model: 20,000 maximum steps, early stopping patience of 1000 steps, a batch size of 500, and a learning rate of 0.001.\n",
    "\n",
    "This configuration helps evaluate whether a shallower network with lighter regularisation can match the performance of deeper models trained under similar conditions.\n",
    "\n",
    "| Parameter                     | Value       |\n",
    "|------------------------------|-------------|\n",
    "| `vectors`                    | `null`      |\n",
    "| `init_tok2vec`               | `null`      |\n",
    "| `batch_size`                 | `500`       |\n",
    "| `encoder architecture`       | `MaxoutWindowEncoder.v2` |\n",
    "| `encoder width`              | `128`       |\n",
    "| `encoder depth`              | `2`         |\n",
    "| `dropout`                    | `0.3`       |\n",
    "| `max_steps`                  | `20000`     |\n",
    "| `patience`                   | `1000`      |\n",
    "| `learn_rate` (Adam)          | `0.001`     |\n",
    "| `eval_frequency`             | `200`       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24d41b0-c3da-441a-aca3-4dc9c16e265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/cnn/config2.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config2.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/cnn/config2.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5738b8c8-a0d0-493b-8e26-6ad48bde6e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-01 01:16:19,564] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory: ../models/spaCy/cnn_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/cnn_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-01 01:16:19,874] [INFO] Set up nlp object from config\n",
      "[2025-07-01 01:16:19,882] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-01 01:16:19,883] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-07-01 01:16:19,883] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2025-07-01 01:16:19,886] [INFO] Created vocabulary\n",
      "[2025-07-01 01:16:19,886] [INFO] Finished initializing nlp object\n",
      "[2025-07-01 01:20:19,011] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-01 01:22:21,709] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-01 01:22:21,718] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-01 01:22:21,719] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     38.00    0.00    0.05    0.00    0.00\n",
      "  0     200         74.31   2334.24   24.17   39.30   17.45    0.24             \n",
      "  0     400         87.87   1369.62   55.49   69.21   46.31    0.55             \n",
      "  0     600        141.82   1406.69   67.84   78.45   59.76    0.68             \n",
      "  0     800        467.80   1531.98   75.74   84.62   68.54    0.76             \n",
      "  0    1000        218.95   1594.84   80.53   88.31   74.00    0.81             \n",
      "  0    1200       1499.97   1684.40   83.34   90.28   77.39    0.83             \n",
      "  0    1400        917.39   1874.33   84.84   90.52   79.82    0.85             \n",
      "  0    1600        373.49   1929.19   87.20   90.57   84.07    0.87             \n",
      "  0    1800        479.78   2254.45   88.41   91.61   85.43    0.88             \n",
      "  0    2000        493.34   2417.24   90.16   93.20   87.31    0.90             \n",
      "  0    2200        672.11   2767.25   91.02   92.73   89.37    0.91             \n",
      "  0    2400        700.24   2928.31   92.03   94.22   89.94    0.92             \n",
      "  0    2600        666.03   2718.42   92.33   92.42   92.25    0.92             \n",
      "  0    2800        730.57   2579.67   93.18   94.24   92.15    0.93             \n",
      "  0    3000        701.34   2406.14   93.78   94.83   92.75    0.94             \n",
      "  0    3200        706.01   2360.84   93.73   95.90   91.66    0.94             \n",
      "  0    3400        708.67   2391.44   93.75   93.99   93.50    0.94             \n",
      "  0    3600        633.29   2061.92   94.10   94.55   93.65    0.94             \n",
      "  0    3800        645.44   2096.90   94.49   95.26   93.73    0.94             \n",
      "  0    4000        671.17   2087.22   94.84   95.23   94.45    0.95             \n",
      "  0    4200        629.99   1819.65   95.05   95.31   94.80    0.95             \n",
      "  0    4400        683.26   1833.07   95.07   95.41   94.74    0.95             \n",
      "  0    4600        679.83   1815.86   95.52   96.83   94.25    0.96             \n",
      "  0    4800        674.10   1627.91   95.54   96.06   95.02    0.96             \n",
      "  0    5000        709.00   1710.10   95.43   95.83   95.04    0.95             \n",
      "  0    5200        659.86   1730.48   95.57   95.50   95.64    0.96             \n",
      "  0    5400        709.06   1652.74   95.70   96.45   94.96    0.96             \n",
      "  0    5600        697.52   1566.06   95.90   96.21   95.60    0.96             \n",
      "  0    5800        637.11   1543.62   96.13   96.99   95.29    0.96             \n",
      "  0    6000        661.64   1591.32   95.98   96.54   95.42    0.96             \n",
      "  0    6200        619.65   1492.85   96.12   96.46   95.78    0.96             \n",
      "  0    6400        642.85   1494.36   96.07   96.28   95.86    0.96             \n",
      "  0    6600        658.34   1555.49   96.38   96.91   95.85    0.96             \n",
      "  0    6800        601.26   1362.56   96.24   96.43   96.05    0.96             \n",
      "  0    7000        659.13   1445.13   95.93   96.89   95.00    0.96             \n",
      "  0    7200        635.44   1348.06   96.41   96.72   96.10    0.96             \n",
      "  0    7400        680.76   1378.18   96.53   96.80   96.26    0.97             \n",
      "  0    7600        775.35   1315.64   96.61   97.28   95.95    0.97             \n",
      "  0    7800        584.38   1204.25   96.47   96.54   96.41    0.96             \n",
      "  0    8000        671.67   1265.46   96.54   96.83   96.25    0.97             \n",
      "  0    8200        711.41   1318.28   96.84   97.14   96.55    0.97             \n",
      "  0    8400        637.63   1281.79   96.61   96.96   96.27    0.97             \n",
      "  0    8600        648.64   1194.54   96.82   97.55   96.09    0.97             \n",
      "  0    8800        684.45   1235.89   96.88   97.20   96.56    0.97             \n",
      "  0    9000        624.24   1158.99   96.71   97.01   96.41    0.97             \n",
      "  0    9200        694.13   1227.63   96.87   97.66   96.10    0.97             \n",
      "  0    9400        706.18   1178.43   96.94   97.36   96.53    0.97             \n",
      "  0    9600        701.54   1236.21   96.92   97.62   96.23    0.97             \n",
      "  0    9800        690.38   1200.84   96.99   97.29   96.70    0.97             \n",
      "  0   10000        738.82   1145.52   97.16   97.56   96.76    0.97             \n",
      "  0   10200        771.10   1152.94   97.11   97.15   97.08    0.97             \n",
      "  0   10400        684.12   1092.98   97.18   97.29   97.07    0.97             \n",
      "  0   10600        743.12   1166.58   97.19   97.44   96.95    0.97             \n",
      "  0   10800        679.97   1147.02   97.20   97.31   97.09    0.97             \n",
      "  0   11000        663.37   1073.93   97.32   97.65   96.99    0.97             \n",
      "  0   11200        843.03   1161.77   97.20   97.31   97.09    0.97             \n",
      "  0   11400        760.66   1065.32   97.22   97.75   96.70    0.97             \n",
      "  0   11600        977.89   1119.72   97.46   97.91   97.00    0.97             \n",
      "  0   11800        701.20   1006.07   97.34   97.88   96.82    0.97             \n",
      "  0   12000        672.80   1004.21   97.30   97.52   97.07    0.97             \n",
      "  0   12200        886.67   1061.00   97.26   97.59   96.93    0.97             \n",
      "  0   12400        759.84   1090.25   97.23   97.48   96.97    0.97             \n",
      "  0   12600        790.56   1011.86   97.52   97.78   97.25    0.98             \n",
      "  0   12800        807.55   1040.57   97.50   98.03   96.98    0.97             \n",
      "  0   13000        709.62   1010.10   97.51   97.91   97.12    0.98             \n",
      "  0   13200        783.62   1027.79   97.35   97.53   97.18    0.97             \n",
      "  0   13400        803.74   1034.17   97.47   97.66   97.29    0.97             \n",
      "  0   13600        760.21    925.89   97.52   97.66   97.38    0.98             \n",
      "  0   13800        873.32    994.05   97.32   97.53   97.11    0.97             \n",
      "  0   14000        849.86   1005.25   97.61   97.81   97.42    0.98             \n",
      "  0   14200       1030.51    994.21   97.47   97.70   97.25    0.97             \n",
      "  0   14400        866.74    965.07   97.51   97.65   97.37    0.98             \n",
      "  0   14600        897.34    971.77   97.51   97.61   97.40    0.98             \n",
      "  0   14800        769.19    978.44   97.62   97.97   97.26    0.98             \n",
      "  0   15000        754.71    928.09   97.57   97.83   97.32    0.98             \n",
      "  0   15200        778.71    980.36   97.47   97.68   97.25    0.97             \n",
      "  0   15400        851.89    919.23   97.78   97.86   97.70    0.98             \n",
      "  0   15600        856.17    878.79   97.69   97.89   97.50    0.98             \n",
      "  0   15800        746.08    897.96   97.70   97.82   97.58    0.98             \n",
      "  0   16000        858.33    902.71   97.65   98.24   97.07    0.98             \n",
      "  0   16200        779.54    840.54   97.76   98.02   97.50    0.98             \n",
      "  0   16400       1018.11    988.94   97.70   98.12   97.27    0.98             \n",
      "Epoch 1:   0%|                                          | 0/200 [00:00<?, ?it/s]\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/cnn_2/model-last\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "!python -m spacy train {CONFIG_PATH / \"config2.cfg\"} \\\n",
    "  --output {SPACY_MODEL_PATH / \"cnn_2\"} \\\n",
    "  --paths.train {SPACY_DATA_PATH / \"train.spacy\"} \\\n",
    "  --paths.dev {SPACY_DATA_PATH / \"val.spacy\"} \\\n",
    "  --gpu-id 0  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0e7bd-7eb1-4f46-9bea-550faf3e89df",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model   | Dropout | Batch Size | Max Steps | Learn Rate | Loss TOK2VEC | Loss NER | F1   |\n",
    "|---------|---------|------------|-----------|------------|---------------|----------|------|\n",
    "| cnn_0   | 0.5     | 1000       | 2000      | 0.001      | 2275.93       | 8088.55  | 0.86 |\n",
    "| cnn_1   | 0.5     | 500        | 20000     | 0.001      | 437.11        | 1739.76  | 0.96 |\n",
    "| cnn_2   | 0.3     | 500        | 20000     | 0.001      | 1018.11       | 988.94   | 0.98 |\n",
    "\n",
    "The third CNN model (`cnn_2`) explored whether a shallower encoder and lighter dropout could still deliver strong results. The encoder depth was reduced from 4 to 2 layers, cutting the number of convolutional operations used to learn contextual features. Dropout was lowered from 0.5 to 0.3 to avoid underfitting, as high regularisation can interfere with training in smaller architectures.\n",
    "\n",
    "All other settings remained unchanged from the previous model: 20,000 maximum steps, early stopping with 1000-step patience, a batch size of 500, and a learning rate of 0.001. This allowed for a focused comparison based solely on encoder size and regularisation strength.\n",
    "\n",
    "Training stopped around step 16400 with a final F1-score of 97.70%. `LOSS NER` dropped to 988.94, the lowest among all models so far. `LOSS TOK2VEC` settled at 1018.11, showing the encoder still adapted well even with reduced depth. Precision (98.12%) and recall (97.27%) remained well balanced, confirming that the model generalised without overfitting.\n",
    "\n",
    "These results indicate that even a simplified CNN encoder can produce high-quality entity recognition when trained with sufficient steps. The combination of longer training and a smaller network appears to have stabilised learning while still capturing essential features from the environmental dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf7e0a-c6e3-47ef-8fac-6827c6685dc6",
   "metadata": {},
   "source": [
    "### 3.4 SpaCy Model 4: Shallow Encoder with Pretrained Static Vectors\n",
    "The fourth model (`cnn_3`) builds on the previous configuration by adding pretrained static word vectors to the pipeline. It retains the same shallow encoder (depth of 2) and batch size of 500, but replaces randomly initialised word embeddings with fixed vectors from SpaCy’s `en_core_web_md` model.\n",
    "\n",
    "These static vectors are trained on a large external corpus and provide high-quality lexical representations from the outset. They are not updated during training but serve as a stable foundation for the encoder. For example, even if the term *runoff pollution* does not appear in the training data, the model may still learn to classify it as `POLLUTANT` because *runoff* and *pollution* have similar embeddings to other known entities like *nitrate runoff* or *air pollution*. This setup can help the model recognise semantically similar expressions, which is especially useful in cases where annotated entities are sparse or varied.\n",
    "\n",
    "To avoid disrupting the pretrained embeddings, the learning rate is reduced to 0.0005. Dropout is also lowered slightly to 0.4, based on the assumption that better initial representations reduce the risk of overfitting. All other settings, such as encoder width, patience, and training steps, are left unchanged from the previous model to isolate the effect of static vectors.\n",
    "\n",
    "This configuration tests whether access to high-quality word-level representations improves generalisation in environmental NER, particularly for rare terms not well covered by the training data.\n",
    "\n",
    "\n",
    "| Parameter                     | Value                    |\n",
    "|------------------------------|--------------------------|\n",
    "| `vectors`                    | `en_core_web_md`          |\n",
    "| `init_tok2vec`               | `null`                   |\n",
    "| `batch_size`                 | `500`                    |\n",
    "| `encoder architecture`       | `MaxoutWindowEncoder.v2` |\n",
    "| `encoder width`              | `128`                    |\n",
    "| `encoder depth`              | `2`                      |\n",
    "| `dropout`                    | `0.4`                    |\n",
    "| `max_steps`                  | `20000`                  |\n",
    "| `patience`                   | `1000`                   |\n",
    "| `learn_rate`                 | `0.0005`                 |\n",
    "| `eval_frequency`             | `200`                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d613905-e12f-4eeb-833e-067ab2dbc28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/cnn/config3.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config3.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/cnn/config3.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a8843ef-bcb7-4e5f-9115-86f36243d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-02 11:43:06,109] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/cnn_3\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-02 11:43:08,202] [INFO] Set up nlp object from config\n",
      "[2025-07-02 11:43:08,211] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-02 11:43:08,212] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-07-02 11:43:08,212] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2025-07-02 11:43:08,214] [INFO] Created vocabulary\n",
      "[2025-07-02 11:43:09,565] [INFO] Added vectors: en_core_web_md\n",
      "[2025-07-02 11:43:09,565] [INFO] Finished initializing nlp object\n",
      "[2025-07-02 11:47:10,075] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-02 11:49:13,241] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-02 11:49:13,251] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-02 11:49:13,252] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-07-02 11:49:13,256] [DEBUG] Removed existing output directory: ../models/spaCy/cnn_3/model-best\n",
      "[2025-07-02 11:49:13,258] [DEBUG] Removed existing output directory: ../models/spaCy/cnn_3/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0005\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     38.00    0.00    0.00    0.00    0.00\n",
      "  0     200         72.35   2598.27   13.24   25.16    8.98    0.13\n",
      "  0     400        110.72   1613.95   43.86   56.32   35.91    0.44\n",
      "  0     600        152.92   1699.61   63.12   75.44   54.26    0.63\n",
      "  0     800        204.85   1806.16   68.65   74.60   63.59    0.69\n",
      "  0    1000        245.51   1988.65   75.67   82.12   70.16    0.76\n",
      "  0    1200        372.85   2086.99   77.46   81.09   74.15    0.77\n",
      "  0    1400        730.50   2265.74   81.50   86.55   77.00    0.82\n",
      "  0    1600        531.50   2453.60   84.09   86.51   81.81    0.84\n",
      "  0    1800        885.06   2823.36   85.26   87.09   83.50    0.85\n",
      "  0    2000        607.36   3085.72   88.19   90.22   86.26    0.88\n",
      "  0    2200        682.89   3547.84   87.81   88.71   86.93    0.88\n",
      "  0    2400        821.34   3832.16   90.21   91.32   89.13    0.90\n",
      "  0    2600        672.87   3525.27   90.63   90.58   90.68    0.91\n",
      "  0    2800        673.69   3289.71   91.14   91.32   90.95    0.91\n",
      "  0    3000        664.36   3090.69   91.85   92.26   91.45    0.92\n",
      "  0    3200        658.42   3059.88   92.22   92.38   92.06    0.92\n",
      "  0    3400        623.49   2959.85   92.49   92.53   92.46    0.92\n",
      "  0    3600        593.91   2647.19   92.39   92.08   92.71    0.92\n",
      "  0    3800        661.74   2601.88   93.06   92.79   93.33    0.93\n",
      "  0    4000        704.25   2650.95   93.41   93.69   93.13    0.93\n",
      "  0    4200        617.18   2459.82   93.38   93.34   93.43    0.93\n",
      "  0    4400        643.51   2418.59   93.65   93.15   94.16    0.94\n",
      "  0    4600        586.10   2304.22   94.38   95.03   93.74    0.94\n",
      "  0    4800        599.47   2168.65   94.22   94.33   94.10    0.94\n",
      "  0    5000        594.05   2170.10   94.23   94.16   94.29    0.94\n",
      "  0    5200        612.48   2175.58   94.67   94.87   94.48    0.95\n",
      "  0    5400        578.25   2099.24   94.20   94.02   94.39    0.94\n",
      "  0    5600        604.14   2069.43   94.80   94.91   94.69    0.95\n",
      "  0    5800        584.62   2020.27   94.82   94.79   94.84    0.95\n",
      "  0    6000        563.87   1983.31   94.88   94.63   95.14    0.95\n",
      "  0    6200        547.08   1921.72   95.05   94.89   95.21    0.95\n",
      "  0    6400        582.03   1860.30   94.46   93.76   95.16    0.94\n",
      "  0    6600        578.50   1876.93   95.18   94.83   95.55    0.95\n",
      "  0    6800        556.42   1812.73   95.07   94.77   95.37    0.95\n",
      "  0    7000        588.99   1711.34   95.19   95.56   94.82    0.95\n",
      "  0    7200        588.78   1763.68   95.37   95.29   95.46    0.95\n",
      "  0    7400        549.55   1728.10   95.75   95.88   95.61    0.96\n",
      "  0    7600        570.35   1759.82   95.82   96.13   95.52    0.96\n",
      "  0    7800        534.66   1570.94   95.53   95.51   95.55    0.96\n",
      "  0    8000        577.24   1669.21   95.47   95.26   95.68    0.95\n",
      "  0    8200        526.96   1663.94   95.82   95.73   95.91    0.96\n",
      "  0    8400        556.16   1612.99   95.84   96.05   95.63    0.96\n",
      "  0    8600        565.10   1574.53   95.79   95.82   95.75    0.96\n",
      "  0    8800        608.25   1595.79   96.18   96.40   95.97    0.96\n",
      "  0    9000        526.39   1565.71   95.90   95.80   95.99    0.96\n",
      "  0    9200        532.80   1536.02   96.18   96.50   95.86    0.96\n",
      "  0    9400        528.62   1438.27   96.20   96.32   96.08    0.96\n",
      "  0    9600        587.78   1500.58   96.11   96.15   96.08    0.96\n",
      "  0    9800        601.05   1484.68   96.16   96.46   95.86    0.96\n",
      "  0   10000        571.15   1490.03   96.18   96.24   96.12    0.96\n",
      "  0   10200        550.67   1353.35   95.97   95.45   96.49    0.96\n",
      "  0   10400        626.23   1511.57   96.11   95.83   96.39    0.96\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/cnn_3/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train {CONFIG_PATH / \"config3.cfg\"} \\\n",
    "  --output {SPACY_MODEL_PATH / \"cnn_3\"} \\\n",
    "  --paths.train {SPACY_DATA_PATH / \"train.spacy\"} \\\n",
    "  --paths.dev {SPACY_DATA_PATH / \"val.spacy\"} \\\n",
    "  --gpu-id 0  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c470f2b-9d37-4444-bcd4-c16af5bfa208",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model   | Dropout | Batch Size | Max Steps | Learn Rate | Loss TOK2VEC | Loss NER | F1   |\n",
    "|---------|---------|------------|-----------|------------|--------------|----------|------|\n",
    "| cnn_0   | 0.5     | 1000       | 2000      | 0.001      | 2275.93      | 8088.55  | 0.86 |\n",
    "| cnn_1   | 0.5     | 500        | 20000     | 0.001      | 437.11       | 1739.76  | 0.96 |\n",
    "| cnn_2   | 0.3     | 500        | 20000     | 0.001      | 1018.11      | 988.94   | 0.98 |\n",
    "| cnn_3   | 0.4     | 500        | 20000     | 0.0005     | 626.23       | 1353.35  | 0.96 |\n",
    "\n",
    "The fourth model (`cnn_3`) adds static pretrained word vectors from `en_core_web_md` to the previous architecture. This is the first model in the series to use external lexical knowledge. By replacing randomly initialised token embeddings with fixed vectors trained on a large external corpus, the model begins training with richer word-level representations.\n",
    "\n",
    "The encoder remains shallow with depth 2 and width 128, and the dropout is moderately set to 0.4. These choices reflect a trade-off between regularisation and training signal strength, especially since the static vectors are not updated during training. The learning rate is lowered to 0.0005 to avoid destabilising the pretrained embeddings, and early stopping is used to prevent overfitting.\n",
    "\n",
    "By step 10400, the model reached an F1-score of 96.20% on the validation set. The NER loss dropped to 1353.35, and the token-to-vector loss stabilised around 600. Compared to the baseline, this is a major improvement, though the gains over `cnn_2` are smaller.\n",
    "\n",
    "Overall, `cnn_3` confirms that static vectors can support strong performance in environmental NER, particularly in cases where entity types are semantically similar or rare. While the final F1 is comparable to `cnn_2`, the consistent precision and recall values suggest improved generalisation. This makes pretrained vectors a practical option for lightweight neural models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58b951-4e89-4c00-92de-793c633b2ff5",
   "metadata": {},
   "source": [
    "### 3.5 Choosing the Best SpaCy CNN Model\n",
    "After training and comparing four configurations of SpaCy’s CNN pipeline, `cnn_2` emerges as the most effective model. It achieves the highest validation F1-score (98.0%) and the lowest NER loss, using a shallower encoder (depth 2) and moderate dropout (0.3). The model performs consistently across evaluation steps and maintains a strong balance between precision and recall.\n",
    "\n",
    "Although `cnn_3` introduced pretrained static vectors (`en_core_web_md`), this did not result in better performance. These vectors are not fine-tuned during training and are likely not well-aligned with the domain-specific vocabulary used in environmental texts. This may have limited their ability to improve entity recognition beyond what the purely learned embeddings in `cnn_2` achieved.\n",
    "\n",
    "No further CNN models are trained, as the results already demonstrate strong generalisation. Given the high scores and stable validation performance, `cnn_2` is selected for final evaluation.\n",
    "\n",
    "To prepare the model for testing, it is retrained using the combined training and validation sets. The original test set remains untouched and is used solely for evaluation in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e918565-c908-48a9-88d7-0711448e2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docbin = DocBin().from_disk(SPACY_DATA_PATH / \"train.spacy\")\n",
    "val_docbin = DocBin().from_disk(SPACY_DATA_PATH / \"val.spacy\")\n",
    "\n",
    "combined_docbin = DocBin()\n",
    "for doc in train_docbin.get_docs(spacy.blank(\"en\").vocab):\n",
    "    combined_docbin.add(doc)\n",
    "for doc in val_docbin.get_docs(spacy.blank(\"en\").vocab):\n",
    "    combined_docbin.add(doc)\n",
    "\n",
    "# Save combined file\n",
    "combined_docbin.to_disk(SPACY_DATA_PATH / \"train_val.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d795ae2-6516-411e-aff5-a69d4e2347e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-02 18:01:54,861] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory: ../models/spaCy/cnn_best\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/cnn_best\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-02 18:01:56,842] [INFO] Set up nlp object from config\n",
      "[2025-07-02 18:01:56,851] [DEBUG] Loading corpus from path: ../data/spaCy/test.spacy\n",
      "[2025-07-02 18:01:56,852] [DEBUG] Loading corpus from path: ../data/spaCy/train_val.spacy\n",
      "[2025-07-02 18:01:56,852] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2025-07-02 18:01:56,854] [INFO] Created vocabulary\n",
      "[2025-07-02 18:01:56,854] [INFO] Finished initializing nlp object\n",
      "[2025-07-02 18:07:08,409] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-02 18:09:52,256] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-02 18:09:52,266] [DEBUG] Loading corpus from path: ../data/spaCy/test.spacy\n",
      "[2025-07-02 18:09:52,267] [DEBUG] Loading corpus from path: ../data/spaCy/train_val.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     38.09    0.00    0.00    0.00    0.00\n",
      "  0     200         77.41   2403.50   20.46   31.42   15.17    0.20             \n",
      "  0     400         84.31   1510.30   52.68   77.95   39.78    0.53             \n",
      "  0     600        343.80   1531.79   66.96   76.58   59.49    0.67             \n",
      "  0     800        167.91   1438.74   75.16   83.89   68.07    0.75             \n",
      "  0    1000        542.64   1662.65   79.66   84.34   75.47    0.80             \n",
      "  0    1200        323.67   1662.64   83.60   87.58   79.97    0.84             \n",
      "  0    1400        324.73   1792.13   85.87   90.33   81.82    0.86             \n",
      "  0    1600        343.43   1928.96   87.75   91.16   84.59    0.88             \n",
      "  0    1800        437.20   2270.48   89.24   92.34   86.34    0.89             \n",
      "  0    2000        494.17   2332.94   90.48   92.91   88.17    0.90             \n",
      "  0    2200        601.71   2714.88   91.08   93.48   88.80    0.91             \n",
      "  0    2400        670.18   2952.39   92.15   94.16   90.23    0.92             \n",
      "  0    2600        650.96   2624.48   92.56   94.00   91.16    0.93             \n",
      "  0    2800        627.85   2579.53   93.31   94.82   91.84    0.93             \n",
      "  0    3000        669.56   2387.30   94.04   95.56   92.56    0.94             \n",
      "  0    3200        607.45   2226.59   94.20   94.96   93.45    0.94             \n",
      "  0    3400        674.28   2123.12   94.19   95.46   92.95    0.94             \n",
      "  0    3600        646.75   2067.25   94.20   94.92   93.49    0.94             \n",
      "  0    3800        570.43   1858.25   94.38   94.65   94.11    0.94             \n",
      "  0    4000        653.32   1948.45   94.70   95.50   93.91    0.95             \n",
      "  0    4200        737.16   1913.98   94.99   95.45   94.54    0.95             \n",
      "  0    4400        602.68   1832.04   94.97   95.78   94.17    0.95             \n",
      "  0    4600        651.40   1711.43   95.30   95.65   94.95    0.95             \n",
      "  0    4800        569.04   1697.88   95.46   95.80   95.12    0.95             \n",
      "  0    5000        608.79   1634.32   95.34   96.25   94.44    0.95             \n",
      "  0    5200        663.55   1756.98   95.53   95.96   95.10    0.96             \n",
      "  0    5400        641.65   1648.72   95.51   95.97   95.05    0.96             \n",
      "  0    5600        593.00   1547.97   95.44   96.56   94.34    0.95             \n",
      "  0    5800        576.32   1452.42   95.94   96.59   95.30    0.96             \n",
      "  0    6000        650.24   1567.37   96.06   96.74   95.39    0.96             \n",
      "  0    6200        639.87   1516.03   95.86   96.43   95.29    0.96             \n",
      "  0    6400        609.85   1437.91   96.12   96.78   95.47    0.96             \n",
      "  0    6600        611.18   1413.86   96.34   96.85   95.83    0.96             \n",
      "  0    6800        645.16   1428.32   96.29   96.62   95.96    0.96             \n",
      "  0    7000        579.99   1421.48   96.29   96.37   96.20    0.96             \n",
      "  0    7200        696.47   1510.77   96.50   97.07   95.94    0.97             \n",
      "  0    7400        633.04   1359.03   96.26   96.68   95.85    0.96             \n",
      "  0    7600        550.24   1348.22   96.48   96.74   96.22    0.96             \n",
      "  0    7800        669.39   1379.85   96.31   96.38   96.25    0.96             \n",
      "  0    8000        624.63   1332.64   96.50   96.59   96.41    0.96             \n",
      "  0    8200        558.78   1250.21   96.52   96.64   96.41    0.97             \n",
      "  0    8400        638.58   1308.78   96.78   97.51   96.06    0.97             \n",
      "  0    8600        666.00   1214.59   96.61   96.45   96.77    0.97             \n",
      "  0    8800        624.38   1206.17   96.64   97.04   96.25    0.97             \n",
      "  0    9000        602.29   1153.87   96.87   97.06   96.68    0.97             \n",
      "  0    9200        666.31   1257.45   96.79   96.97   96.60    0.97             \n",
      "  0    9400        634.10   1184.58   96.75   97.17   96.34    0.97             \n",
      "  0    9600        672.00   1102.19   96.87   96.98   96.77    0.97             \n",
      "  0    9800        663.60   1225.91   96.98   97.44   96.52    0.97             \n",
      "  0   10000        638.17   1188.98   96.88   97.30   96.47    0.97             \n",
      "  0   10200        617.70   1147.60   96.97   97.15   96.79    0.97             \n",
      "  0   10400        617.23   1125.42   97.18   97.33   97.04    0.97             \n",
      "  0   10600        703.77   1114.73   97.24   97.34   97.14    0.97             \n",
      "  0   10800        788.17   1166.62   97.05   97.31   96.79    0.97             \n",
      "  0   11000        780.65   1169.87   97.20   97.22   97.19    0.97             \n",
      "  0   11200        708.21   1030.14   97.17   97.29   97.05    0.97             \n",
      "  0   11400        811.00   1098.77   97.10   97.54   96.66    0.97             \n",
      "  0   11600        858.41   1042.48   97.14   97.30   96.99    0.97             \n",
      "Epoch 1:   0%|                                          | 0/200 [00:00<?, ?it/s]\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/cnn_best/model-last\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "!python -m spacy train {CONFIG_PATH / \"config2.cfg\"} \\\n",
    "  --output {SPACY_MODEL_PATH / \"cnn_best\"} \\\n",
    "  --paths.train {SPACY_DATA_PATH / \"train_val.spacy\"} \\\n",
    "  --paths.dev {SPACY_DATA_PATH / \"test.spacy\"} \\\n",
    "  --gpu-id 0 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349e77d-50a0-4241-9fb8-1f7e06529a43",
   "metadata": {},
   "source": [
    "### 3.6 Evaluating on the Test Set\n",
    "\n",
    "This section evaluates the final CNN model on the held-out test set to assess its generalisation performance. The goal is to verify how well the model handles unseen sentences and whether the patterns learned during training transfer effectively to new contexts.\n",
    "\n",
    "Evaluation is performed by applying the trained model to the test data, extracting predicted entity spans, and comparing them with the gold-standard annotations. The evaluation includes:\n",
    "\n",
    "- A confusion matrix showing common misclassifications\n",
    "- Per-label F1 scores to examine entity-wise accuracy\n",
    "- A ranked list of the most frequently misclassified entity types\n",
    "- Qualitative predictions on raw unseen text samples\n",
    "\n",
    "This will help identify whether specific entity types (e.g. `ENV_PROCESS`, `MEASUREMENT`) are more prone to errors and whether the model captures domain-specific vocabulary and structure effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1b73f-f229-4607-8c6c-eb80c6409e3f",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "The confusion matrix below visualises the model’s classification behaviour across all entity types. Each cell shows the number of times one label was predicted as another, allowing a closer look at common misclassifications.\n",
    "\n",
    "High values along the diagonal indicate correct predictions, while off-diagonal values reveal confusion between entity types. This can highlight weaknesses in boundary detection or semantic overlap, particularly in closely related categories such as `ENV_PROCESS` and `POLLUTANT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f5cec1-f662-44c2-98f1-0e03aadfbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(SPACY_MODEL_PATH / \"cnn_best\" / \"model-best\")\n",
    "\n",
    "# Load the test data\n",
    "doc_bin = DocBin().from_disk(SPACY_DATA_PATH / \"test.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e84525a-9c6a-44f8-bed6-f90553b831e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true and predicted entity labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for doc in docs:\n",
    "    pred_doc = nlp(doc.text)\n",
    "\n",
    "    true_ents = {(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents}\n",
    "    pred_ents = {(ent.start_char, ent.end_char, ent.label_) for ent in pred_doc.ents}\n",
    "\n",
    "    all_spans = true_ents.union(pred_ents)\n",
    "\n",
    "    for start, end, label in all_spans:\n",
    "        true_label = label if (start, end, label) in true_ents else \"O\"\n",
    "        pred_label = label if (start, end, label) in pred_ents else \"O\"\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24ee702-bc05-492c-a75a-466fb6e8d3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get sorted list of all labels including \"O\" (non-entity)\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix for Final CNN Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db5e58-17e3-4450-b285-a1a54edf9c42",
   "metadata": {},
   "source": [
    "The confusion matrix reveals that the final CNN model performs consistently well across all six entity types. The majority of predictions fall along the diagonal, indicating that the model correctly identifies most entities with high precision and recall.\n",
    "\n",
    "The strongest results are seen in high-frequency classes such as `ENV_PROCESS`, `HABITAT`, and `TAXONOMY`, each with over 35,000–54,000 correct predictions and minimal confusion. For instance, `HABITAT` achieves over 54,700 accurate predictions with only 622 tokens misclassified as O, suggesting robust span detection and label assignment.\n",
    "\n",
    "`MEASUREMENT` is also classified reliably, although 398 entities are mislabelled as O. This shows the model occasionally misses short numerical phrases or uncertain measurement formats, but overall maintains strong performance with 14,245 correct predictions.\n",
    "\n",
    "The non-entity label `O` presents a few areas of confusion. Some genuine entities are misclassified as `O`, especially for structurally ambiguous or boundary-sensitive types like `TAXONOMY` and `ENV_PROCESS`. Notably, over 2,600 `TAXONOMY` entities and 1,100 `ENV_PROCESS` tokens are misclassified as `O`, which could suggest boundary inconsistencies or context sparsity in the training data.\n",
    "\n",
    "Inter-entity confusion is relatively rare, with very few cases where one entity is mistaken for a different entity class. The most notable misclassification is between `ENV_PROCESS` and `O`, reinforcing the idea that low-confidence environmental processes may occasionally be overlooked.\n",
    "\n",
    "Overall, the CNN model demonstrates stable, well-balanced behaviour across all categories. It handles high-volume and rare entities alike without introducing significant bias or collapse into overprediction. These results confirm the model’s suitability as a reliable baseline for domain-specific entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791eddc-c954-4169-96c6-b065df5f93ed",
   "metadata": {},
   "source": [
    "#### Per-label F1 score\n",
    "This section reports the F1 score for each entity type to assess how well the final CNN model performs across different labels. While the overall F1 provides a general view, per-label scores reveal class-specific strengths and weaknesses. This is useful for understanding whether certain entities, like `TAXONOMY` or `MEASUREMENT`, are more challenging than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c567294c-157a-4c59-9a2b-39b5decb3d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277300/2912190074.py:11: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette=\"Blues_d\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "f1_scores = {\n",
    "    label: score[\"f1-score\"]\n",
    "    for label, score in report.items()\n",
    "    if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "}\n",
    "\n",
    "# Plot F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette=\"Blues_d\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Entity Label\")\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.title(\"Per-label F1 Scores – Final CNN Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea07f7-82da-45fa-8003-f1d17b1ecbff",
   "metadata": {},
   "source": [
    "The bar chart shows that the final CNN model performs consistently well across all entity types. Each category, including `ENV_PROCESS`, `HABITAT`, `MEASUREMENT`, `POLLUTANT`, and `TAXONOMY`, achieves high F1 scores, indicating strong precision and recall.\n",
    "\n",
    "`HABITAT` and `ENV_PROCESS` are classified most accurately, suggesting that the model handles environmental and locational terms reliably. `POLLUTANT` also achieves strong results, reflecting its ability to detect chemical or contaminant-related spans.\n",
    "\n",
    "Despite their lexical variation, `TAXONOMY` entities are recognised with high confidence. The model appears to generalise well across biological names. `MEASUREMENT` entities, which often include numeric values and units, are also identified accurately, confirming that the model can learn structured patterns effectively.\n",
    "\n",
    "These per-label scores reinforce the overall performance observed earlier. The model maintains robust classification across different entity types and is well-suited for comparison with transformer-based approaches in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e059df-f785-4e12-ac80-56e0a0a92cd9",
   "metadata": {},
   "source": [
    "#### Top Misclassified Labels\n",
    "This section identifies the most frequently confused entity labels by comparing the model’s predictions with the gold-standard spans. It helps reveal patterns in misclassification, such as overlaps between related entity types or consistent errors in boundary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3405315b-5b6c-4a91-9fff-7ae910042a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 misclassified label pairs (true → predicted):\n",
      "\n",
      "TAXONOMY → O: 2613 times\n",
      "O → TAXONOMY: 1786 times\n",
      "ENV_PROCESS → O: 1118 times\n",
      "O → MEASUREMENT: 993 times\n",
      "O → ENV_PROCESS: 895 times\n",
      "O → HABITAT: 764 times\n",
      "HABITAT → O: 622 times\n",
      "MEASUREMENT → O: 398 times\n",
      "POLLUTANT → O: 323 times\n",
      "O → POLLUTANT: 284 times\n"
     ]
    }
   ],
   "source": [
    "# Identify all misclassified spans\n",
    "misclassified = [(true, pred) for true, pred in zip(y_true, y_pred) if true != pred]\n",
    "\n",
    "# Count and display the top 10 misclassified label pairs\n",
    "misclassified_counts = Counter(misclassified)\n",
    "top_misclassified = misclassified_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 misclassified label pairs (true → predicted):\\n\")\n",
    "for (true_label, pred_label), count in top_misclassified:\n",
    "    print(f\"{true_label} → {pred_label}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53205b-793b-4c68-84fd-7194165324be",
   "metadata": {},
   "source": [
    "The most frequent misclassification involves `TAXONOMY` being predicted as `O` (2,613 times), indicating that the model occasionally fails to recognise biological entity spans. Similarly, the reverse pattern (`O` → `TAXONOMY`, 1,786 times) shows a tendency to over-predict taxonomy labels, suggesting some false positives in unlabelled text.\n",
    "\n",
    "Confusion between `ENV_PROCESS` and `O` appears in both directions, with over 2,000 instances split across `ENV_PROCESS` → `O` and `O` → `ENV_PROCESS`. This implies that environmental process terms may lack strong contextual cues, leading to inconsistent boundary detection.\n",
    "\n",
    "Errors involving `MEASUREMENT`, `HABITAT`, and `POLLUTANT` also follow a similar pattern. For example, `O` → `MEASUREMENT` and `MEASUREMENT` → `O` suggest difficulty in identifying numeric or unit-based expressions unless they are strongly structured. `HABITAT` is confused in both directions as well, although less frequently.\n",
    "\n",
    "Overall, most errors involve incorrect classification into or out of the O class. This reflects the challenge of distinguishing entity spans in ambiguous or sparse contexts. However, these patterns are not severe and remain well-contained, further supporting the stability of the final CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4328057-ac73-4a2c-95e2-5507c84f3cfa",
   "metadata": {},
   "source": [
    "#### Predicting Entities in a Raw Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ce3ac8-837a-4e00-95b6-bbc6464038f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During             \n",
      "the                \n",
      "long               \n",
      "-                  \n",
      "term               \n",
      "monitoring         \n",
      "of                 \n",
      "freshwater         \n",
      "ecosystems         HABITAT\n",
      "in                 \n",
      "the                \n",
      "upper              \n",
      "Thames             \n",
      "basin              \n",
      ",                  \n",
      "elevated           \n",
      "concentrations     \n",
      "of                 \n",
      "nitrates           POLLUTANT\n",
      "(                  \n",
      "above              \n",
      "50                 \n",
      "mg                 MEASUREMENT\n",
      "/                  MEASUREMENT\n",
      "L                  MEASUREMENT\n",
      ")                  \n",
      "and                \n",
      "phosphorus         POLLUTANT\n",
      "compounds          \n",
      "were               \n",
      "detected           \n",
      ",                  \n",
      "particularly       \n",
      "near               \n",
      "agricultural       \n",
      "runoff             \n",
      "zones              \n",
      "and                \n",
      "peatland           \n",
      "catchments         \n",
      ".                  \n",
      "The                \n",
      "decline            \n",
      "of                 \n",
      "apple              TAXONOMY\n",
      "crab               \n",
      "populations        \n",
      "was                \n",
      "observed           \n",
      "concurrently       \n",
      "with               \n",
      "increased          \n",
      "sedimentation      \n",
      ",                  \n",
      "urban              \n",
      "expansion          \n",
      ",                  \n",
      "and                \n",
      "rising             \n",
      "temperatures       MEASUREMENT\n",
      "due                \n",
      "to                 \n",
      "climate            ENV_PROCESS\n",
      "change             ENV_PROCESS\n",
      ".                  \n",
      "In                 \n",
      "2021               \n",
      ",                  \n",
      "the                \n",
      "average            \n",
      "air                \n",
      "temperature        MEASUREMENT\n",
      "in                 \n",
      "deciduous          \n",
      "woodland           \n",
      "habitats           HABITAT\n",
      "exceeded           \n",
      "18                 \n",
      "°                  MEASUREMENT\n",
      "C                  MEASUREMENT\n",
      "contributing       \n",
      "to                 \n",
      "a                  \n",
      "shift              \n",
      "in                 \n",
      "breeding           \n",
      "cycles             \n",
      "among              \n",
      "amphibians         TAXONOMY\n",
      "like               \n",
      "rana               \n",
      "temporaria         \n",
      ".                  \n"
     ]
    }
   ],
   "source": [
    "sentence = (\n",
    "    \"During the long-term monitoring of freshwater ecosystems in the upper Thames basin, \"\n",
    "    \"elevated concentrations of nitrates (above 50 mg/L) and phosphorus compounds were detected, \"\n",
    "    \"particularly near agricultural runoff zones and peatland catchments. \"\n",
    "    \"The decline of apple crab populations was observed concurrently with increased sedimentation, \"\n",
    "    \"urban expansion, and rising temperatures due to climate change. \"\n",
    "    \"In 2021, the average air temperature in deciduous woodland habitats exceeded 18 °C \"\n",
    "    \"contributing to a shift in breeding cycles among amphibians like rana temporaria.\"\n",
    ")\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    ent_label = token.ent_type_ if token.ent_type_ else \"\"\n",
    "    print(f\"{token.text:<18} {ent_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb0118-380e-4df3-85d8-615bf5ec92c8",
   "metadata": {},
   "source": [
    "The CNN model correctly identified a broad range of entities in the sample passage, including:\n",
    "\n",
    "- `ecosystems` and `woodland habitats` as `HABITAT`\n",
    "- `nitrates` and `phosphorus` as `POLLUTANT`\n",
    "- `apple crab` and `amphibians` as `TAXONOMY`\n",
    "- `climate change` as `ENV_PROCESS`\n",
    "- `mg/L`, `temperature`, and `°C` as `MEASUREMENT`\n",
    "\n",
    "Some entities were missed, including:\n",
    "\n",
    "- `peatland catchments`, which was not recognised as `HABITAT`\n",
    "- `rana temporaria`, which was not detected as `TAXONOMY`\n",
    "\n",
    "Overall, the model generalised well to this realistic passage containing domain-specific language and entity types. It captured key vocabulary and label boundaries with consistency, particularly for measurements and common pollutant terms. Most errors involved rarer or multi-word habitat and species expressions, which remain challenging under limited supervision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e451ee1-df6c-4aef-a574-6e56198b35c2",
   "metadata": {},
   "source": [
    "## 4. Transformer-based NER Models\n",
    "This section explores transformer-based architectures as the final stage in the environmental NER pipeline. Transformers are widely regarded as the state-of-the-art in NLP due to their ability to model contextual relationships across entire sequences using self-attention. This enables each token to attend to others in the sentence, improving span detection and disambiguation.\n",
    "\n",
    "Unlike SpaCy’s CNN models, which operate on local windows and rely on static token embeddings, transformers generate contextual embeddings that adapt to each sentence. These representations are particularly useful in recognising rare or unseen entities, as the model can infer semantics from context. For example, even if a term like *\"climatic oscillation\"* does not appear in the training data, a transformer can interpret it accurately based on its components.\n",
    "\n",
    "While powerful, transformer models are significantly more demanding in terms of memory and compute. Even compact variants can exceed the limits of modest hardware. To manage this, several lightweight transformer models were considered, including:\n",
    "\n",
    "- `google/bert_uncased_L-2_H-128_A-2` (also known as **BERT-Tiny**)  \n",
    "- `nreimers/BERT-Tiny_L-2_H-128_A-2` (a mirrored version maintained by SentenceTransformers)\n",
    "\n",
    "However, these configurations were found to be unstable or incompatible during training on a GTX 1650 GPU with 4GB VRAM. Due to these limitations, the `prajjwal1/bert-tiny` model was selected as the primary backbone. It offers a reliable trade-off between performance and resource efficiency, enabling smooth integration with SpaCy’s `spacy-transformers` pipeline.\n",
    "\n",
    "SpaCy supports transformer models via the `transformer` and `TransformerListener` components, replacing `tok2vec` and enabling contextual features to flow into the NER classifier. The rest of the training setup, including optimiser behaviour and evaluation logic, remains aligned with the previous CNN-based experiments.\n",
    "\n",
    "The goal is to assess whether pretrained transformer models improve generalisation and entity boundary detection when trained on weakly labelled environmental data. These models are expected to serve as strong benchmarks for domain-specific NER tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d85b4-d333-4929-af39-a72fdbf044d6",
   "metadata": {},
   "source": [
    "### 4.1 Training the Baseline Transformer Model\n",
    "The baseline transformer model uses a compact pretrained encoder integrated into SpaCy’s pipeline. No weights are manually initialised, and the model learns to perform NER by fine-tuning contextual embeddings directly during training.\n",
    "\n",
    "The pipeline includes the components `transformer`, `sentencizer`, and `ner`. Sentence boundaries are handled by the `sentencizer`, and overlapping transformer spans are processed using a striding window approach. Training employs learning rate warmup, early stopping, and gradient accumulation to stabilise convergence and make effective use of GPU resources.\n",
    "\n",
    "The table below summarises the key configuration choices:\n",
    "\n",
    "| **Parameter**              | **Value**                 | **Purpose**                                                     |\n",
    "|---------------------------|---------------------------|-----------------------------------------------------------------|\n",
    "| `transformer model`       | `prajjwal1/bert-tiny`     | Lightweight HuggingFace model for contextual embeddings         |\n",
    "| `span getter`             | `strided_spans.v1`        | Splits input into overlapping spans to support long sequences   |\n",
    "| `dropout`                 | `0.1`                     | Standard regularisation during training                         |\n",
    "| `accumulate_gradient`     | `3`                       | Accumulates gradients to simulate larger batches                |\n",
    "| `batch size`              | `4`                     | Number of padded tokens per batch                               |\n",
    "| `initial learning rate`   | `0.00005`                 | Learning rate with linear warmup scheduler                      |\n",
    "| `warmup steps`            | `250`                     | Gradual increase in learning rate during early steps            |\n",
    "| `max steps`               | `20000`                   | Maximum number of training steps                                |\n",
    "| `patience`                | `1600`                    | Early stopping patience if validation score does not improve    |\n",
    "| `eval frequency`          | `200`                     | Evaluation interval on the validation set                       |\n",
    "\n",
    "This configuration serves as a lightweight baseline to assess whether transformer-based embeddings improve environmental NER performance over previous CNN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa456ca-ce79-4114-b2f1-9f59a52d95ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: sentencizer, ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/transformer/config0.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config0.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/transformer/config0.cfg \\\n",
    "    --lang en \\\n",
    "    --pipeline sentencizer,transformer,ner \\\n",
    "    --optimize accuracy \\\n",
    "    --force \\\n",
    "    --gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af68642d-83f1-42b8-91d2-1e863d5f67ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-02 23:33:27,050] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/transformer_0\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-02 23:33:29,842] [INFO] Set up nlp object from config\n",
      "[2025-07-02 23:33:29,851] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-02 23:33:29,852] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-07-02 23:33:29,852] [INFO] Pipeline: ['sentencizer', 'transformer', 'ner']\n",
      "[2025-07-02 23:33:29,854] [INFO] Created vocabulary\n",
      "[2025-07-02 23:33:29,855] [INFO] Finished initializing nlp object\n",
      "[2025-07-02 23:35:30,822] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-02 23:37:34,694] [INFO] Initialized pipeline components: ['sentencizer', 'transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-02 23:37:34,704] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-02 23:37:34,705] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['sentencizer', 'transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Set annotations on update for: ['sentencizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  SENTS_F  SENTS_P  SENTS_R  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0         107.41    198.69   100.00   100.00   100.00    1.73    0.92   14.71    0.51\n",
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 512], strides() = [1, 128]\n",
      "param.sizes() = [128, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:218.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0     200       28442.63  46345.71   100.00   100.00   100.00    0.00    0.00    0.00    0.50\n",
      "  0     400        2976.54   6188.32   100.00   100.00   100.00   47.62   75.75   34.72    0.74\n",
      "  0     600        2572.77   4681.60   100.00   100.00   100.00   63.22   82.00   51.44    0.82\n",
      "  0     800        2264.89   3847.83   100.00   100.00   100.00   71.71   84.94   62.04    0.86\n",
      "  0    1000        2194.54   3474.28   100.00   100.00   100.00   74.64   86.78   65.48    0.87\n",
      "  0    1200        2144.43   3202.47   100.00   100.00   100.00   77.36   86.16   70.20    0.89\n",
      "  0    1400        2022.99   2845.88   100.00   100.00   100.00   79.82   85.77   74.65    0.90\n",
      "  0    1600        1974.92   2695.30   100.00   100.00   100.00   81.08   88.43   74.86    0.91\n",
      "  0    1800        1996.56   2541.34   100.00   100.00   100.00   81.98   88.26   76.53    0.91\n",
      "  0    2000        4893.40   2562.62   100.00   100.00   100.00   83.16   89.09   77.97    0.92\n",
      "  0    2200        4005.49   2467.73   100.00   100.00   100.00   84.04   89.27   79.39    0.92\n",
      "  0    2400        5308.51   2369.05   100.00   100.00   100.00   84.42   89.97   79.51    0.92\n",
      "  0    2600        6171.99   2329.07   100.00   100.00   100.00   85.26   89.53   81.38    0.93\n",
      "  0    2800        4440.77   2223.56   100.00   100.00   100.00   85.66   89.85   81.84    0.93\n",
      "  0    3000        6129.69   2020.38   100.00   100.00   100.00   85.92   90.98   81.39    0.93\n",
      "  0    3200        2286.44   1930.96   100.00   100.00   100.00   86.44   89.08   83.96    0.93\n",
      "  0    3400        1941.10   1896.89   100.00   100.00   100.00   87.32   91.37   83.61    0.94\n",
      "  0    3600        5183.51   1855.27   100.00   100.00   100.00   88.32   92.06   84.88    0.94\n",
      "  0    3800        2313.84   1678.23   100.00   100.00   100.00   88.63   91.44   86.00    0.94\n",
      "  0    4000        2375.37   1699.07   100.00   100.00   100.00   89.13   91.77   86.64    0.95\n",
      "  0    4200        1468.79   1502.51   100.00   100.00   100.00   89.27   92.57   86.20    0.95\n",
      "  0    4400        1420.27   1494.37   100.00   100.00   100.00   89.65   91.53   87.86    0.95\n",
      "  0    4600        1386.20   1464.08   100.00   100.00   100.00   89.87   92.96   86.98    0.95\n",
      "  0    4800        1294.06   1352.25   100.00   100.00   100.00   90.04   92.94   87.33    0.95\n",
      "  0    5000        1417.33   1439.70   100.00   100.00   100.00   90.18   92.42   88.05    0.95\n",
      "  0    5200        1344.45   1376.05   100.00   100.00   100.00   90.59   92.55   88.72    0.95\n",
      "  0    5400        1158.19   1262.64   100.00   100.00   100.00   90.77   93.23   88.45    0.95\n",
      "  0    5600        1298.32   1279.91   100.00   100.00   100.00   91.12   93.53   88.83    0.96\n",
      "  0    5800        1310.74   1278.43   100.00   100.00   100.00   91.28   93.80   88.88    0.96\n",
      "  0    6000        1306.43   1298.67   100.00   100.00   100.00   91.55   94.39   88.87    0.96\n",
      "  0    6200        1322.34   1299.07   100.00   100.00   100.00   91.88   93.55   90.26    0.96\n",
      "  0    6400        1262.21   1257.83   100.00   100.00   100.00   91.74   94.38   89.24    0.96\n",
      "  0    6600        1262.24   1290.12   100.00   100.00   100.00   91.88   94.46   89.43    0.96\n",
      "  0    6800        1215.17   1222.79   100.00   100.00   100.00   92.16   94.14   90.27    0.96\n",
      "  0    7000        1162.87   1139.66   100.00   100.00   100.00   92.10   94.85   89.50    0.96\n",
      "  0    7200        1200.27   1150.81   100.00   100.00   100.00   92.42   94.43   90.50    0.96\n",
      "  0    7400        1092.06   1069.60   100.00   100.00   100.00   92.66   94.26   91.12    0.96\n",
      "  0    7600        1157.53   1102.45   100.00   100.00   100.00   92.68   94.51   90.91    0.96\n",
      "  0    7800        1119.60   1072.88   100.00   100.00   100.00   92.68   94.79   90.67    0.96\n",
      "  0    8000        1197.79   1170.63   100.00   100.00   100.00   92.94   94.44   91.48    0.96\n",
      "  0    8200        1068.28   1043.10   100.00   100.00   100.00   92.84   94.01   91.70    0.96\n",
      "  0    8400        1155.93   1083.90   100.00   100.00   100.00   92.94   94.05   91.86    0.96\n",
      "  0    8600        1067.12    993.10   100.00   100.00   100.00   93.12   94.58   91.70    0.97\n",
      "  0    8800        1156.01   1077.12   100.00   100.00   100.00   93.08   94.29   91.90    0.97\n",
      "  0    9000        1094.16   1016.21   100.00   100.00   100.00   93.18   94.76   91.66    0.97\n",
      "  0    9200        1033.59   1002.42   100.00   100.00   100.00   93.23   95.09   91.45    0.97\n",
      "  0    9400        1088.19    988.33   100.00   100.00   100.00   93.39   94.45   92.34    0.97\n",
      "  0    9600        1159.71   1077.62   100.00   100.00   100.00   93.47   95.00   91.98    0.97\n",
      "  0    9800        1094.63   1025.44   100.00   100.00   100.00   93.50   94.70   92.33    0.97\n",
      "  0   10000         999.99    946.98   100.00   100.00   100.00   93.51   94.76   92.29    0.97\n",
      "  0   10200        1074.67   1015.19   100.00   100.00   100.00   93.71   95.11   92.34    0.97\n",
      "  0   10400        1080.25    967.88   100.00   100.00   100.00   93.81   95.28   92.38    0.97\n",
      "  0   10600        1092.84    965.12   100.00   100.00   100.00   93.90   94.99   92.84    0.97\n",
      "  0   10800        1108.40   1000.40   100.00   100.00   100.00   93.86   95.26   92.49    0.97\n",
      "  0   11000        1024.49    938.87   100.00   100.00   100.00   93.93   95.21   92.68    0.97\n",
      "  0   11200        1133.45   1030.44   100.00   100.00   100.00   93.94   94.76   93.14    0.97\n",
      "  0   11400        1122.42   1004.71   100.00   100.00   100.00   94.01   95.12   92.92    0.97\n",
      "  0   11600         950.14    864.20   100.00   100.00   100.00   93.98   95.06   92.93    0.97\n",
      "  0   11800        1037.22    933.99   100.00   100.00   100.00   94.07   94.98   93.18    0.97\n",
      "  0   12000         981.38    917.31   100.00   100.00   100.00   94.11   94.89   93.34    0.97\n",
      "  0   12200        1034.85    931.90   100.00   100.00   100.00   94.15   95.54   92.79    0.97\n",
      "  0   12400         933.86    848.20   100.00   100.00   100.00   94.11   94.91   93.33    0.97\n",
      "  0   12600        1017.01    910.20   100.00   100.00   100.00   94.20   95.06   93.35    0.97\n",
      "  0   12800        1045.93    949.59   100.00   100.00   100.00   94.30   95.75   92.90    0.97\n",
      "  0   13000        1003.85    915.08   100.00   100.00   100.00   94.25   95.63   92.91    0.97\n",
      "  0   13200        1003.60    906.52   100.00   100.00   100.00   94.23   95.60   92.91    0.97\n",
      "  0   13400        1063.33    942.07   100.00   100.00   100.00   94.45   95.77   93.17    0.97\n",
      "  0   13600        1042.86    938.45   100.00   100.00   100.00   94.40   95.13   93.67    0.97\n",
      "  0   13800        1117.00    974.19   100.00   100.00   100.00   94.45   95.54   93.39    0.97\n",
      "  0   14000         953.93    841.23   100.00   100.00   100.00   94.48   95.32   93.65    0.97\n",
      "  0   14200         929.87    833.59   100.00   100.00   100.00   94.42   95.22   93.64    0.97\n",
      "  0   14400         902.78    802.36   100.00   100.00   100.00   94.44   95.62   93.28    0.97\n",
      "  0   14600        1017.25    873.31   100.00   100.00   100.00   94.50   95.40   93.62    0.97\n",
      "  0   14800         975.59    865.40   100.00   100.00   100.00   94.53   95.50   93.58    0.97\n",
      "  0   15000         960.42    861.61   100.00   100.00   100.00   94.58   95.30   93.88    0.97\n",
      "  0   15200        1003.74    877.48   100.00   100.00   100.00   94.56   95.79   93.36    0.97\n",
      "  0   15400         934.58    856.06   100.00   100.00   100.00   94.55   95.36   93.76    0.97\n",
      "  0   15600         974.49    832.65   100.00   100.00   100.00   94.53   95.40   93.68    0.97\n",
      "  0   15800         959.49    833.80   100.00   100.00   100.00   94.61   95.23   93.99    0.97\n",
      "  0   16000         940.31    809.56   100.00   100.00   100.00   94.66   95.52   93.81    0.97\n",
      "  0   16200         987.77    844.12   100.00   100.00   100.00   94.57   95.27   93.89    0.97\n",
      "  0   16400         937.17    810.70   100.00   100.00   100.00   94.66   95.82   93.52    0.97\n",
      "  0   16600         964.19    833.17   100.00   100.00   100.00   94.68   95.56   93.81    0.97\n",
      "  0   16800         956.11    827.71   100.00   100.00   100.00   94.66   95.54   93.78    0.97\n",
      "  0   17000         961.32    836.27   100.00   100.00   100.00   94.74   95.76   93.73    0.97\n",
      "  0   17200         979.31    851.35   100.00   100.00   100.00   94.70   95.58   93.83    0.97\n",
      "  0   17400         919.75    805.24   100.00   100.00   100.00   94.73   95.86   93.63    0.97\n",
      "  0   17600         924.67    792.81   100.00   100.00   100.00   94.70   95.56   93.85    0.97\n",
      "  0   17800         912.47    826.27   100.00   100.00   100.00   94.75   95.69   93.83    0.97\n",
      "  0   18000        1033.13    877.71   100.00   100.00   100.00   94.75   95.73   93.80    0.97\n",
      "  0   18200         977.92    834.55   100.00   100.00   100.00   94.75   95.69   93.83    0.97\n",
      "  0   18400         863.96    751.63   100.00   100.00   100.00   94.74   95.67   93.83    0.97\n",
      "  0   18600         926.43    812.59   100.00   100.00   100.00   94.74   95.75   93.75    0.97\n",
      "  0   18800         957.93    812.94   100.00   100.00   100.00   94.74   95.63   93.87    0.97\n",
      "  0   19000         886.82    782.54   100.00   100.00   100.00   94.77   95.72   93.84    0.97\n",
      "  0   19200         956.20    820.07   100.00   100.00   100.00   94.77   95.68   93.89    0.97\n",
      "  0   19400         910.69    807.83   100.00   100.00   100.00   94.76   95.58   93.95    0.97\n",
      "  0   19600         884.77    769.23   100.00   100.00   100.00   94.78   95.58   93.99    0.97\n",
      "  0   19800         911.62    790.60   100.00   100.00   100.00   94.78   95.65   93.93    0.97\n",
      "  0   20000         935.39    791.58   100.00   100.00   100.00   94.79   95.64   93.95    0.97\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/transformer_0/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./spaCy_configs/transformer/config0.cfg \\\n",
    "  --output ../models/spaCy/transformer_0 \\\n",
    "  --paths.train ../data/spaCy/train.spacy \\\n",
    "  --paths.dev ../data/spaCy/val.spacy \\\n",
    "  --gpu-id 0 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7447f-913c-448d-8037-0c9c22a924b2",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model         | Dropout | Batch Size | Max Steps | Learn Rate | Mixed Precision | Loss TRANSFORMER | Loss NER | F1     |\n",
    "|---------------|---------|------------|-----------|------------|-----------------|------------------|----------|--------|\n",
    "| transformer_0 | 0.1     | 4          | 20000     | 0.00005    | false           | 935.39           | 791.58   | 94.79 |\n",
    "\n",
    "The baseline transformer model was trained using the `prajjwal1/bert-tiny` architecture with a small batch size of 4 and a conservative learning rate of 0.00005. Dropout was set to 0.1 to maintain regularisation while accommodating the increased capacity of the transformer layers. The model ran for 20,000 steps using full GPU acceleration.\n",
    "\n",
    "Unlike the CNN baseline, this model leveraged contextual embeddings from a pretrained transformer, replacing the static `tok2vec` block with a `transformer` and `TransformerListener`. This resulted in faster and more stable convergence, with F1 rising from 1.73% at step 0 to 94.79% at the final checkpoint.\n",
    "\n",
    "`LOSS TRANSFORMER` remained relatively stable after the initial spike, indicating that the model did not require large weight updates to adapt the pretrained embeddings. Most learning occurred in the NER classification head, reflected in the consistent decline in `LOSS NER` as entity span predictions improved.\n",
    "\n",
    "The final F1-score of 94.79% is a significant improvement over the CNN baseline. Precision reached 95.64%, with recall at 93.95%, suggesting the model effectively balanced confident predictions with broad coverage. This highlights the advantage of pretrained contextual representations when generalising from weak supervision in domain-specific tasks.\n",
    "\n",
    "Overall, this transformer model demonstrates high accuracy and robustness despite being lightweight. It confirms the benefits of contextual modelling for environmental NER and serves as a strong benchmark for evaluating future architectures or fine-tuning strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1913e82-b54c-451b-9f7b-910846c9eb3d",
   "metadata": {},
   "source": [
    "### 4.2 Training Transformer Model 2\n",
    "The second transformer model builds on the baseline by increasing the batch size and adjusting regularisation and training duration. It uses mixed precision to improve memory efficiency and aims to test whether a larger batch and longer training horizon help the model generalise better without overfitting.\n",
    "\n",
    "The `dropout` rate is increased from `0.1` to `0.2` to apply stronger regularisation, which can help stabilise learning in deeper networks. Since the baseline model reached strong performance before early stopping could trigger, `max_steps` is extended to `30000` to allow further improvement. Mixed precision is enabled to reduce memory usage and improve throughput without changing model behaviour.\n",
    "\n",
    "These changes aim to balance training depth, memory efficiency, and model robustness, while keeping the configuration lightweight.\n",
    "\n",
    "| **Parameter**              | **Value**                 |\n",
    "|---------------------------|---------------------------|\n",
    "| `transformer model`       | `prajjwal1/bert-tiny`     |\n",
    "| `span getter`             | `strided_spans.v1`        |\n",
    "| `dropout`                 | `0.2`                     |\n",
    "| `accumulate_gradient`     | `1`                       |\n",
    "| `batch size`              | `16`                      |\n",
    "| `initial learning rate`   | `0.00005`                 |\n",
    "| `warmup steps`            | `250`                     |\n",
    "| `max steps`               | `30000`                   |\n",
    "| `patience`                | `1600`                    |\n",
    "| `eval frequency`          | `200`                     |\n",
    "| `mixed_precision`         | `true`                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcfc4bf1-8f85-4288-9a8e-154f5fd9541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: sentencizer, ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "spaCy_configs/transformer/config1.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config1.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ./spaCy_configs/transformer/config1.cfg \\\n",
    "    --lang en \\\n",
    "    --pipeline sentencizer,transformer,ner \\\n",
    "    --optimize accuracy \\\n",
    "    --force \\\n",
    "    --gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd6bfe0-cdfc-4972-a8d0-dfd30ad30c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-03 10:48:21,961] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory: ../models/spaCy/transformer_1\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/transformer_1\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-03 10:48:24,055] [INFO] Set up nlp object from config\n",
      "[2025-07-03 10:48:24,065] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-03 10:48:24,066] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "[2025-07-03 10:48:24,066] [INFO] Pipeline: ['sentencizer', 'transformer', 'ner']\n",
      "[2025-07-03 10:48:24,069] [INFO] Created vocabulary\n",
      "[2025-07-03 10:48:24,069] [INFO] Finished initializing nlp object\n",
      "[2025-07-03 10:50:24,236] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-03 10:52:27,480] [INFO] Initialized pipeline components: ['sentencizer', 'transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-03 10:52:27,491] [DEBUG] Loading corpus from path: ../data/spaCy/val.spacy\n",
      "[2025-07-03 10:52:27,493] [DEBUG] Loading corpus from path: ../data/spaCy/train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['sentencizer', 'transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Set annotations on update for: ['sentencizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  SENTS_F  SENTS_P  SENTS_R  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0          51.72    113.26   100.00   100.00   100.00    1.93    1.02   17.57    0.51\n",
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 512], strides() = [1, 128]\n",
      "param.sizes() = [128, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:218.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0     200       15397.61  24482.78   100.00   100.00   100.00    0.00    0.00    0.00    0.50\n",
      "  0     400        1645.60   3499.81   100.00   100.00   100.00   42.21   72.05   29.85    0.71\n",
      "  0     600        1493.48   2756.30   100.00   100.00   100.00   56.29   78.49   43.88    0.78\n",
      "  0     800        1290.23   2240.66   100.00   100.00   100.00   62.74   80.01   51.61    0.81\n",
      "  0    1000        1264.19   2142.79   100.00   100.00   100.00   68.16   86.97   56.03    0.84\n",
      "  0    1200        1181.07   1906.36   100.00   100.00   100.00   72.89   85.48   63.53    0.86\n",
      "  0    1400        1093.53   1677.66   100.00   100.00   100.00   73.95   84.43   65.79    0.87\n",
      "  0    1600        1174.66   1659.23   100.00   100.00   100.00   76.49   85.97   68.88    0.88\n",
      "  0    1800        1186.19   1653.06   100.00   100.00   100.00   77.97   85.93   71.37    0.89\n",
      "  0    2000        1299.33   1588.48   100.00   100.00   100.00   78.14   87.99   70.27    0.89\n",
      "  0    2200        2500.21   1476.81   100.00   100.00   100.00   80.20   86.44   74.80    0.90\n",
      "  0    2400        1227.95   1395.22   100.00   100.00   100.00   81.23   86.84   76.30    0.91\n",
      "  0    2600        1034.86   1317.44   100.00   100.00   100.00   81.38   87.79   75.84    0.91\n",
      "  0    2800        1556.91   1321.77   100.00   100.00   100.00   82.49   87.11   78.34    0.91\n",
      "  0    3000        2666.47   1343.96   100.00   100.00   100.00   82.89   88.38   78.05    0.91\n",
      "  0    3200        4168.30   1461.54   100.00   100.00   100.00   83.18   88.90   78.16    0.92\n",
      "  0    3400        5208.97   1427.77   100.00   100.00   100.00   84.01   88.58   79.89    0.92\n",
      "  0    3600        7373.78   1508.64   100.00   100.00   100.00   84.17   89.14   79.72    0.92\n",
      "  0    3800        4300.11   1338.90   100.00   100.00   100.00   84.35   89.63   79.65    0.92\n",
      "  0    4000        1972.02   1132.96   100.00   100.00   100.00   84.97   89.41   80.96    0.92\n",
      "  0    4200        4307.11   1232.94   100.00   100.00   100.00   85.09   89.52   81.07    0.93\n",
      "  0    4400        2532.82   1127.69   100.00   100.00   100.00   85.45   87.72   83.29    0.93\n",
      "  0    4600        1833.87   1029.24   100.00   100.00   100.00   85.99   89.43   82.80    0.93\n",
      "  0    4800        2154.81   1055.88   100.00   100.00   100.00   86.14   88.71   83.71    0.93\n",
      "  0    5000        1266.52    961.32   100.00   100.00   100.00   86.54   89.67   83.62    0.93\n",
      "  0    5200         908.22    944.50   100.00   100.00   100.00   86.89   91.33   82.85    0.93\n",
      "  0    5400        2099.60    948.03   100.00   100.00   100.00   87.10   89.82   84.53    0.94\n",
      "  0    5600         849.18    853.01   100.00   100.00   100.00   87.81   90.58   85.21    0.94\n",
      "  0    5800         805.93    803.64   100.00   100.00   100.00   88.45   91.46   85.64    0.94\n",
      "  0    6000         815.24    852.48   100.00   100.00   100.00   88.53   91.43   85.80    0.94\n",
      "  0    6200         777.51    807.72   100.00   100.00   100.00   88.86   91.14   86.69    0.94\n",
      "  0    6400         820.00    850.94   100.00   100.00   100.00   89.00   92.00   86.19    0.95\n",
      "  0    6600         754.29    762.00   100.00   100.00   100.00   89.06   91.76   86.51    0.95\n",
      "  0    6800        8958.38   1018.28   100.00   100.00   100.00   89.47   92.65   86.49    0.95\n",
      "  0    7000        1007.68    771.36   100.00   100.00   100.00   89.70   91.72   87.76    0.95\n",
      "  0    7200         770.58    768.51   100.00   100.00   100.00   89.82   92.40   87.39    0.95\n",
      "  0    7400        2386.58    875.00   100.00   100.00   100.00   89.85   92.74   87.14    0.95\n",
      "  0    7600         706.23    704.07   100.00   100.00   100.00   89.92   91.83   88.09    0.95\n",
      "  0    7800         695.13    685.61   100.00   100.00   100.00   90.17   92.49   87.95    0.95\n",
      "  0    8000         779.73    700.79   100.00   100.00   100.00   90.16   93.18   87.33    0.95\n",
      "  0    8200         682.75    656.08   100.00   100.00   100.00   90.36   92.83   88.01    0.95\n",
      "  0    8400         695.21    685.62   100.00   100.00   100.00   90.37   92.99   87.89    0.95\n",
      "  0    8600         645.91    639.90   100.00   100.00   100.00   90.54   92.71   88.47    0.95\n",
      "  0    8800         779.50    695.64   100.00   100.00   100.00   90.59   93.09   88.22    0.95\n",
      "  0    9000         664.35    643.27   100.00   100.00   100.00   90.73   92.30   89.23    0.95\n",
      "  0    9200         751.81    664.39   100.00   100.00   100.00   90.99   92.18   89.83    0.95\n",
      "  0    9400         693.09    674.30   100.00   100.00   100.00   91.08   92.19   89.99    0.96\n",
      "  0    9600         632.81    600.60   100.00   100.00   100.00   91.35   93.57   89.24    0.96\n",
      "  0    9800         771.10    634.41   100.00   100.00   100.00   91.22   93.41   89.12    0.96\n",
      "  0   10000         695.77    647.05   100.00   100.00   100.00   91.53   93.27   89.86    0.96\n",
      "  0   10200         648.03    591.86   100.00   100.00   100.00   91.53   93.93   89.25    0.96\n",
      "  0   10400         756.73    647.98   100.00   100.00   100.00   91.64   93.41   89.93    0.96\n",
      "  0   10600         655.75    585.85   100.00   100.00   100.00   91.58   94.28   89.04    0.96\n",
      "  0   10800         765.30    653.75   100.00   100.00   100.00   91.83   94.03   89.72    0.96\n",
      "  0   11000         665.20    642.43   100.00   100.00   100.00   91.87   92.98   90.79    0.96\n",
      "  0   11200         671.87    612.20   100.00   100.00   100.00   91.89   94.02   89.86    0.96\n",
      "  0   11400         740.02    644.32   100.00   100.00   100.00   91.89   94.64   89.29    0.96\n",
      "  0   11600         727.24    633.88   100.00   100.00   100.00   92.16   94.26   90.14    0.96\n",
      "  0   11800         733.45    652.22   100.00   100.00   100.00   92.16   94.10   90.30    0.96\n",
      "  0   12000         640.14    578.68   100.00   100.00   100.00   92.21   94.46   90.06    0.96\n",
      "  0   12200         642.37    618.03   100.00   100.00   100.00   92.32   94.18   90.54    0.96\n",
      "  0   12400         692.28    645.58   100.00   100.00   100.00   92.33   94.37   90.38    0.96\n",
      "  0   12600         630.45    592.64   100.00   100.00   100.00   92.31   94.13   90.56    0.96\n",
      "  0   12800         677.94    617.45   100.00   100.00   100.00   92.48   94.32   90.71    0.96\n",
      "  0   13000         625.54    557.35   100.00   100.00   100.00   92.41   94.26   90.64    0.96\n",
      "  0   13200         657.45    549.07   100.00   100.00   100.00   92.43   94.78   90.20    0.96\n",
      "  0   13400         645.88    573.65   100.00   100.00   100.00   92.56   94.64   90.56    0.96\n",
      "  0   13600         709.16    592.54   100.00   100.00   100.00   92.52   94.94   90.22    0.96\n",
      "  0   13800         648.59    590.32   100.00   100.00   100.00   92.49   94.98   90.13    0.96\n",
      "  0   14000         658.74    555.57   100.00   100.00   100.00   92.65   94.51   90.86    0.96\n",
      "  0   14200         608.51    542.70   100.00   100.00   100.00   92.50   94.99   90.14    0.96\n",
      "  0   14400         629.26    552.64   100.00   100.00   100.00   92.71   94.51   90.97    0.96\n",
      "  0   14600         650.75    551.23   100.00   100.00   100.00   92.64   94.84   90.54    0.96\n",
      "  0   14800         670.68    574.45   100.00   100.00   100.00   92.76   94.75   90.84    0.96\n",
      "  0   15000         557.41    515.43   100.00   100.00   100.00   92.71   94.96   90.57    0.96\n",
      "  0   15200         652.27    582.71   100.00   100.00   100.00   92.81   94.24   91.43    0.96\n",
      "  0   15400         624.35    534.69   100.00   100.00   100.00   92.83   94.80   90.95    0.96\n",
      "  0   15600         634.46    553.03   100.00   100.00   100.00   92.90   94.59   91.28    0.96\n",
      "  0   15800         702.28    592.79   100.00   100.00   100.00   92.97   94.45   91.54    0.96\n",
      "  0   16000         555.00    492.29   100.00   100.00   100.00   92.83   95.07   90.69    0.96\n",
      "  0   16200         565.13    469.67   100.00   100.00   100.00   92.92   94.31   91.57    0.96\n",
      "  0   16400         625.23    541.69   100.00   100.00   100.00   92.84   94.59   91.17    0.96\n",
      "  0   16600         590.31    535.64   100.00   100.00   100.00   92.97   94.54   91.46    0.96\n",
      "  0   16800         711.94    611.62   100.00   100.00   100.00   93.04   94.50   91.62    0.97\n",
      "  0   17000         620.97    535.75   100.00   100.00   100.00   93.05   94.66   91.50    0.97\n",
      "  0   17200         623.62    558.61   100.00   100.00   100.00   93.05   94.92   91.25    0.97\n",
      "  0   17400         605.18    526.04   100.00   100.00   100.00   93.06   94.78   91.41    0.97\n",
      "  0   17600         581.43    518.48   100.00   100.00   100.00   93.01   94.92   91.18    0.97\n",
      "  0   17800         619.83    554.15   100.00   100.00   100.00   93.13   94.73   91.58    0.97\n",
      "  0   18000         672.00    585.92   100.00   100.00   100.00   93.17   94.95   91.45    0.97\n",
      "  0   18200         599.26    520.39   100.00   100.00   100.00   93.18   94.65   91.75    0.97\n",
      "  0   18400         617.07    543.12   100.00   100.00   100.00   93.19   94.87   91.57    0.97\n",
      "  0   18600         601.99    510.84   100.00   100.00   100.00   93.23   94.74   91.77    0.97\n",
      "  0   18800         654.63    558.83   100.00   100.00   100.00   93.22   94.83   91.66    0.97\n",
      "  0   19000         657.77    591.85   100.00   100.00   100.00   93.25   94.73   91.81    0.97\n",
      "  0   19200         606.19    527.15   100.00   100.00   100.00   93.21   94.79   91.69    0.97\n",
      "  0   19400         621.06    530.44   100.00   100.00   100.00   93.22   94.79   91.69    0.97\n",
      "  0   19600         536.33    466.02   100.00   100.00   100.00   93.23   94.78   91.73    0.97\n",
      "  0   19800         641.31    526.36   100.00   100.00   100.00   93.23   94.88   91.64    0.97\n",
      "  0   20000         672.07    570.44   100.00   100.00   100.00   93.24   94.90   91.63    0.97\n",
      "  0   20200         617.54    544.35   100.00   100.00   100.00   93.24   94.90   91.63    0.97\n",
      "  0   20400         632.30    539.54   100.00   100.00   100.00   93.24   94.90   91.63    0.97\n",
      "  0   20600         651.61    546.61   100.00   100.00   100.00   93.24   94.90   91.63    0.97\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/transformer_1/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./spaCy_configs/transformer/config1.cfg \\\n",
    "  --output ../models/spaCy/transformer_1 \\\n",
    "  --paths.train ../data/spaCy/train.spacy \\\n",
    "  --paths.dev ../data/spaCy/val.spacy \\\n",
    "  --gpu-id 0 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478df26d-0841-4a6b-b02f-ca343ff1f525",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model         | Dropout | Batch Size | Max Steps | Learn Rate | Mixed Precision | Loss TRANSFORMER | Loss NER | F1     |\n",
    "|---------------|---------|------------|-----------|------------|-----------------|------------------|----------|--------|\n",
    "| transformer_0 | 0.1     | 4          | 20000     | 0.00005    | false           | 935.39           | 791.58   | 94.79  |\n",
    "| transformer_1 | 0.2     | 16         | 30000     | 0.00005    | true            | 651.61           | 546.61   | 93.24  |\n",
    "\n",
    "Transformer model 1 explores the effects of longer training and more aggressive regularisation. It retains the same `prajjwal1/bert-tiny` backbone but increases the dropout rate from 0.1 to 0.2 and raises the batch size from 4 to 16. This setup enables the model to simulate a larger training horizon while maintaining stability through gradient smoothing. The maximum number of steps was increased to 30,000 to allow additional convergence time, since early stopping did not trigger in the previous run.\n",
    "\n",
    "Mixed precision training was enabled for this model to improve memory efficiency and reduce training time, particularly important given the increased batch size. The learning rate and warmup scheduler remained unchanged to preserve the stability of the optimiser dynamics.\n",
    "\n",
    "Training progressed smoothly, with `LOSS NER` reducing steadily from over 24,000 at step 200 to **546.61** at step 30,000. Similarly, `LOSS TRANSFORMER` declined to **651.61**, suggesting successful fine-tuning of both the contextual embeddings and the NER classification head.\n",
    "\n",
    "However, the final F1-score reached **93.24%**, slightly lower than the baseline's 94.79%. Precision and recall remained balanced at later stages, but the model did not achieve the same level of generalisation. This suggests that the higher dropout and longer training time may have led to underfitting or hindered learning in low-frequency or ambiguous entity contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da932ae-4fdb-4622-b3f3-f39422b71a55",
   "metadata": {},
   "source": [
    "### 4.3 Training Transformer Model 3\n",
    "\n",
    "This third transformer model builds on previous configurations by experimenting with more aggressive learning dynamics and relaxed regularisation. The dropout is removed entirely (`0.0`) to allow full parameter updates on each pass, and the learning rate is doubled to `0.0001` to encourage faster convergence during early training. \n",
    "\n",
    "The warmup steps were increased to `500` to accommodate the higher learning rate, ensuring that early updates do not destabilise the pretrained transformer weights. This change was introduced because the previous models, while accurate, plateaued quickly. It was hypothesised that a slightly more exploratory optimiser schedule could help the model escape local minima and potentially improve recall on low-frequency entity types.\n",
    "\n",
    "The score weights were also modified to focus evaluation strictly on entity-level F1 (`ents_f = 1.5`) while ignoring sentence segmentation metrics. This reflects the priority of entity extraction performance in the environmental NER pipeline.\n",
    "\n",
    "The table below summarises the configuration changes:\n",
    "\n",
    "| **Parameter**              | **Value**                 |\n",
    "|---------------------------|---------------------------|\n",
    "| `transformer model`       | `prajjwal1/bert-tiny`     |\n",
    "| `span getter`             | `strided_spans.v1`        |\n",
    "| `dropout`                 | `0.0`                     |\n",
    "| `accumulate_gradient`     | `1`                       |\n",
    "| `batch size`              | `16`                      |\n",
    "| `initial learning rate`   | `0.0001`                  |\n",
    "| `warmup steps`            | `500`                     |\n",
    "| `max steps`               | `30000`                   |\n",
    "| `patience`                | `1000`                    |\n",
    "| `eval frequency`          | `200`                     |\n",
    "| `mixed precision`         | `true`                    |\n",
    "| `score weights`           | `ents_f = 1.5`            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372c990-f1ae-4ea1-855b-21a854fca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config ./spaCy_configs/transformer/config2.cfg \\\n",
    "    --lang en \\\n",
    "    --pipeline sentencizer,transformer,ner \\\n",
    "    --optimize accuracy \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a4cc4c-74be-40f8-9078-a9c23f1ab58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-04 13:53:43,665] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/transformer_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-04 13:53:45,736] [INFO] Set up nlp object from config\n",
      "[2025-07-04 13:53:45,746] [DEBUG] Loading corpus from path: ../data/spaCy/val_sentencized.spacy\n",
      "[2025-07-04 13:53:45,747] [DEBUG] Loading corpus from path: ../data/spaCy/train_sentencized.spacy\n",
      "[2025-07-04 13:53:45,747] [INFO] Pipeline: ['sentencizer', 'transformer', 'ner']\n",
      "[2025-07-04 13:53:45,751] [INFO] Created vocabulary\n",
      "[2025-07-04 13:53:45,751] [INFO] Finished initializing nlp object\n",
      "[2025-07-04 13:56:07,789] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-04 13:59:02,416] [INFO] Initialized pipeline components: ['sentencizer', 'transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-04 13:59:02,439] [DEBUG] Loading corpus from path: ../data/spaCy/val_sentencized.spacy\n",
      "[2025-07-04 13:59:02,441] [DEBUG] Loading corpus from path: ../data/spaCy/train_sentencized.spacy\n",
      "[2025-07-04 13:59:02,454] [DEBUG] Removed existing output directory: ../models/spaCy/transformer_2/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['sentencizer', 'transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Set annotations on update for: ['sentencizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  SENTS_F  SENTS_P  SENTS_R  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0          49.33    107.51   100.00   100.00   100.00    1.93    1.02   17.56    0.41\n",
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 512], strides() = [1, 128]\n",
      "param.sizes() = [128, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:218.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0     200       16327.39  24230.90   100.00   100.00   100.00    0.00    0.00    0.00    0.40\n",
      "  0     400        1544.75   3303.14   100.00   100.00   100.00   44.40   70.01   32.51    0.67\n",
      "  0     600        1301.11   2425.27   100.00   100.00   100.00   62.57   78.47   52.02    0.78\n",
      "  0     800        1279.42   2100.35   100.00   100.00   100.00   72.43   86.67   62.20    0.83\n",
      "  0    1000        1053.73   1636.53   100.00   100.00   100.00   76.05   84.34   69.25    0.86\n",
      "  0    1200        1067.73   1511.09   100.00   100.00   100.00   78.80   86.65   72.27    0.87\n",
      "  0    1400        1022.31   1381.85   100.00   100.00   100.00   80.73   85.87   76.17    0.88\n",
      "  0    1600        1053.41   1354.48   100.00   100.00   100.00   81.97   83.74   80.27    0.89\n",
      "  0    1800        3023.30   1304.05   100.00   100.00   100.00   82.95   88.22   78.28    0.90\n",
      "  0    2000        1150.40   1082.33   100.00   100.00   100.00   84.14   85.63   82.70    0.90\n",
      "  0    2200        2269.47   1074.23   100.00   100.00   100.00   85.16   89.61   81.13    0.91\n",
      "  0    2400        9082.78   1199.74   100.00   100.00   100.00   85.83   88.30   83.49    0.91\n",
      "  0    2600        6163.94   1180.72   100.00   100.00   100.00   85.96   90.81   81.60    0.92\n",
      "  0    2800        3231.38   1087.99   100.00   100.00   100.00   86.41   88.70   84.24    0.92\n",
      "  0    3000        2241.66    979.70   100.00   100.00   100.00   87.42   89.28   85.65    0.92\n",
      "  0    3200         738.33    835.27   100.00   100.00   100.00   88.25   91.11   85.56    0.93\n",
      "  0    3400         755.34    852.38   100.00   100.00   100.00   88.98   91.23   86.84    0.93\n",
      "  0    3600         797.58    870.08   100.00   100.00   100.00   89.02   92.06   86.18    0.93\n",
      "  0    3800         704.94    770.24   100.00   100.00   100.00   89.60   92.03   87.29    0.94\n",
      "  0    4000         778.59    832.98   100.00   100.00   100.00   89.59   92.67   86.71    0.94\n",
      "  0    4200         670.97    712.82   100.00   100.00   100.00   90.36   92.00   88.79    0.94\n",
      "  0    4400         608.34    663.41   100.00   100.00   100.00   90.21   91.34   89.11    0.94\n",
      "  0    4600         658.80    664.58   100.00   100.00   100.00   90.18   91.63   88.77    0.94\n",
      "  0    4800         728.12    741.59   100.00   100.00   100.00   90.73   93.16   88.42    0.94\n",
      "  0    5000         747.05    752.64   100.00   100.00   100.00   91.09   93.18   89.09    0.95\n",
      "  0    5200         689.62    675.86   100.00   100.00   100.00   91.15   92.93   89.44    0.95\n",
      "  0    5400         628.30    630.85   100.00   100.00   100.00   91.05   93.27   88.94    0.95\n",
      "  0    5600         659.95    621.26   100.00   100.00   100.00   91.50   92.95   90.10    0.95\n",
      "  0    5800         663.83    631.53   100.00   100.00   100.00   91.84   93.74   90.02    0.95\n",
      "  0    6000         723.85    689.99   100.00   100.00   100.00   91.78   93.13   90.47    0.95\n",
      "  0    6200         642.91    600.95   100.00   100.00   100.00   91.95   93.24   90.69    0.95\n",
      "  0    6400         648.56    584.89   100.00   100.00   100.00   92.11   93.92   90.36    0.95\n",
      "  0    6600         643.53    607.21   100.00   100.00   100.00   92.36   93.48   91.27    0.95\n",
      "  0    6800         629.25    577.40   100.00   100.00   100.00   92.45   93.97   90.97    0.95\n",
      "  0    7000         650.30    585.75   100.00   100.00   100.00   92.60   94.24   91.03    0.96\n",
      "  0    7200         626.23    597.38   100.00   100.00   100.00   92.84   93.98   91.73    0.96\n",
      "  0    7400         656.76    612.49   100.00   100.00   100.00   92.57   94.15   91.05    0.96\n",
      "  0    7600         598.31    566.14   100.00   100.00   100.00   92.77   94.79   90.85    0.96\n",
      "  0    7800         573.90    527.48   100.00   100.00   100.00   92.95   94.13   91.81    0.96\n",
      "  0    8000         665.65    588.54   100.00   100.00   100.00   92.73   94.47   91.06    0.96\n",
      "  0    8200         624.94    562.77   100.00   100.00   100.00   93.04   93.29   92.80    0.96\n",
      "  0    8400         623.96    563.11   100.00   100.00   100.00   93.19   94.21   92.19    0.96\n",
      "  0    8600         587.59    517.13   100.00   100.00   100.00   93.28   94.52   92.07    0.96\n",
      "  0    8800         610.76    537.85   100.00   100.00   100.00   93.46   94.73   92.23    0.96\n",
      "  0    9000         538.38    483.62   100.00   100.00   100.00   93.62   94.68   92.58    0.96\n",
      "  0    9200         597.10    479.33   100.00   100.00   100.00   93.31   94.42   92.21    0.96\n",
      "  0    9400         602.59    503.52   100.00   100.00   100.00   93.62   94.79   92.49    0.96\n",
      "  0    9600         523.71    458.14   100.00   100.00   100.00   93.76   94.53   93.00    0.96\n",
      "  0    9800         604.92    513.44   100.00   100.00   100.00   93.62   94.86   92.40    0.96\n",
      "  0   10000         561.32    454.34   100.00   100.00   100.00   93.78   94.34   93.23    0.96\n",
      "  0   10200         630.53    537.50   100.00   100.00   100.00   93.85   94.65   93.07    0.96\n",
      "  0   10400        6435.36    653.98   100.00   100.00   100.00   93.98   95.43   92.57    0.96\n",
      "  0   10600         566.52    472.40   100.00   100.00   100.00   93.95   94.52   93.38    0.96\n",
      "  0   10800         598.94    458.16   100.00   100.00   100.00   93.93   96.16   91.79    0.96\n",
      "  0   11000         630.94    500.34   100.00   100.00   100.00   93.94   94.41   93.48    0.96\n",
      "  0   11200         640.08    501.37   100.00   100.00   100.00   94.10   94.88   93.34    0.96\n",
      "  0   11400         667.36    527.20   100.00   100.00   100.00   94.24   95.07   93.42    0.97\n",
      "  0   11600         599.70    473.88   100.00   100.00   100.00   94.20   95.14   93.28    0.97\n",
      "  0   11800         635.03    491.08   100.00   100.00   100.00   94.14   94.55   93.73    0.96\n",
      "  0   12000         572.63    475.40   100.00   100.00   100.00   94.18   95.14   93.24    0.97\n",
      "  0   12200         571.30    427.86   100.00   100.00   100.00   94.29   94.74   93.85    0.97\n",
      "  0   12400         548.68    426.23   100.00   100.00   100.00   94.45   95.40   93.51    0.97\n",
      "  0   12600         576.00    417.58   100.00   100.00   100.00   94.34   95.05   93.63    0.97\n",
      "  0   12800         548.93    435.12   100.00   100.00   100.00   94.23   95.02   93.45    0.97\n",
      "  0   13000         614.58    435.51   100.00   100.00   100.00   94.49   95.81   93.21    0.97\n",
      "  0   13200         630.74    462.50   100.00   100.00   100.00   94.41   95.07   93.76    0.97\n",
      "  0   13400         660.29    496.12   100.00   100.00   100.00   94.59   95.11   94.08    0.97\n",
      "  0   13600         534.22    391.48   100.00   100.00   100.00   94.44   94.90   93.99    0.97\n",
      "  0   13800         575.05    432.13   100.00   100.00   100.00   94.73   95.47   94.01    0.97\n",
      "  0   14000         563.29    414.05   100.00   100.00   100.00   94.70   95.13   94.28    0.97\n",
      "  0   14200         529.66    390.18   100.00   100.00   100.00   94.71   95.36   94.06    0.97\n",
      "  0   14400         506.73    383.32   100.00   100.00   100.00   94.82   95.67   93.98    0.97\n",
      "  0   14600         630.65    421.97   100.00   100.00   100.00   94.79   95.92   93.69    0.97\n",
      "  0   14800         588.25    431.23   100.00   100.00   100.00   94.81   95.60   94.03    0.97\n",
      "  0   15000         565.79    415.32   100.00   100.00   100.00   94.90   95.36   94.44    0.97\n",
      "  0   15200         645.93    453.91   100.00   100.00   100.00   94.98   95.69   94.29    0.97\n",
      "  0   15400         516.56    399.02   100.00   100.00   100.00   94.92   96.00   93.86    0.97\n",
      "  0   15600         628.31    447.82   100.00   100.00   100.00   94.99   95.64   94.34    0.97\n",
      "  0   15800         574.31    424.22   100.00   100.00   100.00   94.94   95.44   94.45    0.97\n",
      "  0   16000         665.20    454.00   100.00   100.00   100.00   95.06   96.02   94.12    0.97\n",
      "  0   16200         551.58    401.96   100.00   100.00   100.00   94.87   95.76   94.01    0.97\n",
      "  0   16400         576.44    390.35   100.00   100.00   100.00   95.13   96.21   94.07    0.97\n",
      "  0   16600         634.66    338.17   100.00   100.00   100.00   95.07   95.71   94.45    0.97\n",
      "  0   16800         593.18    431.68   100.00   100.00   100.00   95.22   95.92   94.53    0.97\n",
      "  0   17000         556.90    374.05   100.00   100.00   100.00   95.20   95.43   94.97    0.97\n",
      "  0   17200         632.71    414.78   100.00   100.00   100.00   95.14   95.84   94.46    0.97\n",
      "  0   17400         635.44    427.05   100.00   100.00   100.00   95.19   95.97   94.43    0.97\n",
      "  0   17600         582.91    385.72   100.00   100.00   100.00   95.18   95.87   94.50    0.97\n",
      "  0   17800         563.52    376.76   100.00   100.00   100.00   95.14   95.56   94.72    0.97\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "../models/spaCy/transformer_2/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./spaCy_configs/transformer/config2.cfg \\\n",
    "  --output ../models/spaCy/transformer_2 \\\n",
    "  --paths.train ../data/spaCy/train.spacy \\\n",
    "  --paths.dev ../data/spaCy/val.spacy \\\n",
    "  --verbose --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc33fe-fb54-4f50-8129-c00772b9c77d",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "\n",
    "| Model         | Dropout | Batch Size | Max Steps | Learn Rate | Mixed Precision | Loss TRANSFORMER | Loss NER | F1     |\n",
    "|---------------|---------|------------|-----------|------------|-----------------|------------------|----------|--------|\n",
    "| transformer_0 | 0.1     | 4          | 20000     | 0.00005    | false           | 935.39           | 791.58   | 94.79  |\n",
    "| transformer_1 | 0.2     | 16         | 30000     | 0.00005    | true            | 651.61           | 546.61   | 93.24  |\n",
    "| transformer_2 | 0.0     | 16         | 30000     | 0.0001     | true            | 563/52           | 376.76   | 95.14 |\n",
    "\n",
    "Transformer model 2 introduces a higher learning rate and disables dropout to encourage faster convergence and stronger gradient signals. It maintains the `prajjwal1/bert-tiny` backbone and uses the same batch size and training duration as the previous model, allowing for a fair comparison. Mixed precision training is enabled to reduce memory usage and speed up training.\n",
    "\n",
    "The learning rate is increased to 0.0001, double that of earlier runs. This change was intended to accelerate fine-tuning, particularly for the NER classification head. Disabling dropout entirely allows the model to learn more aggressively without regularisation, which can be beneficial when the training data is relatively clean and well-aligned.\n",
    "\n",
    "Training was stable across all steps, with `LOSS NER` decreasing steadily and reaching a final value of **376.76**. `LOSS TRANSFORMER` also declined to **563.52**, indicating effective adaptation of the pretrained embeddings. The model achieved a final F1-score of **95.14%**, the highest among all transformer models.\n",
    "\n",
    "This configuration delivered the best balance between learning speed, loss reduction, and final accuracy. It outperformed both the baseline and regularised variant, suggesting that in this domain-specific, weakly supervised setting, lighter regularisation and stronger updates may be more effective than conservative tuning. Transformer 2 is therefore selected for final evaluation on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9afa9-e106-403f-b7cb-1264164a53ee",
   "metadata": {},
   "source": [
    "### 4.4 Choosing the Best Transformer Model\n",
    "Among the three transformer models trained, `transformer_2` achieved the highest F1-score on the validation set and showed the most consistent loss reduction throughout training. Its configuration balanced stability, learning capacity, and convergence rate more effectively than the other variants. This model is therefore selected as the final candidate for test evaluation.\n",
    "\n",
    "Only three transformer models were trained due to practical constraints. Transformer models are significantly more computationally expensive than CRF or CNN alternatives. Each training run took several hours, even with mixed precision enabled on a GPU. Due to time and resource limitations, it was not feasible to perform a full hyperparameter search.\n",
    "\n",
    "To improve generalisation and reduce overfitting to the validation split, the best model (`transformer_2`) is retrained using both the original training and validation datasets. This allows the model to learn from a larger portion of the labelled data while preserving a separate held-out test set for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6e472d-f873-40b2-ae27-de5032419aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 10:03:09,405] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: ../models/spaCy/transformer_best\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2025-07-05 10:03:11,353] [INFO] Set up nlp object from config\n",
      "[2025-07-05 10:03:11,361] [DEBUG] Loading corpus from path: ../data/spaCy/test.spacy\n",
      "[2025-07-05 10:03:11,362] [DEBUG] Loading corpus from path: ../data/spaCy/train_val.spacy\n",
      "[2025-07-05 10:03:11,362] [INFO] Pipeline: ['sentencizer', 'transformer', 'ner']\n",
      "[2025-07-05 10:03:11,365] [INFO] Created vocabulary\n",
      "[2025-07-05 10:03:11,365] [INFO] Finished initializing nlp object\n",
      "[2025-07-05 10:05:36,813] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, grc, id, lb, mk, pt, ru, sr, ta, th\n",
      "\n",
      "Load the table in your config with:\n",
      "\n",
      "[initialize.lookups]\n",
      "@misc = \"spacy.LookupsDataLoader.v1\"\n",
      "lang = ${nlp.lang}\n",
      "tables = [\"lexeme_norm\"]\n",
      "\n",
      "[2025-07-05 10:08:08,022] [INFO] Initialized pipeline components: ['sentencizer', 'transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2025-07-05 10:08:08,031] [DEBUG] Loading corpus from path: ../data/spaCy/test.spacy\n",
      "[2025-07-05 10:08:08,033] [DEBUG] Loading corpus from path: ../data/spaCy/train_val.spacy\n",
      "[2025-07-05 10:08:08,037] [DEBUG] Removed existing output directory: ../models/spaCy/transformer_best/model-best\n",
      "[2025-07-05 10:08:08,042] [DEBUG] Removed existing output directory: ../models/spaCy/transformer_best/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['sentencizer', 'transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Set annotations on update for: ['sentencizer']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  SENTS_F  SENTS_P  SENTS_R  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  -------  -------  -------  ------  ------  ------  ------\n",
      "  0       0          65.18    120.03   100.00   100.00   100.00    1.95    1.03   17.68    0.41\n",
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [128, 512], strides() = [1, 128]\n",
      "param.sizes() = [128, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/autograd/functions/accumulate_grad.h:218.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0     200       14918.09  24188.11   100.00   100.00   100.00    0.00    0.00    0.00    0.40\n",
      "\u001b[38;5;3m⚠ Aborting and saving the final best model. Encountered exception:\n",
      "RuntimeError('The size of tensor a (528) must match the size of tensor b (512)\n",
      "at non-singleton dimension 1')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/click/core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/typer/core.py\", line 740, in main\n",
      "    return _main(\n",
      "           ^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/typer/core.py\", line 195, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/click/core.py\", line 1697, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/click/core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/click/core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/typer/main.py\", line 698, in wrapper\n",
      "    return callback(**use_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/cli/train.py\", line 54, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/cli/train.py\", line 84, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/training/loop.py\", line 135, in train\n",
      "    raise e\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/training/loop.py\", line 118, in train\n",
      "    for batch, info, is_best_checkpoint in training_step_iterator:\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/training/loop.py\", line 220, in train_while_improving\n",
      "    nlp.update(\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy/language.py\", line 1196, in update\n",
      "    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy_transformers/pipeline_component.py\", line 294, in update\n",
      "    trf_full, bp_trf_full = self.model.begin_update(docs)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/thinc/model.py\", line 328, in begin_update\n",
      "    return self._func(self, X, is_train=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/spacy_transformers/layers/transformer_model.py\", line 199, in forward\n",
      "    model_output, bp_tensors = transformer(wordpieces, is_train)\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/thinc/model.py\", line 310, in __call__\n",
      "    return self._func(self, X, is_train=is_train)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/thinc/layers/pytorchwrapper.py\", line 225, in forward\n",
      "    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/thinc/shims/pytorch.py\", line 95, in __call__\n",
      "    return self.begin_update(inputs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/thinc/shims/pytorch.py\", line 137, in begin_update\n",
      "    output = self._model(*inputs.args, **inputs.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 1078, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "                       ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py\", line 217, in forward\n",
      "    embeddings += position_embeddings\n",
      "RuntimeError: The size of tensor a (528) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./spaCy_configs/transformer/config2.cfg \\\n",
    "  --output ../models/spaCy/transformer_best \\\n",
    "  --paths.train ../data/spaCy/train_val.spacy \\\n",
    "  --paths.dev ../data/spaCy/test.spacy \\\n",
    "  --verbose --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004114bf-42f0-4659-b3f4-2fad82423e23",
   "metadata": {},
   "source": [
    "### 4.5 Evaluating on the Test Set\n",
    "This section evaluates the final transformer model (`transformer_best`) on the held-out test set, following the same process used for the CNN model.\n",
    "\n",
    "The aim is to measure how well the transformer generalises to unseen sentences and whether it captures environmental domain patterns effectively. Evaluation is performed using the test split that was never seen during training or validation.\n",
    "\n",
    "The analysis includes:\n",
    "\n",
    "- A confusion matrix of predicted vs gold entity labels\n",
    "- Per-label F1 scores for all five entity categories\n",
    "- A ranked list of the most common misclassification types\n",
    "- Example predictions on raw unseen environmental text\n",
    "\n",
    "This allows a direct comparison with the CNN-based results and helps highlight whether transformer-based architectures provide tangible improvements for domain-specific NER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe20751-d323-4ffb-b8e4-4b0523950263",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "The confusion matrix below shows how often each entity type was predicted correctly or confused with another. It helps highlight specific weaknesses, such as overlap between similar categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8095635e-7e8e-4f29-9fd4-ac8aad748f59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load transformer model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m transformer_nlp = \u001b[43mspacy\u001b[49m.load(SPACY_MODEL_PATH / \u001b[33m\"\u001b[39m\u001b[33mtransformer_best\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mmodel-best\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the test data\u001b[39;00m\n\u001b[32m      5\u001b[39m doc_bin = DocBin().from_disk(SPACY_DATA_PATH / \u001b[33m\"\u001b[39m\u001b[33mtest.spacy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load transformer model\n",
    "transformer_nlp = spacy.load(SPACY_MODEL_PATH / \"transformer_best\" / \"model-best\")\n",
    "\n",
    "# Load the test data\n",
    "doc_bin = DocBin().from_disk(SPACY_DATA_PATH / \"test.spacy\")\n",
    "docs = list(doc_bin.get_docs(transformer_nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817b4aa-eef1-4dba-8c01-84913b8bc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true and predicted entity labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for doc in docs:\n",
    "    pred_doc = transformer_nlp(doc.text)\n",
    "\n",
    "    true_ents = {(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents}\n",
    "    pred_ents = {(ent.start_char, ent.end_char, ent.label_) for ent in pred_doc.ents}\n",
    "\n",
    "    all_spans = true_ents.union(pred_ents)\n",
    "\n",
    "    for start, end, label in all_spans:\n",
    "        true_label = label if (start, end, label) in true_ents else \"O\"\n",
    "        pred_label = label if (start, end, label) in pred_ents else \"O\"\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad650bd-5e40-4a71-8c7c-9f7d2275ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(ax=ax, xticks_rotation=45, cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix for Final Transformer Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d4e6f-b590-4380-b8d0-a200d9d8fd39",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc148c56-194c-4151-a95e-b251e85c43e0",
   "metadata": {},
   "source": [
    "#### Per-label F1 Score\n",
    "This section shows the F1 score for each entity type using the final transformer model. It helps identify which labels the model handles well and which remain more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137fc20-8162-4584-b5b1-6888e0548c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "f1_scores = {\n",
    "    label: score[\"f1-score\"]\n",
    "    for label, score in report.items()\n",
    "    if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "}\n",
    "\n",
    "# Plot F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(f1_scores.keys()), y=list(f1_scores.values()), palette=\"Blues_d\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Entity Label\")\n",
    "plt.ylim(0.0, 1.05)\n",
    "plt.title(\"Per-label F1 Scores – Final Transformer Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9b389-fb58-40c7-8657-a99f255425d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de321afe-f853-465b-815b-35ad112101b9",
   "metadata": {},
   "source": [
    "#### Top Misclassified Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86333f6b-88e7-4426-a480-5fe77ae0f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all misclassified spans\n",
    "misclassified = [(true, pred) for true, pred in zip(y_true, y_pred) if true != pred]\n",
    "\n",
    "# Count and display the top 10 misclassified label pairs\n",
    "misclassified_counts = Counter(misclassified)\n",
    "top_misclassified = misclassified_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 misclassified label pairs (true → predicted):\\n\")\n",
    "for (true_label, pred_label), count in top_misclassified:\n",
    "    print(f\"{true_label} → {pred_label}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8811210-6b2b-4e5c-9ac8-70a2f3293ce8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab3db92e-5541-4e27-99e2-347fec83df67",
   "metadata": {},
   "source": [
    "#### Predicting Entities in a Raw Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cea68-f7db-4a83-ab1b-877bb50974c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\n",
    "    \"During the long-term monitoring of freshwater ecosystems in the upper Thames basin, \"\n",
    "    \"elevated concentrations of nitrates (above 50 mg/L) and phosphorus compounds were detected, \"\n",
    "    \"particularly near agricultural runoff zones and peatland catchments. \"\n",
    "    \"The decline of apple crab populations was observed concurrently with increased sedimentation, \"\n",
    "    \"urban expansion, and rising temperatures due to climate change. \"\n",
    "    \"In 2021, the average air temperature in deciduous woodland habitats exceeded 18 °C \"\n",
    "    \"contributing to a shift in breeding cycles among amphibians like rana temporaria.\"\n",
    ")\n",
    "\n",
    "doc = transformer_nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    ent_label = token.ent_type_ if token.ent_type_ else \"\"\n",
    "    print(f\"{token.text:<18} {ent_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e28687-bb9e-4990-b337-2c02019527a8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER (GPU)",
   "language": "python",
   "name": "ner-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
