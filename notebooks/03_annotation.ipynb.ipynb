{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c9e7aa-69c2-484d-a277-f39810c290a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the input text data\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [line.strip().lower() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762c957d-87ca-431d-9973-b299897a04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_missing_spaces(text):\n",
    "    return re.sub(r'(?<=[a-zA-Z0-9])(?=[.?!])(?=[^\\s])', r'\\g<0> ', text)\n",
    "\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [\n",
    "        fix_missing_spaces(line.strip().lower())\n",
    "        for line in f if line.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7f36e09-77a0-4cde-8563-9dc518cdd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "\n",
    "def build_automaton(vocab_terms):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for term in vocab_terms:\n",
    "        A.add_word(term, term)\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "def is_inside_hyphenated_word(text, start, end):\n",
    "    # Check if the match is attached to another token via a hyphen\n",
    "    if start > 0 and text[start - 1] == '-':\n",
    "        return True\n",
    "    if end < len(text) and text[end] == '-':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def annotate_text_with_vocab(text, automaton, label):\n",
    "    text_length = len(text)\n",
    "    matches = []\n",
    "\n",
    "    # Iterate using the automaton\n",
    "    for end_index, term in automaton.iter(text):\n",
    "        start_index = end_index - len(term) + 1\n",
    "\n",
    "        # Whole word check\n",
    "        if (start_index == 0 or not text[start_index - 1].isalnum()) and (\n",
    "            end_index + 1 == text_length or not text[end_index + 1].isalnum()\n",
    "        ):\n",
    "            if not is_inside_hyphenated_word(text, start_index, end_index + 1):\n",
    "                matches.append([start_index, end_index + 1, label])\n",
    "\n",
    "    # Sort by start index, then longer spans first\n",
    "    matches.sort(key=lambda x: (x[0], x[1] - x[0]), reverse=False)\n",
    "    \n",
    "    # Don't block overlaps — just collect all clean, whole-word matches\n",
    "    annotations = [[start, end, label] for start, end, label in matches]\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3606d0f6-c26d-44e0-bfa3-3ed860fe7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded 196 terms to 312 (with plurals added).\n",
      "📁 Saved to: ..\\vocabularies\\meas_with_plurals.txt\n"
     ]
    }
   ],
   "source": [
    "# import inflect\n",
    "# from pathlib import Path\n",
    "\n",
    "# p = inflect.engine()\n",
    "\n",
    "# # Load vocab file\n",
    "# VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "# input_path = VOCAB_DIR / \"pollutant.txt\"\n",
    "# output_path = VOCAB_DIR / \"pollutant_with_plurals.txt\"\n",
    "\n",
    "# # Read and pluralise\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     original_terms = {line.strip().lower() for line in f if line.strip()}\n",
    "\n",
    "# expanded_terms = set()\n",
    "\n",
    "# for term in original_terms:\n",
    "#     expanded_terms.add(term)\n",
    "#     # Try to pluralise single words\n",
    "#     if len(term.split()) == 1:\n",
    "#         plural = p.plural(term)\n",
    "#         if plural and plural != term:\n",
    "#             expanded_terms.add(plural)\n",
    "\n",
    "# # Write expanded vocab\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for term in sorted(expanded_terms):\n",
    "#         f.write(term + \"\\n\")\n",
    "\n",
    "# print(f\"✅ Expanded {len(original_terms)} terms to {len(expanded_terms)} (with plurals added).\")\n",
    "# print(f\"📁 Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce544cf6-8463-40bc-8060-595fec6162c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_files = [\n",
    "    \"measurement.txt\", \n",
    "    \"pollutant.txt\", \n",
    "    \"env_process.txt\", \n",
    "    \"habitat.txt\", \n",
    "    \"taxonomy.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46bb4d33-16a5-4724-a604-e26bcb514854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10977cae-7209-444a-947e-a58c0c1d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating category: MEASUREMENT from measurement.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: POLLUTANT from pollutant.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: ENV_PROCESS from env_process.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: HABITAT from habitat.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: TAXONOMY from taxonomy.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "✅ Done! Annotated 272,840 unique texts.\n",
      "📁 Saved to: ../data/processed_data/training_data.jsonl\n",
      "⏱ Total time: 32.77 seconds\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "text_to_annotations = defaultdict(list)\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_name.upper()\n",
    "    print(f\"Annotating category: {label} from {fname}\")\n",
    "\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    automaton = build_automaton(vocab_terms)\n",
    "\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        annotations = annotate_text_with_vocab(text, automaton, label)\n",
    "        if annotations:\n",
    "            text_to_annotations[text].extend(annotations)\n",
    "            text_to_annotations[text] = resolve_overlaps(text_to_annotations[text])\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Processed {i + 1:,}/{len(preprocessed_texts):,} texts...\")\n",
    "\n",
    "# Save final output\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, annotations in text_to_annotations.items():\n",
    "        json.dump({\"text\": text, \"label\": annotations}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ Done! Annotated {len(text_to_annotations):,} unique texts.\")\n",
    "print(f\"📁 Saved to: {annotated_path}\")\n",
    "print(f\"⏱ Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d597e0b9-38c3-4034-ba60-15b1b39187cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampled 1000 items to ../data/processed_data/sample_for_manual_testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_FILE = Path(\"..\") / \"data\" / \"processed_data\" / \"training_data.jsonl\"\n",
    "OUTPUT_FILE = Path(\"..\") / \"data\" / \"processed_data\" / \"sample_for_manual_testing.jsonl\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load safely, skip blank or bad lines\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"⚠️ Skipped malformed line\")\n",
    "\n",
    "# Sample\n",
    "sampled = random.sample(all_data, min(1000, len(all_data)))\n",
    "\n",
    "# Save sample\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Sampled {len(sampled)} items to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fae8-2e5f-4296-ba5b-2c17ef3cf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbec58a-70f6-40c3-ad62-b5564c3db6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8c12-6fa7-4dc8-983f-74db7550db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12450218-9566-41d7-b94a-f4f9aec934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932c0f9-7127-45f0-8468-015b23677b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a067e-c779-45cb-aee7-34c4660ca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# --- Directory Setup ---\n",
    "BASE_DIR = Path(\"data\") / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"data\") / \"processed_data\"\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "# --- SpaCy Setup ---\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# --- Load Annotated Data ---\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# --- Save Cleaned Data ---\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned {len(valid_data)} valid samples.\")\n",
    "print(f\"Skipped {len(invalid_data)} invalid samples.\")\n",
    "print(f\"Saved cleaned annotations to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2395795-d5b6-4e1b-95cc-e7898a446e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 4961 terms down to 4554 unique ones.\n",
      "Saved to: ..\\vocabularies\\taxonomy_lemmatized.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15b779-938f-4870-a8eb-8ece8d98893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0094bf-9ed0-4798-8fac-4869b3cf4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ddf063d-f7c9-4e14-9af3-e84418bc49f2",
   "metadata": {},
   "source": [
    "# Training an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "708eb60d-656d-4474-99c9-392efa664d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "SPACY_DIR = Path(\"..\") / \"data\" / \"spacy_data\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(SPACY_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "TRAIN_JSONL = SPACY_DIR / \"train.jsonl\"\n",
    "DEV_JSONL = SPACY_DIR / \"dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02cf0cdc-f364-434e-b2ac-9ce01c66444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 245556 train and 27284 dev examples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "split = int(len(all_data) * 0.9)\n",
    "train_data, dev_data = all_data[:split], all_data[split:]\n",
    "\n",
    "with open(TRAIN_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(DEV_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dev_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(train_data)} train and {len(dev_data)} dev examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dae3f74-d2ad-4cf9-9a24-40d0c03d3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted to .spacy format\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_to_docbin(input_path, output_path, nlp):\n",
    "    doc_bin = DocBin()\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            doc = nlp.make_doc(item[\"text\"])\n",
    "            ents = []\n",
    "            for start, end, label in item[\"label\"]:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span is not None:\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "            doc_bin.add(doc)\n",
    "    doc_bin.to_disk(output_path)\n",
    "\n",
    "# Load blank model for tokenization\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "convert_to_docbin(Path(\"../data/spacy_data/train.jsonl\"), Path(\"../data/spacy_data/train.spacy\"), nlp)\n",
    "convert_to_docbin(Path(\"../data/spacy_data/dev.jsonl\"), Path(\"../data/spacy_data/dev.spacy\"), nlp)\n",
    "\n",
    "print(\"✅ Converted to .spacy format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c3527ba-8682-4b56-988e-5cd0a9102295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "../config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ../config.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1393f16-6249-4802-8f3d-3ecd7318d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output models/env_ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c5fe0e0-10fe-4ea5-98c8-d702f5752a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okapi -> TAXONOMY\n",
      "forest -> HABITAT\n",
      "habitat -> HABITAT\n",
      "gardens -> HABITAT\n",
      "reefs -> HABITAT\n",
      "ocean sunfish -> TAXONOMY\n",
      "weddell seal -> TAXONOMY\n",
      "forests -> HABITAT\n",
      "humidity -> MEASUREMENT\n",
      "carbon -> POLLUTANT\n",
      "badger -> TAXONOMY\n",
      "red kite -> TAXONOMY\n",
      "hedgehogs -> TAXONOMY\n",
      "owls -> TAXONOMY\n",
      "dormice -> TAXONOMY\n",
      "coppice -> HABITAT\n",
      "hills -> HABITAT\n",
      "climate -> ENV_PROCESS\n",
      "bat -> TAXONOMY\n",
      "cow -> TAXONOMY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load your trained model (adjust path as needed)\n",
    "nlp = spacy.load(\"../models/env_ner/model-best\")\n",
    "\n",
    "# Long test text with mixed taxonomy terms (common + scientific)\n",
    "text = \"\"\"\n",
    "In a remote corner of Madagascar, conservationists have discovered a new population of the aye-aye, a rare lemur species known for its elongated middle finger. Meanwhile, sightings of the elusive okapi (*Okapia johnstoni*) have increased near the Ituri forest, prompting renewed efforts in habitat preservation.\n",
    "\n",
    "Botanists at Kew Gardens have cultivated the corpse flower (*Amorphophallus titanum*), which bloomed for the first time in over a decade, attracting thousands of visitors. In nearby plots, the dragon’s blood tree (*Dracaena cinnabari*) and the queen of the Andes (*Puya raimondii*) are showing promising signs of seed viability.\n",
    "\n",
    "Marine biologists studying coastal reefs in Australia reported the presence of the ocean sunfish (*Mola mola*), often referred to as the heaviest bony fish, alongside the rarer leafy seadragon. Additionally, deep-sea ROV footage captured what appears to be a specimen of *Leptonychotes weddellii*, a Weddell seal far from its usual Antarctic range.\n",
    "\n",
    "In fungi research, samples of *Clathrus archeri*, or devil’s fingers, were found in pine forests previously thought unsuitable for its growth. The zombie-ant fungus (*Ophiocordyceps unilateralis*) is also showing unusual activity due to increased humidity levels in Central American rainforests.\n",
    "\n",
    "On the microbiological front, *Bacillus subtilis* strains are being engineered to combat soil degradation, while *Prochlorococcus marinus*, one of the smallest photosynthetic organisms, continues to intrigue oceanographers for its role in carbon cycling.\n",
    "\n",
    "In local news, a badger sett was disturbed by hikers in the Peak District, raising concerns among naturalists. Meanwhile, schools in Norfolk are teaching children to identify native British species such as the common frog, red kite, and silver birch tree.\n",
    "\n",
    "The National Trust has launched a new awareness campaign featuring hedgehogs, tawny owls, and ancient oak trees to rekindle interest in local biodiversity. Interestingly, a small group of dormice was spotted nesting in a restored hazel coppice near the Chiltern Hills.\n",
    "\n",
    "Elsewhere, climate scientists in Greenland observed behavioral shifts in *Tardigrada*, microscopic organisms renowned for surviving extreme conditions, hinting at yet-unknown responses to melting permafrost.\n",
    "\n",
    "The Wildlife Photographer of the Year exhibition featured dramatic captures of a *Nannospalax leucodon* tunneling near an archaeological dig site, and a nocturnal glimpse of *Desmodus rotundus*, the common vampire bat, feeding on a stray cow near a rainforest outpost.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with your model\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "# Print predicted entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbac07-4fa2-4d62-a2a9-6c13d169b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
