{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c9e7aa-69c2-484d-a277-f39810c290a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded 0 total sentences from ../data/sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.tokens import DocBin\n",
    "import ahocorasick\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"sentences\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"json\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the input text data\n",
    "preprocessed_texts = []\n",
    "for file in BASE_DIR.rglob(\"*.txt\"):\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "        preprocessed_texts.extend(lines)\n",
    "print(f\"📄 Loaded {len(preprocessed_texts):,} total sentences from {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762c957d-87ca-431d-9973-b299897a04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_spaces(text):\n",
    "    return re.sub(r'(?<=[a-zA-Z0-9])(?=[.?!])(?=[^\\s])', r'\\g<0> ', text)\n",
    "\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [\n",
    "        fix_missing_spaces(line.strip().lower())\n",
    "        for line in f if line.strip()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f36e09-77a0-4cde-8563-9dc518cdd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_automaton(vocab_terms):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for term in vocab_terms:\n",
    "        A.add_word(term, term)\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "def is_inside_hyphenated_word(text, start, end):\n",
    "    # Check if the match is attached to another token via a hyphen\n",
    "    if start > 0 and text[start - 1] == '-':\n",
    "        return True\n",
    "    if end < len(text) and text[end] == '-':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def annotate_text_with_vocab(text, automaton, label):\n",
    "    text_length = len(text)\n",
    "    matches = []\n",
    "\n",
    "    # Iterate using the automaton\n",
    "    for end_index, term in automaton.iter(text):\n",
    "        start_index = end_index - len(term) + 1\n",
    "\n",
    "        # Whole word check\n",
    "        if (start_index == 0 or not text[start_index - 1].isalnum()) and (\n",
    "            end_index + 1 == text_length or not text[end_index + 1].isalnum()\n",
    "        ):\n",
    "            if not is_inside_hyphenated_word(text, start_index, end_index + 1):\n",
    "                matches.append([start_index, end_index + 1, label])\n",
    "\n",
    "    # Sort by start index, then longer spans first\n",
    "    matches.sort(key=lambda x: (x[0], x[1] - x[0]), reverse=False)\n",
    "    \n",
    "    # Don't block overlaps — just collect all clean, whole-word matches\n",
    "    annotations = [[start, end, label] for start, end, label in matches]\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3606d0f6-c26d-44e0-bfa3-3ed860fe7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded 196 terms to 312 (with plurals added).\n",
      "📁 Saved to: ..\\vocabularies\\meas_with_plurals.txt\n"
     ]
    }
   ],
   "source": [
    "# import inflect\n",
    "# from pathlib import Path\n",
    "\n",
    "# p = inflect.engine()\n",
    "\n",
    "# # Load vocab file\n",
    "# VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "# input_path = VOCAB_DIR / \"pollutant.txt\"\n",
    "# output_path = VOCAB_DIR / \"pollutant_with_plurals.txt\"\n",
    "\n",
    "# # Read and pluralise\n",
    "# with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     original_terms = {line.strip().lower() for line in f if line.strip()}\n",
    "\n",
    "# expanded_terms = set()\n",
    "\n",
    "# for term in original_terms:\n",
    "#     expanded_terms.add(term)\n",
    "#     # Try to pluralise single words\n",
    "#     if len(term.split()) == 1:\n",
    "#         plural = p.plural(term)\n",
    "#         if plural and plural != term:\n",
    "#             expanded_terms.add(plural)\n",
    "\n",
    "# # Write expanded vocab\n",
    "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for term in sorted(expanded_terms):\n",
    "#         f.write(term + \"\\n\")\n",
    "\n",
    "# print(f\"✅ Expanded {len(original_terms)} terms to {len(expanded_terms)} (with plurals added).\")\n",
    "# print(f\"📁 Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce544cf6-8463-40bc-8060-595fec6162c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_files = [\n",
    "    \"measurement.txt\", \n",
    "    \"pollutant.txt\", \n",
    "    \"env_process.txt\", \n",
    "    \"habitat.txt\", \n",
    "    \"taxonomy.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bb4d33-16a5-4724-a604-e26bcb514854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10977cae-7209-444a-947e-a58c0c1d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Annotating MEASUREMENT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating POLLUTANT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating ENV_PROCESS\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating HABITAT\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "🔍 Annotating TAXONOMY\n",
      "Processed 100,000/2,910,834 texts...\n",
      "Processed 200,000/2,910,834 texts...\n",
      "Processed 300,000/2,910,834 texts...\n",
      "Processed 400,000/2,910,834 texts...\n",
      "Processed 500,000/2,910,834 texts...\n",
      "Processed 600,000/2,910,834 texts...\n",
      "Processed 700,000/2,910,834 texts...\n",
      "Processed 800,000/2,910,834 texts...\n",
      "Processed 900,000/2,910,834 texts...\n",
      "Processed 1,000,000/2,910,834 texts...\n",
      "Processed 1,100,000/2,910,834 texts...\n",
      "Processed 1,200,000/2,910,834 texts...\n",
      "Processed 1,300,000/2,910,834 texts...\n",
      "Processed 1,400,000/2,910,834 texts...\n",
      "Processed 1,500,000/2,910,834 texts...\n",
      "Processed 1,600,000/2,910,834 texts...\n",
      "Processed 1,700,000/2,910,834 texts...\n",
      "Processed 1,800,000/2,910,834 texts...\n",
      "Processed 1,900,000/2,910,834 texts...\n",
      "Processed 2,000,000/2,910,834 texts...\n",
      "Processed 2,100,000/2,910,834 texts...\n",
      "Processed 2,200,000/2,910,834 texts...\n",
      "Processed 2,300,000/2,910,834 texts...\n",
      "Processed 2,400,000/2,910,834 texts...\n",
      "Processed 2,500,000/2,910,834 texts...\n",
      "Processed 2,600,000/2,910,834 texts...\n",
      "Processed 2,700,000/2,910,834 texts...\n",
      "Processed 2,800,000/2,910,834 texts...\n",
      "Processed 2,900,000/2,910,834 texts...\n",
      "✅ Done! Annotated 622,647 unique texts.\n",
      "📁 Saved to: ../data/json/training_data.jsonl\n",
      "⏱ Total time: 63.30 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "text_to_annotations = defaultdict(list)\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_name.upper()\n",
    "    print(f\"🔍 Annotating {label}\")\n",
    "\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    automaton = build_automaton(vocab_terms)\n",
    "\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        annotations = annotate_text_with_vocab(text, automaton, label)\n",
    "        if annotations:\n",
    "            text_to_annotations[text].extend(annotations)\n",
    "            text_to_annotations[text] = resolve_overlaps(text_to_annotations[text])\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Processed {i + 1:,}/{len(preprocessed_texts):,} texts...\")\n",
    "\n",
    "# Save final output\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, annotations in text_to_annotations.items():\n",
    "        json.dump({\"text\": text, \"label\": annotations}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ Done! Annotated {len(text_to_annotations):,} unique texts.\")\n",
    "print(f\"📁 Saved to: {annotated_path}\")\n",
    "print(f\"⏱ Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d597e0b9-38c3-4034-ba60-15b1b39187cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampled 1000 items to ../data/json/sample_for_manual_testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"training_data.jsonl\"\n",
    "OUTPUT_FILE = Path(\"..\") / \"data\" / \"json\" / \"sample_for_manual_testing.jsonl\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load safely, skip blank or bad lines\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"⚠️ Skipped malformed line\")\n",
    "\n",
    "# Sample\n",
    "sampled = random.sample(all_data, min(1000, len(all_data)))\n",
    "\n",
    "# Save sample\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Sampled {len(sampled)} items to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fae8-2e5f-4296-ba5b-2c17ef3cf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbec58a-70f6-40c3-ad62-b5564c3db6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8c12-6fa7-4dc8-983f-74db7550db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12450218-9566-41d7-b94a-f4f9aec934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932c0f9-7127-45f0-8468-015b23677b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a067e-c779-45cb-aee7-34c4660ca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Directory Setup ---\n",
    "BASE_DIR = Path(\"data\") / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"data\") / \"processed_data\"\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "# --- SpaCy Setup ---\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# --- Load Annotated Data ---\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# --- Save Cleaned Data ---\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned {len(valid_data)} valid samples.\")\n",
    "print(f\"Skipped {len(invalid_data)} invalid samples.\")\n",
    "print(f\"Saved cleaned annotations to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2395795-d5b6-4e1b-95cc-e7898a446e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 4961 terms down to 4554 unique ones.\n",
      "Saved to: ..\\vocabularies\\taxonomy_lemmatized.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15b779-938f-4870-a8eb-8ece8d98893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0094bf-9ed0-4798-8fac-4869b3cf4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ddf063d-f7c9-4e14-9af3-e84418bc49f2",
   "metadata": {},
   "source": [
    "# Training an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708eb60d-656d-4474-99c9-392efa664d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"json\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "SPACY_DIR = Path(\"..\") / \"data\" / \"spacy_data\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(SPACY_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILE = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "TRAIN_JSONL = SPACY_DIR / \"train.jsonl\"\n",
    "DEV_JSONL = SPACY_DIR / \"dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02cf0cdc-f364-434e-b2ac-9ce01c66444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 560382 train and 62265 dev examples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "split = int(len(all_data) * 0.9)\n",
    "train_data, dev_data = all_data[:split], all_data[split:]\n",
    "\n",
    "with open(TRAIN_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "with open(DEV_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dev_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(train_data)} train and {len(dev_data)} dev examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dae3f74-d2ad-4cf9-9a24-40d0c03d3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted to .spacy format\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_to_docbin(input_path, output_path, nlp):\n",
    "    doc_bin = DocBin()\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            doc = nlp.make_doc(item[\"text\"])\n",
    "            ents = []\n",
    "            for start, end, label in item[\"label\"]:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span is not None:\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "            doc_bin.add(doc)\n",
    "    doc_bin.to_disk(output_path)\n",
    "\n",
    "# Load blank model for tokenization\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "convert_to_docbin(Path(\"../data/spacy_data/train.jsonl\"), Path(\"../data/spacy_data/train.spacy\"), nlp)\n",
    "convert_to_docbin(Path(\"../data/spacy_data/dev.jsonl\"), Path(\"../data/spacy_data/dev.spacy\"), nlp)\n",
    "\n",
    "print(\"✅ Converted to .spacy format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c3527ba-8682-4b56-988e-5cd0a9102295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "../config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config ../config.cfg --lang en --pipeline ner --optimize accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1393f16-6249-4802-8f3d-3ecd7318d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output models/env_ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c5fe0e0-10fe-4ea5-98c8-d702f5752a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "islands -> HABITAT\n",
      "tortoise -> TAXONOMY\n",
      "forest -> HABITAT\n",
      "habitat -> HABITAT\n",
      "bogs -> HABITAT\n",
      "temperature -> MEASUREMENT\n",
      "mercury -> POLLUTANT\n",
      "wetland -> HABITAT\n",
      "fish -> TAXONOMY\n",
      "delta -> HABITAT\n",
      "habitat -> HABITAT\n",
      "forests -> HABITAT\n",
      "mammals -> TAXONOMY\n",
      "cat -> TAXONOMY\n",
      "particulate matter -> POLLUTANT\n",
      "glacier -> HABITAT\n",
      "rock -> HABITAT\n",
      "coral reef -> HABITAT\n",
      "habitats -> HABITAT\n",
      "salinity -> MEASUREMENT\n",
      "reef -> HABITAT\n",
      "habitats -> HABITAT\n",
      "habitat -> HABITAT\n",
      "pesticide -> POLLUTANT\n",
      "overgrazing -> HABITAT\n"
     ]
    }
   ],
   "source": [
    "# Load your trained model (adjust path as needed)\n",
    "nlp = spacy.load(\"../models/env_ner/model-best\")\n",
    "\n",
    "# Long test text with mixed taxonomy terms (common + scientific)\n",
    "text = \"\"\"\n",
    "Researchers in the Galápagos Islands have observed an unusual breeding pattern in Chelonoidis hoodensis, the Española giant tortoise, within fragmented dry forest patches. This habitat is increasingly threatened by rising nitrogen oxide levels, a pollutant linked to agricultural runoff.\n",
    "\n",
    "Meanwhile, in the peat bogs of northern Scotland, the rare moss Sphagnum balticum is showing signs of resilience under fluctuating temperature gradients — a direct consequence of erratic jet stream shifts, an environmental process becoming more frequent due to Arctic warming.\n",
    "\n",
    "An alarming spike in mercury concentration was recorded in wetland fish near the Mekong delta, where biodiversity is under pressure. The riverine habitat has also experienced abnormal algal blooms — a sign of nutrient loading, which is a key measurement in eutrophication studies.\n",
    "\n",
    "In southern India, the tropical deciduous forests host a variety of mammals including the rusty-spotted cat (*Prionailurus rubiginosus*), one of the smallest wild felines, which now faces threats due to increasing particulate matter in the air. This pollutant, PM2.5, has exceeded safe thresholds in over 40% of sampled areas.\n",
    "\n",
    "Glaciologists reported that meltwater runoff from the Vatnajökull glacier has doubled in the past decade — a clear signal of accelerated cryospheric change, an environmental process linked to anthropogenic CO₂ emissions. Concurrently, the lichen *Xanthoria parietina* has colonised newly exposed rock surfaces, suggesting rapid ecological succession in formerly ice-covered zones.\n",
    "\n",
    "In a monitoring project across coral reef habitats in the Seychelles, elevated salinity and pH fluctuations were documented — measurements that correlate strongly with declining populations of *Symbiodinium*, a genus of symbiotic algae critical for coral health. Oil residues, another pollutant from passing tankers, were also found on reef sediments.\n",
    "\n",
    "A surprising comeback of the great bustard (*Otis tarda*) in steppe habitats of eastern Spain was attributed to habitat restoration programmes and reduced pesticide use. The species had previously vanished due to soil degradation — a gradual environmental process driven by overgrazing and monoculture practices.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text with your model\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "# Print predicted entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0dbac07-4fa2-4d62-a2a9-6c13d169b9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk -> TAXONOMY\n",
      "glacier -> HABITAT\n",
      "valley -> HABITAT\n",
      "glacier -> HABITAT\n",
      "glacier -> HABITAT\n",
      "climate change -> ENV_PROCESS\n",
      "rivers -> HABITAT\n",
      "glue -> POLLUTANT\n",
      "mountains -> HABITAT\n",
      "glacier -> HABITAT\n",
      "avalanche -> ENV_PROCESS\n",
      "valley -> HABITAT\n",
      "cloud -> ENV_PROCESS\n",
      "dust -> POLLUTANT\n",
      "landslide -> HABITAT\n",
      "glaciers -> HABITAT\n",
      "temperatures -> MEASUREMENT\n",
      "paris -> TAXONOMY\n",
      "climate -> ENV_PROCESS\n",
      "climate -> ENV_PROCESS\n",
      "glacier -> HABITAT\n",
      "landslides -> HABITAT\n"
     ]
    }
   ],
   "source": [
    "# Load your trained model (adjust path as needed)\n",
    "nlp = spacy.load(\"../models/env_ner/model-best\")\n",
    "\n",
    "# Long test text with mixed taxonomy terms (common + scientific)\n",
    "text = \"\"\"\n",
    "The Swiss village of Blatten has been partially destroyed after a huge chunk of glacier crashed down into the valley.\n",
    "\n",
    "Although the village had been evacuated some days ago because of fears the Birch glacier was disintegrating, one person has been reported missing, and many homes have been completely flattened.\n",
    "\n",
    "Blatten's mayor, Matthias Bellwald, said \"the unimaginable has happened\" but promised the village still had a future.\n",
    "\n",
    "Local authorities have requested support from the Swiss army's disaster relief unit and members of the Swiss government are on their way to the scene.\n",
    "\n",
    "The disaster that has befallen Blatten is the worst nightmare for communities across the Alps.\n",
    "\n",
    "The village's 300 inhabitants had to leave their homes on 19 May after geologists monitoring the area warned that the glacier appeared unstable. Now many of them may never be able to return.\n",
    "\n",
    "Appearing to fight back tears, Bellwald said: \"We have lost our village, but not our heart. We will support each other and console each other. After a long night, it will be morning again.\"\n",
    "\n",
    "The Swiss government has already promised funding to make sure residents can stay, if not in the village itself, at least in the locality.\n",
    "\n",
    "However, Raphaël Mayoraz, head of the regional office for Natural Hazards, warned that further evacuations in the areas close to Blatten might be necessary.\n",
    "\n",
    "Climate change is causing the glaciers - frozen rivers of ice - to melt faster and faster, and the permafrost, often described as the glue that holds the high mountains together, is also thawing.\n",
    "\n",
    "Drone footage showed a large section of the Birch glacier collapsing at about 15:30 (14:30 BST) on Wednesday. The avalanche of mud that swept over Blatten sounded like a deafening roar, as it swept down into the valley leaving an enormous cloud of dust.\n",
    "\n",
    "Glaciologists monitoring the thaw have warned for years that some alpine towns and villages could be at risk, and Blatten is not even the first to be evacuated.\n",
    "\n",
    "In eastern Switzerland, residents of the village of Brienz were evacuated two years ago because the mountainside above them was crumbling.\n",
    "\n",
    "Since then, they have only been permitted to return for short periods.\n",
    "\n",
    "In 2017, eight hikers were killed, and many homes destroyed, when the biggest landslide in over a century came down close to the village of Bondo.\n",
    "\n",
    "The most recent report into the condition of Switzerland's glaciers suggested they could all be gone within a century, if global temperatures could not be kept within a rise of 1.5C above pre-industrial levels, agreed ten years ago by almost 200 countries under the Paris climate accord.\n",
    "\n",
    "Many climate scientists suggest that target has already been missed, meaning the glacier thaw will continue to accelerate, increasing the risk of flooding and landslides, and threatening more communities like Blatten.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Process the text with your model\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "# Print predicted entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbfc2b-bf8d-4270-84a6-4aa3269d4626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
