{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c9e7aa-69c2-484d-a277-f39810c290a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the input text data\n",
    "with open(BASE_DIR / \"env_data.txt\", encoding=\"utf-8\") as f:\n",
    "    preprocessed_texts = [line.strip().lower() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f36e09-77a0-4cde-8563-9dc518cdd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "\n",
    "def build_automaton(vocab_terms):\n",
    "    A = ahocorasick.Automaton()\n",
    "    for term in vocab_terms:\n",
    "        A.add_word(term, term)\n",
    "    A.make_automaton()\n",
    "    return A\n",
    "\n",
    "def annotate_text_with_vocab(text, automaton, label):\n",
    "    text_length = len(text)\n",
    "    matches = []\n",
    "\n",
    "    # Iterate using the automaton\n",
    "    for end_index, term in automaton.iter(text):\n",
    "        start_index = end_index - len(term) + 1\n",
    "\n",
    "        # Whole word check\n",
    "        if (start_index == 0 or not text[start_index - 1].isalnum()) and (\n",
    "            end_index + 1 == text_length or not text[end_index + 1].isalnum()\n",
    "        ):\n",
    "            matches.append([start_index, end_index + 1, label])\n",
    "\n",
    "    # Sort by start index, then longer spans first\n",
    "    matches.sort(key=lambda x: (x[0], x[1] - x[0]), reverse=False)\n",
    "    \n",
    "    # Don't block overlaps â€” just collect all clean, whole-word matches\n",
    "    annotations = [[start, end, label] for start, end, label in matches]\n",
    "\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3606d0f6-c26d-44e0-bfa3-3ed860fe7b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Expanded 196 terms to 312 (with plurals added).\n",
      "ðŸ“ Saved to: ..\\vocabularies\\meas_with_plurals.txt\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "from pathlib import Path\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "# Load vocab file\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "input_path = VOCAB_DIR / \"pollutant.txt\"\n",
    "output_path = VOCAB_DIR / \"pollutant_with_plurals.txt\"\n",
    "\n",
    "# Read and pluralise\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_terms = {line.strip().lower() for line in f if line.strip()}\n",
    "\n",
    "expanded_terms = set()\n",
    "\n",
    "for term in original_terms:\n",
    "    expanded_terms.add(term)\n",
    "    # Try to pluralise single words\n",
    "    if len(term.split()) == 1:\n",
    "        plural = p.plural(term)\n",
    "        if plural and plural != term:\n",
    "            expanded_terms.add(plural)\n",
    "\n",
    "# Write expanded vocab\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in sorted(expanded_terms):\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Expanded {len(original_terms)} terms to {len(expanded_terms)} (with plurals added).\")\n",
    "print(f\"ðŸ“ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce544cf6-8463-40bc-8060-595fec6162c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_files = [\n",
    "    \"measurement.txt\", \n",
    "    \"pollutant.txt\", \n",
    "    \"env_process.txt\", \n",
    "    \"habitat.txt\", \n",
    "    \"taxonomy.txt\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46bb4d33-16a5-4724-a604-e26bcb514854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10977cae-7209-444a-947e-a58c0c1d4253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating category: MEASUREMENT from measurement.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: POLLUTANT from pollutant.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: ENV_PROCESS from env_process.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: HABITAT from habitat.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "Annotating category: TAXONOMY from taxonomy.txt\n",
      "Processed 100,000/564,547 texts...\n",
      "Processed 200,000/564,547 texts...\n",
      "Processed 300,000/564,547 texts...\n",
      "Processed 400,000/564,547 texts...\n",
      "Processed 500,000/564,547 texts...\n",
      "âœ… Done! Annotated 281,591 unique texts.\n",
      "ðŸ“ Saved to: ..\\data\\processed_data\\training_data.jsonl\n",
      "â± Total time: 59.79 seconds\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "text_to_annotations = defaultdict(list)\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_name.upper()\n",
    "    print(f\"Annotating category: {label} from {fname}\")\n",
    "\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    automaton = build_automaton(vocab_terms)\n",
    "\n",
    "    for i, text in enumerate(preprocessed_texts):\n",
    "        annotations = annotate_text_with_vocab(text, automaton, label)\n",
    "        if annotations:\n",
    "            text_to_annotations[text].extend(annotations)\n",
    "            text_to_annotations[text] = resolve_overlaps(text_to_annotations[text])\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Processed {i + 1:,}/{len(preprocessed_texts):,} texts...\")\n",
    "\n",
    "# Save final output\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text, annotations in text_to_annotations.items():\n",
    "        json.dump({\"text\": text, \"label\": annotations}, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Done! Annotated {len(text_to_annotations):,} unique texts.\")\n",
    "print(f\"ðŸ“ Saved to: {annotated_path}\")\n",
    "print(f\"â± Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d597e0b9-38c3-4034-ba60-15b1b39187cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sampled 1000 items to ..\\data\\processed_data\\sample_for_manual_testing.jsonl\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_FILE = Path(\"..\") / \"data\" / \"processed_data\" / \"training_data.jsonl\"\n",
    "OUTPUT_FILE = Path(\"..\") / \"data\" / \"processed_data\" / \"sample_for_manual_testing.jsonl\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Load safely, skip blank or bad lines\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš ï¸ Skipped malformed line\")\n",
    "\n",
    "# Sample\n",
    "sampled = random.sample(all_data, min(1000, len(all_data)))\n",
    "\n",
    "# Save sample\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"âœ… Sampled {len(sampled)} items to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4aa003-7c3a-42c6-a5d7-4aeace749703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fae8-2e5f-4296-ba5b-2c17ef3cf83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbec58a-70f6-40c3-ad62-b5564c3db6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8c12-6fa7-4dc8-983f-74db7550db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12450218-9566-41d7-b94a-f4f9aec934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932c0f9-7127-45f0-8468-015b23677b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a067e-c779-45cb-aee7-34c4660ca928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# --- Directory Setup ---\n",
    "BASE_DIR = Path(\"data\") / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"data\") / \"processed_data\"\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "# --- SpaCy Setup ---\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# --- Load Annotated Data ---\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# --- Save Cleaned Data ---\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Cleaned {len(valid_data)} valid samples.\")\n",
    "print(f\"Skipped {len(invalid_data)} invalid samples.\")\n",
    "print(f\"Saved cleaned annotations to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2395795-d5b6-4e1b-95cc-e7898a446e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized 4961 terms down to 4554 unique ones.\n",
      "Saved to: ..\\vocabularies\\taxonomy_lemmatized.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15b779-938f-4870-a8eb-8ece8d98893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0094bf-9ed0-4798-8fac-4869b3cf4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860184e-08dc-4ca8-87b4-c4e751d68bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708eb60d-656d-4474-99c9-392efa664d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef9f3d83-e223-4596-8f86-f06e2ea40fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split complete. 4963 single-word terms and 66979 multi-word terms saved.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# File paths\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "single_output = VOCAB_DIR / \"taxonomy_single_word.txt\"\n",
    "multi_output = VOCAB_DIR / \"taxonomy_multi_word.txt\"\n",
    "\n",
    "# Read taxonomy terms\n",
    "with open(input_path, encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Separate by word count\n",
    "single_word_terms = [term for term in terms if len(term.split()) == 1]\n",
    "multi_word_terms = [term for term in terms if len(term.split()) > 1]\n",
    "\n",
    "# Save them\n",
    "with open(single_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in sorted(set(single_word_terms)):\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "with open(multi_output, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in sorted(set(multi_word_terms)):\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Split complete. {len(single_word_terms)} single-word terms and {len(multi_word_terms)} multi-word terms saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf0cdc-f364-434e-b2ac-9ce01c66444c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
