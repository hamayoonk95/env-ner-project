{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe5d15a-ad7c-41af-98e7-d39b433fe9d5",
   "metadata": {},
   "source": [
    "# Vocabulary Curation for Environmental Science NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8cb71-f73d-494e-afa4-06d2b01395f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Background and Purpose\n",
    "\n",
    "This notebook focuses on building a vocabulary of environmental terms to support downstream Named Entity Recognition (NER). It is the second step in a pipeline designed to develop a domain-specific NER dataset, following on from the raw text preparation carried out in `01_data_collection.ipynb`.\n",
    "\n",
    "The aim is to identify and organise key domain-specific terms that appear in environmental texts. These include species names, habitat types, ecological processes, pollutants, and scientific measurements. General-purpose NER models often fail to capture such terms accurately, as they are either underrepresented or entirely absent in mainstream corpora.\n",
    "\n",
    "The vocabulary lists created here will be used in the next step to apply rule-based annotation. This pre-annotation will enable the creation of weakly labelled data, which will then be used to train statistical and machine learning models for environmental NER. As a result, this stage is critical in determining the coverage and relevance of named entities in the training set.\n",
    "\n",
    "Vocabulary terms are gathered from a range of publicly available sources, including domain-specific glossaries, classification systems, and structured datasets. Some are extracted using scripts, while others are compiled manually. The final outputs will be cleaned, deduplicated, and stored in a structured format ready for annotation.\n",
    "\n",
    "### 1.2 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "- To define a set of named entity categories that reflect the types of concepts commonly found in environmental science\n",
    "- To identify and extract vocabulary terms for each category from trusted sources, including ontologies, glossaries, and environmental datasets\n",
    "- To standardise and clean the collected terms, removing duplicates and formatting them consistently\n",
    "- To save the resulting vocabulary files in a format suitable for rule-based matching and future model training\n",
    "\n",
    "Each category is handled in a separate section, with a clear explanation of sources used, any filtering applied, and a final cleaned list. These outputs will be used to annotate the sentence-level dataset produced earlier in the pipeline.\n",
    "\n",
    "### 1.3 Challenges in Vocabulary Development\n",
    "\n",
    "Developing vocabulary lists for environmental NER presents several challenges that need to be addressed carefully:\n",
    "\n",
    "- **Choosing meaningful categories**  \n",
    "  There is no single standard for environmental entity types. Categories must be defined based on project needs, typical text content, and the structure of existing ontologies. Each category should be well defined and distinct from the others.\n",
    "\n",
    "- **Finding reliable and relevant sources**  \n",
    "  Domain-specific terms are spread across many different resources. These resources vary in structure and accessibility. Some are easy to extract from; others require manual effort or custom scripts. Some of these are easy to extract; others require manual effort or custom scripts.\n",
    "\n",
    "- **Managing ambiguity**  \n",
    "  Certain terms have multiple meanings in different contexts. For example, “lead” may refer to a pollutant or act as a verb. These terms need to be flagged and treated cautiously during annotation.\n",
    "\n",
    "- **Balancing coverage with precision**  \n",
    "  A broad vocabulary may capture more entities but also introduces noise. It is important to focus on terms that are likely to appear in the corpus and are useful for the task. Rare, overly general, or ambiguous entries may be excluded or reviewed manually.\n",
    "\n",
    "- **Handling formatting inconsistencies**  \n",
    "  Terms from different sources may vary in spelling, case, pluralisation, or punctuation. A standard cleaning process is needed to normalise these variations and ensure consistent matching later.\n",
    "\n",
    "These challenges underline the need for a clear and reproducible method for vocabulary collection. Each step must be documented and carefully structured to ensure high-quality results in the next phase of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fb50b-ae17-4a36-b7b7-95d990aec703",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Defining Entity Categories\n",
    "\n",
    "### 2.1 Choosing the Entity Categories\n",
    "\n",
    "Named Entity Recognition in environmental science requires clearly defined entity types that reflect the domain’s specialised vocabulary. General NER categories, such as “Organisation” or “Location,” are not specific enough to capture the kinds of entities that appear in environmental text. Terms such as species names, habitat descriptors, or chemical pollutants need to be grouped meaningfully to support accurate tagging.\n",
    "\n",
    "The categories chosen for this notebook aim to balance three key criteria: specificity, clarity, and practical relevance. Each category should represent a well-understood environmental concept, be distinct from the others, and be useful for annotating terms that frequently appear in environmental documents. This includes both scientific abstracts and semi-structured metadata.\n",
    "\n",
    "The category design also draws on patterns seen in domain vocabularies, classification systems, and earlier review of the text corpus collected earlier. These sources reveal recurring term types such as biological taxa, ecosystem names, pollution indicators, and measurement units. Grouping these into separate categories improves both annotation consistency and future model training.\n",
    "\n",
    "### 2.2 Category Review Process\n",
    "The selection of entity categories was based on a review of environmental materials and the text data collected earlier. The aim was to identify recurring patterns in how key concepts are mentioned and described across different sources.\n",
    "\n",
    "To guide this process, a range of domain-specific materials were considered, including glossaries, classification systems, and datasets. These were not used to define categories directly, but to observe common term types that appear in environmental science. In parallel, the collected corpus was manually reviewed to spot frequent entities and assess their natural groupings.\n",
    "\n",
    "The process was iterative. Some categories, such as species names and pollutants, were clearly distinguishable from the start. Others, like environmental processes or measurements, required refinement to avoid overlapping meanings or inconsistent boundaries. Wherever possible, categories were defined to be specific, distinct, and useful for labelling terms that appear regularly in environmental text.\n",
    "\n",
    "### 2.3 Final Entity Categories\n",
    "\n",
    "Based on the review process above, five named entity categories were selected. Each category is designed to capture a specific type of domain-relevant term found in environmental science text.\n",
    "\n",
    "| Entity Category | Description | Example Terms |\n",
    "|------------------|-------------|----------------|\n",
    "| TAXONOMY         | Names of species, genera, families, or other taxonomic units | *Salmo salar*, *Panthera leo* |\n",
    "| HABITAT          | Names of ecosystems, land cover types, or habitat descriptors | estuary, peatland, saltmarsh |\n",
    "| ENV_PROCESS      | Environmental or ecological processes, including both natural and human-driven events | erosion, eutrophication, acidification |\n",
    "| POLLUTANT        | Chemical or physical substances known to cause pollution or environmental harm | mercury, nitrate, microplastics |\n",
    "| MEASUREMENT      | Units, quantities, or indicators used to measure environmental variables | pH, mg/L, temperature, CO₂ levels |\n",
    "\n",
    "These categories are used to group vocabulary terms in the next sections. Each term list is built and processed separately, then saved in a structured format for annotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b248cc7-ad83-4760-898d-b2c382e1d31e",
   "metadata": {},
   "source": [
    "## 3. Collecting Vocabulary Terms\n",
    "This section focuses on building the vocabulary lists for each of the entity categories defined earlier. For each category, relevant terms are collected from domain-specific resources such as glossaries, classification systems, and downloadable datasets. Where possible, structured sources are processed programmatically. Others are manually reviewed or extracted depending on format and accessibility.\n",
    "\n",
    "The goal is to compile targeted, category-specific lists that reflect the kinds of entities typically found in environmental science texts. Each vocabulary is cleaned, standardised, and saved in a structured format for later use in annotation and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06a429c5-af4c-4b5c-b4b7-1bbcd5385843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"../vocabs\")\n",
    "RAW_PATH = BASE_PATH / \"raw\"\n",
    "EXTRACTED_PATH = BASE_PATH / \"extracted\"\n",
    "FINAL_PATH = BASE_PATH / \"final\"\n",
    "\n",
    "RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXTRACTED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "FINAL_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaf952-52b9-409c-b5cc-5fae26d02965",
   "metadata": {},
   "source": [
    "### 3.1 Taxonomy\n",
    "\n",
    "The taxonomy vocabulary is based on the GBIF Backbone Taxonomy dataset [link](https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c), which provides a large set of biological names across all major taxonomic groups. The raw file was downloaded as a TSV and includes a wide range of vernacular terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235e85f-eddf-4fce-8038-5a8a184552e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonID</th>\n",
       "      <th>vernacularName</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>sex</th>\n",
       "      <th>lifeStage</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5371864</td>\n",
       "      <td>sels næp</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5371864</td>\n",
       "      <td>spreng-rood</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5371864</td>\n",
       "      <td>syle-næbber</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3034225</td>\n",
       "      <td>angelsrot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3034225</td>\n",
       "      <td>mjølkerot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   taxonID vernacularName language country countryCode  sex lifeStage  \\\n",
       "0  5371864       sels næp       no     NaN         NaN  NaN       NaN   \n",
       "1  5371864    spreng-rood       no     NaN         NaN  NaN       NaN   \n",
       "2  5371864    syle-næbber       no     NaN         NaN  NaN       NaN   \n",
       "3  3034225      angelsrot      NaN     NaN         NaN  NaN       NaN   \n",
       "4  3034225      mjølkerot      NaN     NaN         NaN  NaN       NaN   \n",
       "\n",
       "                                     source  \n",
       "0  Nordic plant uses from Gunnerus and Høeg  \n",
       "1  Nordic plant uses from Gunnerus and Høeg  \n",
       "2  Nordic plant uses from Gunnerus and Høeg  \n",
       "3  Nordic plant uses from Gunnerus and Høeg  \n",
       "4  Nordic plant uses from Gunnerus and Høeg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RAW_PATH / \"VernacularName.tsv\", sep='\\t', dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1de8e-2c61-4315-a7f9-42cee573a69c",
   "metadata": {},
   "source": [
    "An initial preview of the file revealed that the entries span multiple languages, stored under a `language` column. Since the corpus being annotated is in English, only entries marked as English (`language == 'en'`) are retained for this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a51dce83-e5fa-44a1-9bcb-ce4a15a8fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_PATH / \"VernacularName.tsv\", sep='\\t', usecols=[\"vernacularName\", \"language\"], dtype=str)\n",
    "\n",
    "df = df[df[\"language\"] == \"en\"]\n",
    "df = df.dropna(subset=[\"vernacularName\"])\n",
    "df[\"vernacularName\"] = df[\"vernacularName\"].str.strip().str.lower()\n",
    "df = df[df[\"vernacularName\"] != \"\"]\n",
    "\n",
    "taxonomy_terms = df[\"vernacularName\"].drop_duplicates().sort_values()\n",
    "\n",
    "taxonomy_path = EXTRACTED_PATH / \"taxonomy.txt\"\n",
    "taxonomy_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "taxonomy_terms.to_csv(taxonomy_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf81422e-d4f4-4ea2-866b-fb303f4fcdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229559\n"
     ]
    }
   ],
   "source": [
    "print(len(taxonomy_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fda9ce79-7e69-4901-84d7-f60ebd7fe07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandoned industrial site\n",
      "accidental release of organisms\n",
      "afforestation\n",
      "afforestations\n",
      "agricultural landscape\n",
      "alluvial plain\n",
      "altitude\n",
      "altitudes\n",
      "amusement park\n",
      "anaerobic lagoon\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539757b-8946-4545-a0ff-84eefa96835f",
   "metadata": {},
   "source": [
    "The extracted list contains approximately 229,000 unique English-language names. From a brief manual inspection, the terms appear accurate, well-formed, and highly relevant to biological taxonomy. Many include common species and group-level names, making this a strong starting point.\n",
    "\n",
    "While the number of terms is large, it is not expected to negatively impact annotation. Any names not found in the corpus will simply not be matched. Basic cleaning will be applied later to remove duplicates, overly generic names, or problematic entries where necessary.\n",
    "\n",
    "Due to the size of the list, full manual review is not feasible. However, spot checks indicate the data quality is high. A standardised version of the vocabulary will be prepared during the cleaning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac70d01-ebc0-4cdd-a8b3-d8a3c1c7a39e",
   "metadata": {},
   "source": [
    "### 3.2 Habitat\n",
    "The habitat vocabulary is based on the GEMET thesaurus, which includes a wide range of environmental and ecological terms. It provides structured terminology for describing natural environments, land cover types, and habitat-related concepts. The terms were downloaded in structured format and filtered for habitat-relevant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "235176f5-e1fe-44b3-8b4b-735cfe348b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gemet_theme(theme_id: int):\n",
    "    \"\"\"Scrapes GEMET terms under a given theme and writes them to a text file\"\"\"\n",
    "    \n",
    "    base_url = f\"https://www.eionet.europa.eu/gemet/en/theme/{theme_id}/concepts/?page={{}}&letter=0\"\n",
    "    \n",
    "    page = 1\n",
    "    terms = set()\n",
    "\n",
    "    print(f\"Scraping theme ID {theme_id}\")\n",
    "    \n",
    "    while True:\n",
    "        url = base_url.format(page)\n",
    "        print(f\"Page {page}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        term_elements = soup.select(\"ul.listing.columns.split-20 li a\")\n",
    "        if not term_elements:\n",
    "            break\n",
    "\n",
    "        new_terms = {el.get_text(strip=True) for el in term_elements}\n",
    "        if new_terms.issubset(terms):\n",
    "            break\n",
    "\n",
    "        terms.update(new_terms)\n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return sorted(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adce0ced-91cf-422f-aa8f-2061a7c38aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping theme ID 23\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Saved habitat terms to habitat.txt\n"
     ]
    }
   ],
   "source": [
    "habitat_terms = scrape_gemet_theme(theme_id=23)\n",
    "\n",
    "output_path = EXTRACTED_PATH / \"habitat.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in habitat_terms:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Saved habitat terms to {output_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eeb6517-ea0f-4ded-bbb0-76359c593b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n"
     ]
    }
   ],
   "source": [
    "print(len(habitat_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5acb88f-8111-4561-ae7c-a1b8fc9d777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperate woodland\n",
      "wildlife sanctuary\n",
      "dam draining\n",
      "coast protection\n",
      "rivers\n",
      "periurban space\n",
      "site rehabilitation\n",
      "area under stress\n",
      "resource scarcity\n",
      "bays\n",
      "forest fire\n",
      "cultural ecosystem services\n",
      "coastal environment\n",
      "fens\n",
      "nesting area\n",
      "forest\n",
      "forest reserve\n",
      "integrated environmental assessment\n",
      "bocages\n",
      "abandoned industrial site\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bf8fc-f4c6-4a2f-9a38-2442e3743285",
   "metadata": {},
   "source": [
    "The extracted list for habitat terms includes 467 unique entries from the GEMET thesaurus under the \"natural areas\" theme. While this number is smaller compared to the taxonomy list, it is expected since habitats are a more constrained and well-defined concept, with fewer distinct variations than species names.\n",
    "\n",
    "The list covers a broad set of environments, including terms such as *marshes*, *coastal environment*, *catchment area*, *bog*, *sand dune fixation*, *mountain forest*, and *terraced landscape*. These are well-formed, domain-relevant, and reflect both natural and managed habitats.\n",
    "\n",
    "Initial inspection confirms the quality and relevance of the terms. The vocabulary will be further cleaned and standardised before annotation, but no immediate expansion is needed unless specific gaps are identified later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfae0b-802d-4efd-aa1d-d51feeb8e71b",
   "metadata": {},
   "source": [
    "### 3.3 Environmental Processes\n",
    "This category includes vocabulary terms that describe natural, physical, or chemical processes occurring within the environment. These may include terms related to climate, hydrology, soil dynamics, pollution cycles, and other system-level interactions commonly found in environmental science texts.\n",
    "\n",
    "The goal is to capture processes that are relevant to ecological modelling, environmental monitoring, and scientific descriptions of system change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fd76f93-8981-4e6e-bd13-9ad2edd9157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping theme ID 7\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Saved env_process terms to env_process.txt\n"
     ]
    }
   ],
   "source": [
    "env_process_path = EXTRACTED_PATH / \"env_process.txt\"\n",
    "\n",
    "climate_terms = scrape_gemet_theme(theme_id=7)\n",
    "\n",
    "with open(env_process_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in climate_terms:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Saved env_process terms to {env_process_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c6971ac-0a7e-4682-9fa5-f1149155aee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(climate_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e1ca632-373a-4a3b-9a66-e028e645b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation enhancement\n",
      "season\n",
      "meteorological disaster\n",
      "oceanic climate\n",
      "flood forecast\n",
      "haze\n",
      "atmospheric structure\n",
      "climate regulation\n",
      "tornado\n",
      "weather modification\n",
      "snow\n",
      "storm damage\n",
      "air conditioning\n",
      "troposphere\n",
      "meteorological research\n",
      "mountain climate\n",
      "ozone layer\n",
      "atmospheric precipitation\n",
      "climate protection\n",
      "atmospheric composition\n"
     ]
    }
   ],
   "source": [
    "with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40ef20-3323-4885-a332-97a0c5c777c7",
   "metadata": {},
   "source": [
    "The environmental process vocabulary was initially sourced from the GEMET thesaurus under the \"climate\" theme, which produced 128 terms. This list includes core concepts such as *global warming*, *ozone layer*, *atmospheric composition*, *feedback loop*, and *water scarcity*.\n",
    "\n",
    "While these are relevant and well-formed, the coverage is narrow and heavily climate-focused. Environmental processes span a wider range of concepts including soil dynamics, chemical pollution cycles, hydrology, and ecosystem-level change. These are not well represented in the current list.\n",
    "\n",
    "Additional sources will be explored to expand this vocabulary, including manually compiled lists and process-focused glossaries or ontologies. The GEMET terms will still be included as part of the final vocabulary after cleaning and standardisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e86504ad-4329-4c26-a6ae-1bb6fa812695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: climate\n",
      "Scraping theme ID 5\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Scraping: pollution\n",
      "Scraping theme ID 26\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Scraping: natural_dynamics\n",
      "Scraping theme ID 36\n",
      "Page 1\n",
      "Page 2\n"
     ]
    }
   ],
   "source": [
    "theme_ids = {\n",
    "    \"climate\": 5,\n",
    "    \"pollution\": 26,\n",
    "    \"natural_dynamics\": 36,\n",
    "}\n",
    "\n",
    "all_terms = set()\n",
    "\n",
    "for theme, theme_id in theme_ids.items():\n",
    "    print(f\"Scraping: {theme}\")\n",
    "    \n",
    "    terms = scrape_gemet_theme(theme_id)\n",
    "    all_terms.update(term.lower() for term in terms if term.strip())\n",
    "    \n",
    "    with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            term = line.strip().lower()\n",
    "            if term:\n",
    "                all_terms.add(term)\n",
    "\n",
    "with open(env_process_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in sorted(all_terms):\n",
    "        f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd96a917-f92e-4772-b3e4-334e06039581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(all_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed8b1fff-f21b-47d7-befc-c3f6fcf08e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microclimate\n",
      "land restoration\n",
      "combined cycle-power station\n",
      "tundra\n",
      "bleaching process\n",
      "anaerobic treatment\n",
      "advection\n",
      "building service\n",
      "chemical decontamination\n",
      "environmental impact of agriculture\n",
      "hail\n",
      "salination\n",
      "polluter-pays principle\n",
      "olfactory pollution\n",
      "nursery garden\n",
      "water pollution\n",
      "pollution monitoring\n",
      "building site\n",
      "physical alteration\n",
      "sea level rise\n"
     ]
    }
   ],
   "source": [
    "with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24885b-182d-476c-8511-99cbb70fe2da",
   "metadata": {},
   "source": [
    "The expanded list for environmental processes now includes 1,003 unique terms aggregated from multiple GEMET themes, including climate, pollution, and natural dynamics. This broader coverage captures a wide range of relevant concepts such as microclimate, chemical decontamination, salination, environmental impact of agriculture, and pollution monitoring. While some entries are loosely defined or overlap with other categories, the overall list is diverse and strongly aligned with the types of system-level processes found in environmental science literature. The vocabulary will be reviewed and refined in the next stage to remove ambiguous or overly general terms, but the current coverage is considered sufficient for initial annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ad82e-a726-4adf-a663-970a4ae52a2a",
   "metadata": {},
   "source": [
    "### 3.4 Pollutants\n",
    "The pollutants vocabulary includes chemical substances and other environmental stressors that are commonly referenced in environmental science. These may be individual compounds such as benzene or lead, or broader classes such as pesticides, microplastics, or particulate matter. These terms are often found in regulatory reports, monitoring programmes, and scientific publications that focus on pollution, exposure, and environmental impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5f67e-b06a-4850-9cbe-dfc91ec6d584",
   "metadata": {},
   "source": [
    "To build this vocabulary, the Toxics Release Inventory (TRI) Chemical List was selected as the source. This is a publicly available list of substances that are tracked under the US EPA’s TRI reporting programme, which requires facilities to report on the management of certain toxic chemicals. The list is maintained as part of the EPA’s environmental transparency efforts and includes both individual chemicals and chemical categories.\n",
    "\n",
    "The full list was downloaded in spreadsheet format from the following source:\n",
    "https://www.epa.gov/toxics-release-inventory-tri-program/tri-listed-chemicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28aff3ae-6889-4c1c-884a-6efa6533469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASRN</th>\n",
       "      <th>TRI Chemical or Chemical Category Name</th>\n",
       "      <th>Chemical Structure</th>\n",
       "      <th>De Minimis Limit %</th>\n",
       "      <th>M,P/OU Thresholds (lb)</th>\n",
       "      <th>Category Description</th>\n",
       "      <th>Category Member</th>\n",
       "      <th>Additional Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71751-41-2</td>\n",
       "      <td>Abamectin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30560-19-1</td>\n",
       "      <td>Acephate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75-07-0</td>\n",
       "      <td>Acetaldehyde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60-35-5</td>\n",
       "      <td>Acetamide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75-05-8</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CASRN TRI Chemical or Chemical Category Name  Chemical Structure  \\\n",
       "0  71751-41-2                               Abamectin                NaN   \n",
       "1  30560-19-1                                Acephate                NaN   \n",
       "2     75-07-0                            Acetaldehyde                NaN   \n",
       "3     60-35-5                               Acetamide                NaN   \n",
       "4     75-05-8                            Acetonitrile                NaN   \n",
       "\n",
       "  De Minimis Limit % M,P/OU Thresholds (lb) Category Description  \\\n",
       "0                  1          25,000/10,000                  NaN   \n",
       "1                  1          25,000/10,000                  NaN   \n",
       "2                0.1          25,000/10,000                  NaN   \n",
       "3                0.1          25,000/10,000                  NaN   \n",
       "4                  1          25,000/10,000                  NaN   \n",
       "\n",
       "  Category Member Additional Information  \n",
       "0             NaN                    NaN  \n",
       "1             NaN                    NaN  \n",
       "2             NaN                    NaN  \n",
       "3             NaN                    NaN  \n",
       "4             NaN                    NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(RAW_PATH / \"2024_tri_chemical_list.xlsx\", engine=\"openpyxl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e974c0-d897-48cf-a6ed-abd6f0ed29d3",
   "metadata": {},
   "source": [
    "The TRI chemical file includes a structured list of pollutant names, along with metadata such as CAS numbers, thresholds, and classification notes. A preview of the dataset shows chemical entries like Abamectin, Acetaldehyde, and Acetonitrile, which reflect a balance of technical specificity and practical relevance.\n",
    "\n",
    "This initial inspection confirms that the file contains suitable pollutant terms for vocabulary extraction. Further processing will focus on selecting and standardising these names into a plain text list for use in annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07402b9d-3c5f-4c33-9189-9deb44622f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "pollutant_terms = (\n",
    "    df[\"TRI Chemical or Chemical Category Name\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .drop_duplicates()\n",
    "    .sort_values()\n",
    ")\n",
    "\n",
    "pollutant_path = EXTRACTED_PATH / \"pollutant.txt\"\n",
    "pollutant_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "pollutant_terms.to_csv(pollutant_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05d230dd-0c5b-49c8-9064-536b24121276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pollutant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "698ccb56-4803-4460-b424-1bf5ee0a475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dichlorobromomethane\n",
      "nickel\n",
      "paraldehyde\n",
      "acrolein\n",
      "bromine\n",
      "dihydrosafrole\n",
      "cycloate\n",
      "\"3,3'-dimethylbenzidine dihydrofluoride\"\n",
      "\"3,3'-dimethylbenzidine dihydrochloride\"\n",
      "triforine\n",
      "diaminotoluene (mixed isomers) (toluenediamine)\n",
      "c.i. direct blue 218\n",
      "\"hexamethylene-1,6-diisocyanate\"\n",
      "\"1,2-dichloro-1,1,3,3,3-pentafluoropropane (hcfc-225da)\"\n",
      "\"1,2,3,7,8‑pentachlorodibenzo-p-dioxin\"\n",
      "chlorothalonil\n",
      "2-nitrophenol (o-nitrophenol)\n",
      "perchloromethyl mercaptan\n",
      "sodium azide\n",
      "\"1,1,2,2-tetrachloroethane\"\n"
     ]
    }
   ],
   "source": [
    "with open(pollutant_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac192e7-f993-46c5-bb2b-23238fe52dc1",
   "metadata": {},
   "source": [
    "A total of 728 unique pollutant terms were extracted from the TRI list. The sample includes a mix of well-formed names and more complex entries, such as those wrapped in quotation marks or containing grouped chemicals.\n",
    "\n",
    "Basic cleaning will be applied to remove entries with formatting issues or compound categories that are too broad for direct annotation. Since the list is relatively small, this will be done manually.\n",
    "\n",
    "Some common terms like pesticide, ozone, or microplastic are not included in the TRI list. These will be added separately to improve coverage across general environmental texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be16e3-823d-447a-bf4d-a187c3f8e640",
   "metadata": {},
   "source": [
    "### 3.5 Measurements\n",
    "This category covers vocabulary related to physical units, environmental quantities, and measurement-related descriptors. These often appear alongside named entities or as standalone spans in scientific texts, particularly when reporting thresholds, concentrations, or observed values.\n",
    "\n",
    "Examples include standard expressions such as mg/L, µg/m3, °C, ppm, as well as context-specific terms like emission factor, exceedance, and threshold concentration. Recognising these phrases is important for accurate annotation and for interpreting the significance of quantitative information in environmental contexts.\n",
    "\n",
    "The collected dataset contains a wide variety of these patterns in both structured and unstructured form. Terms like 25°C, 2 mg/L, and sound level (decibels) appear frequently, with and without spacing. These variations will be captured in the vocabulary to improve annotation quality and allow models to generalise across expression formats.\n",
    "\n",
    "To support this, one of the sources used is a structured reference list of units based on the Unified Code for Units of Measure (UCUM): https://ucum.org/docs . UCUM provides a machine-readable and standardised set of units that includes SI, derived, and domain-relevant formats. These are well suited to controlled vocabulary development, where consistency and interoperability are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eaab8f7-10a9-4028-a8c5-cf9478fae2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row # (not a code)</th>\n",
       "      <th>UCUM_CODE</th>\n",
       "      <th>Description of the Unit \\n(using UCUM descriptions where they exist)</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Last Updated (see Revisions tab for details)</th>\n",
       "      <th>Version correction published</th>\n",
       "      <th>Corrected by (initials RG = Rebecca Goodwin)</th>\n",
       "      <th>Row # \\nat time of correction (not a code)</th>\n",
       "      <th>Previous UCUM version (with Errors or Omissions)</th>\n",
       "      <th>Description of Change Made</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.L/min</td>\n",
       "      <td>10 liter per minute</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10.L/(min.m2)</td>\n",
       "      <td>10 liter per minute per square meter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10.uN.s/(cm5.m2)</td>\n",
       "      <td>10 micronewton second per centimeter to the fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10*4/uL</td>\n",
       "      <td>10 thousand per microliter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10*8</td>\n",
       "      <td>100 million</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row # (not a code)         UCUM_CODE  \\\n",
       "0                   1          10.L/min   \n",
       "1                   2     10.L/(min.m2)   \n",
       "2                   3  10.uN.s/(cm5.m2)   \n",
       "3                   4           10*4/uL   \n",
       "4                   5              10*8   \n",
       "\n",
       "  Description of the Unit \\n(using UCUM descriptions where they exist)  \\\n",
       "0                                10 liter per minute                     \n",
       "1               10 liter per minute per square meter                     \n",
       "2  10 micronewton second per centimeter to the fi...                     \n",
       "3                         10 thousand per microliter                     \n",
       "4                                       100 million                      \n",
       "\n",
       "  Comment Last Updated (see Revisions tab for details)  \\\n",
       "0     NaN                                          NaT   \n",
       "1     NaN                                          NaT   \n",
       "2     NaN                                          NaT   \n",
       "3     NaN                                          NaT   \n",
       "4     NaN                                          NaT   \n",
       "\n",
       "  Version correction published Corrected by (initials RG = Rebecca Goodwin)  \\\n",
       "0                          NaN                                          NaN   \n",
       "1                          NaN                                          NaN   \n",
       "2                          NaN                                          NaN   \n",
       "3                          NaN                                          NaN   \n",
       "4                          NaN                                          NaN   \n",
       "\n",
       "   Row # \\nat time of correction (not a code)  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                         NaN   \n",
       "3                                         NaN   \n",
       "4                                         NaN   \n",
       "\n",
       "  Previous UCUM version (with Errors or Omissions) Description of Change Made  \n",
       "0                                              NaN                        NaN  \n",
       "1                                              NaN                        NaN  \n",
       "2                                              NaN                        NaN  \n",
       "3                                              NaN                        NaN  \n",
       "4                                              NaN                        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ucum_path = Path(\"../vocabs/raw/TableOfUcumCodes.xlsx\")\n",
    "\n",
    "df = pd.read_excel(ucum_path, engine=\"openpyxl\", skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "753486da-7e7d-4727-a425-f008a7b0feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"UCUM_CODE\"])\n",
    "\n",
    "units = set()\n",
    "for raw_unit in df[\"UCUM_CODE\"]:\n",
    "    # Remove everything inside parentheses\n",
    "    cleaned = re.sub(r\"\\(.*?\\)\", \"\", raw_unit)\n",
    "\n",
    "    # Remove leading numeric prefixes (e.g. 10., 10*4/)\n",
    "    cleaned = re.sub(r\"^[\\d\\.\\*/]+\", \"\", cleaned)\n",
    "\n",
    "    # Remove whitespace and keep only alphanumeric + slash + dot\n",
    "    cleaned = re.sub(r\"[^\\w/\\.]\", \"\", cleaned)\n",
    "\n",
    "    if cleaned:\n",
    "        units.add(cleaned.lower())\n",
    "\n",
    "units = sorted(units)\n",
    "output_path = EXTRACTED_PATH / \"measurement.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for unit in units:\n",
    "        f.write(unit + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554211bf-bc25-4f84-bff6-954540818af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5adaf027-5478-4093-b0e1-f0cd12ff6f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l/min\n",
      "umol/molhb\n",
      "cm2\n",
      "oif\n",
      "nkat\n",
      "ug/kg/min\n",
      "log_iu/ml\n",
      "h/d\n",
      "ug/l/\n",
      "ahfu\n",
      "gtot_nit\n",
      "uu/g\n",
      "mmol\n",
      "lpf\n",
      "bacteria\n",
      "mmol/dl\n",
      "g/h/m2\n",
      "u/ghb\n",
      "u/1010\n",
      "meq/kg/h\n"
     ]
    }
   ],
   "source": [
    "sampled = random.sample(units, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fffaca-68ca-4a1d-b1f4-75b69e1a3cd7",
   "metadata": {},
   "source": [
    "A total of 733 unique measurement terms were extracted from the UCUM-based unit list. These include scientific and domain-specific expressions such as mg/min, g/kg/h, mmol, u/ghb, and log_iu/ml. The list also features more specialised indicators like immunecomplexu, hemolysis, and time- or dose-related forms such as at_60_min and ug/kg/min.\n",
    "\n",
    "While the vocabulary provides strong technical coverage, it does not include simpler measurement descriptors commonly used in environmental texts. Terms like temperature, concentration, volume, wind speed, and AQI are absent from the list. To address this, a small number of additional terms will be manually added to improve coverage across both scientific and narrative styles of environmental writing.\n",
    "\n",
    "Spot checks using the final list against a subset of the corpus showed high match rates for commonly used units and descriptors, supporting its suitability for annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf818c-638e-483e-8de4-b20a1b457532",
   "metadata": {},
   "source": [
    "## 4. Cleaning and Standardising Vocabulary Terms\n",
    "The vocabulary terms collected in the previous section come from a range of sources and file types. As a result, they include inconsistencies in case, punctuation, spacing, and structure. Some terms are duplicated or contain noise such as trailing symbols or placeholders.\n",
    "\n",
    "To prepare them for matching and annotation, a shared cleaning pipeline is applied. This involves:\n",
    "\n",
    "- Converting to lowercase and trimming whitespace\n",
    "- Removing trailing punctuation and artefacts\n",
    "- Filtering known junk or irrelevant terms\n",
    "- Deduplicating\n",
    "- Adding plural variants using `inflect`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08e603c7-565e-4714-90c0-bb40dd64a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POLLUTANT – 808 terms\n",
      "Sample: ['abamectin', 'acephate', 'acetaldehyde', 'acetamide', 'acetone']\n",
      "\n",
      "ENV_PROCESS – 966 terms\n",
      "Sample: ['abandoned industrial site', 'acceptable daily intake', 'acceptable risk level', 'access road', 'accidental release of organisms']\n",
      "\n",
      "MEASUREMENT – 844 terms\n",
      "Sample: ['/a', '/d', '/g', '/hpf', '/l']\n",
      "\n",
      "HABITAT – 401 terms\n",
      "Sample: ['abandoned industrial site', 'accidental release of organisms', 'agricultural landscape', 'alluvial plain', 'altitude']\n",
      "\n",
      "TAXONOMY – 72970 terms\n",
      "Sample: ['aaom snail', 'aardvark', 'aardvarks', 'aardwolf', 'aardwolves']\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "for filepath in EXTRACTED_PATH.glob(\"*.txt\"):\n",
    "    category = filepath.stem\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        terms = [line.strip() for line in f if line.strip()]\n",
    "    vocab_dict[category] = terms\n",
    "\n",
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"\\n{category.upper()} – {len(terms)} terms\")\n",
    "    print(\"Sample:\", terms[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be678129-9864-44ea-927e-823847ff1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLLUTANT – 808\n",
      "ENV_PROCESS – 966\n",
      "MEASUREMENT – 844\n",
      "HABITAT – 401\n",
      "TAXONOMY – 72970\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"{category.upper()} – {len(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f21c9-7b5c-45be-a43f-986679600f6f",
   "metadata": {},
   "source": [
    "### 4.1 Basic Cleaning\n",
    "Manual inspection of the raw vocabulary lists showed a number of common issues, including inconsistent casing, stray punctuation, and trailing whitespace. These inconsistencies can affect exact matching during annotation.\n",
    "\n",
    "To address this, basic cleaning is performed to standardises the format by lowercasing terms, removing artefacts, normalising spacing, and filtering out empty entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "319da90c-eeaf-447b-8632-a98fc5548558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_terms(raw_terms):\n",
    "    \"\"\"\n",
    "    Basic cleaning for a list of terms:\n",
    "    - Lowercase\n",
    "    - Strip whitespace\n",
    "    - Remove trailing punctuation\n",
    "    - Collapse multiple spaces\n",
    "    - Filter empty strings\n",
    "    - Remove duplicates\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for term in raw_terms:\n",
    "        term = term.lower().strip()\n",
    "        term = re.sub(r'[\\.\\,\\;\\:\\'\\\"\\)\\(]+$', '', term)\n",
    "        term = re.sub(r'\\s+', ' ', term)\n",
    "        \n",
    "        if not term:\n",
    "            continue\n",
    "        cleaned.append(term)\n",
    "    \n",
    "    return list(dict.fromkeys(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe468896-c754-4a4d-8615-c11449c37961",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in vocab_dict:\n",
    "    vocab_dict[category] = clean_terms(vocab_dict[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c775e3d-4fc7-4301-bd30-0dcd41ee5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLLUTANT – 805\n",
      "ENV_PROCESS – 966\n",
      "MEASUREMENT – 844\n",
      "HABITAT – 401\n",
      "TAXONOMY – 72968\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"{category.upper()} – {len(terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3bbcc-407e-43bb-a12d-dfeda0d87a8a",
   "metadata": {},
   "source": [
    "### 4.2 Short Term Detection and Removal\n",
    "Very short terms (fewer than 4 characters) are unlikely to be meaningful in most categories. These often include symbols, abbreviations, malformed fragments, or noise introduced during parsing. However, some short terms may be valid in specific contexts, such as chemical units or element symbols in the pollutant or measurement categories.\n",
    "\n",
    "To identify potential junk entries, all terms under 4 characters are printed by category for manual inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22da11ef-31da-4295-bf7c-04379706ffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POLLUTANT – 13\n",
      "['ash', 'cd', 'cds', 'cfc', 'ddt', 'hcl', 'ink', 'nox', 'pah', 'pb', 'pbs', 'pcb', 'tar']\n",
      "\n",
      "ENV_PROCESS – 2\n",
      "['fog', 'ice']\n",
      "\n",
      "MEASUREMENT – 146\n",
      "['/a', '/d', '/g', '/l', '/ml', '/ul', '/wk', 'a', 'a/m', 'aru', 'atm', 'au', 'bar', 'bq', 'cal', 'cel', 'cfu', 'cg', 'ch', 'cl', 'cm', 'cm2', 'cm3', 'cp', 'cpm', 'cst', 'd', 'd/', 'db', 'deg', 'dg', 'dl', 'dm', 'drp', 'eq', 'erg', 'ev', 'f', 'fg', 'fiu', 'fl', 'fm', 'g', 'g.m', 'g/', 'g/d', 'g/g', 'g/h', 'g/l', 'ghb', 'gy', 'h', 'h/d', 'hb', 'hpf', 'hz', 'inr', 'isr', 'iu', 'iu/', 'j', 'j/l', 'k', 'k/w', 'kat', 'kau', 'kg', 'kg/', 'kl', 'km', 'kpa', 'ks', 'ku', 'l', 'l/', 'l/d', 'l/h', 'l/l', 'l/s', 'lm', 'log', 'lpf', 'm', 'm/s', 'm2', 'm3', 'ma', 'meq', 'mg', 'mg/', 'min', 'ml', 'ml/', 'mm', 'mm2', 'mo', 'mol', 'mpa', 'ms', 'mv', 'n', 'n.s', 'ng', 'ng/', 'nl', 'nm', 'ns', 'ohm', 'oif', 'osm', 'pa', 'pg', 'ph', 'pl', 'pm', 'ppb', 'ppm', 'ps', 'psi', 'pt', 's', 'sv', 't', 'tbu', 'u', 'u/', 'u/d', 'u/g', 'u/h', 'u/l', 'u/s', 'ueq', 'ug', 'ug/', 'uiu', 'ul', 'ul/', 'um', 'us', 'uv', 'v', 'vol', 'wb', 'wk', 'aqi', 'bod']\n",
      "\n",
      "HABITAT – 4\n",
      "['bay', 'bog', 'dam', 'fen']\n",
      "\n",
      "TAXONOMY – 67\n",
      "['ani', 'ant', 'aol', 'ara', 'asp', 'ass', 'atu', 'auk', 'ayu', 'bat', 'bee', 'bib', 'boa', 'cat', 'cod', 'cow', 'dab', 'dog', 'eel', 'elf', 'elk', 'emu', 'ewe', 'foa', 'fox', 'gag', 'gar', 'gem', 'hen', 'hog', 'ide', 'jac', 'jew', 'kea', 'kit', 'kob', 'koi', 'lai', 'les', 'lox', 'mao', 'nil', 'odo', 'oia', 'olm', 'oo', 'oos', 'ou', 'ous', 'owl', 'pea', 'pig', 'ram', 'rat', 'ray', 'rog', 'sbt', 'sim', 'sol', 'tit', 'tui', 'tur', 'uku', 'wel', 'wip', 'xes', 'yak']\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    short_terms = [t for t in terms if len(t) < 4]\n",
    "    if short_terms:\n",
    "        print(f\"\\n{category.upper()} – {len(short_terms)}\")\n",
    "        print(short_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db571afb-e81a-4cda-aa1f-054ec1ed8821",
   "metadata": {},
   "source": [
    "Some of the short terms in the taxonomy vocabulary were found to be invalid fragments, such as \"x\", \"xes\", or \"le\". These were not recognisable as valid species names or taxonomic entities.\n",
    "\n",
    "The following entries are removed from the taxonomy list manually. Other categories are left unchanged for now, as many of their short terms are meaningful (e.g. \"oz\", \"mg\", \"co\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "847c1f12-758c-42f0-bba5-5aab97a435be",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_terms = {\"x\", \"xes\", \"uku\", \"atu\", \"le\", \"de\", \"’s\", \"la\", \"el\", \"fo\", \"b\", \"z\", \"ye\", \"ha\"}\n",
    "\n",
    "vocab_dict[\"taxonomy\"] = [t for t in vocab_dict[\"taxonomy\"] if t not in junk_terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3be08c-6d1e-423d-a826-4172ff9c4e7f",
   "metadata": {},
   "source": [
    "### 4.3 Checking for Duplicates Across Categories\n",
    "\n",
    "Some terms may appear in more than one category. This can cause problems during annotation, where the same term could be labelled as multiple entity types.\n",
    "\n",
    "To check for this, each pair of vocabulary categories is compared and any shared terms are printed. This helps identify overlaps that might need to be removed or reassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6781606a-85e3-42a5-8976-79514d29865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overlap between 'pollutant' and 'env_process' (10 terms):\n",
      "['acid rain', 'algal bloom', 'asbestos', 'dust', 'industrial waste', 'nitrate', 'oil spill', 'radioactive waste', 'sewage', 'soot']\n",
      "\n",
      "Overlap between 'env_process' and 'measurement' (3 terms):\n",
      "['emission factor', 'soil salinity', 'turbidity']\n",
      "\n",
      "Overlap between 'env_process' and 'habitat' (18 terms):\n",
      "['abandoned industrial site', 'accidental release of organisms', 'amusement park', 'artificial lake', 'boreal forest dieback', 'botanical garden', 'camping site', 'ecotoxicological evaluation', 'estuary pollution', 'excavation site', 'field damage', 'glacier', 'holiday camp', 'industrial wasteland', 'polar region', 'polluted site', 'rain forest', 'urban green']\n",
      "\n",
      "Overlap between 'habitat' and 'taxonomy' (2 terms):\n",
      "['swamp', 'swamps']\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for (cat1, terms1), (cat2, terms2) in combinations(vocab_dict.items(), 2):\n",
    "    overlap = set(terms1) & set(terms2)\n",
    "    if overlap:\n",
    "        print(f\"\\nOverlap between '{cat1}' and '{cat2}' ({len(overlap)} terms):\")\n",
    "        print(sorted(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add39103-50dd-48bc-a488-3e81604bfeb3",
   "metadata": {},
   "source": [
    "Some terms appear in more than one category. To avoid ambiguity, each term is assigned to the category where it makes the most sense.\n",
    "\n",
    "- Terms like \"asbestos\", \"nitrate\", and \"soot\" are pollutants, not processes.\n",
    "- Physical places like \"glacier\", \"camping site\", or \"urban green\" belong in habitat, not env_process.\n",
    "- Measurement-related terms such as \"turbidity\" and \"emission factor\" are kept in measurement only.\n",
    "- \"Swamp\" and \"swamps\" are removed from taxonomy, since they describe habitats.\n",
    "\n",
    "These terms are removed from the categories where they don’t belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2d8f2cc-a707-46e4-a9c6-8d52042e851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_from_env_process = {\n",
    "    \"acid rain\", \"algal bloom\", \"asbestos\", \"dust\", \"industrial waste\",\n",
    "    \"nitrate\", \"oil spill\", \"radioactive waste\", \"sewage\", \"soot\",\n",
    "    \"emission factor\", \"soil salinity\", \"turbidity\",\n",
    "    \"abandoned industrial site\", \"accidental release of organisms\", \"amusement park\",\n",
    "    \"artificial lake\", \"boreal forest dieback\", \"botanical garden\", \"camping site\",\n",
    "    \"ecotoxicological evaluation\", \"estuary pollution\", \"excavation site\", \"field damage\",\n",
    "    \"glacier\", \"holiday camp\", \"industrial wasteland\", \"polar region\", \"polluted site\",\n",
    "    \"rain forest\", \"urban green\"\n",
    "}\n",
    "\n",
    "remove_from_taxonomy = {\n",
    "    \"swamp\", \"swamps\"\n",
    "}\n",
    "\n",
    "vocab_dict[\"env_process\"] = [\n",
    "    t for t in vocab_dict[\"env_process\"] if t not in remove_from_env_process\n",
    "]\n",
    "\n",
    "vocab_dict[\"taxonomy\"] = [\n",
    "    t for t in vocab_dict[\"taxonomy\"] if t not in remove_from_taxonomy\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56f932e5-3933-4d13-841c-44703cc46cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (cat1, terms1), (cat2, terms2) in combinations(vocab_dict.items(), 2):\n",
    "    overlap = set(terms1) & set(terms2)\n",
    "    if overlap:\n",
    "        print(f\"\\nOverlap between '{cat1}' and '{cat2}' ({len(overlap)} terms):\")\n",
    "        print(sorted(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4677f59-af5a-4886-bdda-c76ff765a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pollutant: 805 terms\n",
      "env_process: 935 terms\n",
      "measurement: 844 terms\n",
      "habitat: 401 terms\n",
      "taxonomy: 72963 terms\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"{category}: {len(terms)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4a768-23c8-4abe-9be7-ebcddeb0db83",
   "metadata": {},
   "source": [
    "### 4.4 Pluralisation\n",
    "Most vocabulary terms are stored in their singular form. However, the text corpus may contain both singular and plural mentions of the same entity, especially for common nouns like \"lake\" / \"lakes\" or \"pesticide\" / \"pesticides\".\n",
    "\n",
    "If only the singular form is matched during annotation, many valid entity mentions could be missed. To avoid this, plural variants are added to each category using the `inflect` library. This improves recall during weak labelling by covering more surface forms without relying on exact duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75096a4f-4ebc-49ef-b8b4-3c9f95e8153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plural variants added to all categories\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "\n",
    "for category, terms in vocab_dict.items():\n",
    "    expanded = set(terms)\n",
    "    for term in terms:\n",
    "        plural = p.plural(term)\n",
    "        if plural != term:\n",
    "            expanded.add(plural)\n",
    "    vocab_dict[category] = sorted(expanded)\n",
    "\n",
    "print(\"Plural variants added to all categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58e344ca-dec6-4646-90d2-22138229362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pollutant: 1401 terms\n",
      "env_process: 1868 terms\n",
      "measurement: 1639 terms\n",
      "habitat: 638 terms\n",
      "taxonomy: 129474 terms\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"{category}: {len(terms)} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc3abd43-f69a-446d-ba7c-0e768170fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POLLUTANT\n",
      "  abamectin → abamectins\n",
      "  abamectins → abamectin\n",
      "  acephate → acephates\n",
      "  acephates → acephate\n",
      "  acetaldehyde → acetaldehydes\n",
      "\n",
      "ENV_PROCESS\n",
      "  acceptable daily intake → acceptable daily intakes\n",
      "  acceptable risk level → acceptable risk levels\n",
      "  access road → access roads\n",
      "  acid deposition → acid depositions\n",
      "  activated sludge → activated sludges\n",
      "\n",
      "MEASUREMENT\n",
      "  /a → /as\n",
      "  /as → /a\n",
      "  /d → /ds\n",
      "  /ds → /d\n",
      "  /g → /gs\n",
      "\n",
      "HABITAT\n",
      "  abandoned industrial site → abandoned industrial sites\n",
      "  accidental release of organisms → accidental releases of organisms\n",
      "  agricultural landscape → agricultural landscapes\n",
      "  alluvial plain → alluvial plains\n",
      "  altitude → altitudes\n",
      "\n",
      "TAXONOMY\n",
      "  aaom snail → aaom snails\n",
      "  aardvark → aardvarks\n",
      "  aardvarks → aardvark\n",
      "  aardwolf → aardwolves\n",
      "  aardwolve → aardwolves\n"
     ]
    }
   ],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    print(f\"\\n{category.upper()}\")\n",
    "    count = 0\n",
    "    for term in terms:\n",
    "        plural = p.plural(term)\n",
    "        if plural != term and plural in terms:\n",
    "            print(f\"  {term} → {plural}\")\n",
    "            count += 1\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf55cc6-9810-4d80-9e24-4304a2f5434b",
   "metadata": {},
   "source": [
    "The increase in term counts confirms that plural variants were successfully added. For example, `pollutant` grew from 805 to 1401 terms, and `taxonomy` nearly doubled in size.\n",
    "\n",
    "This step ensures that plural mentions in the corpus are not missed during annotation. For example, \"abamectin\" → \"abamectins\" was correctly added in the `pollutant` category. These plural forms help capture more entity mentions without requiring duplicates in the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b55045e5-9775-4214-9f2f-ee0f74bd4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, terms in vocab_dict.items():\n",
    "    out_path = FINAL_PATH / f\"{category}.txt\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for term in terms:\n",
    "            f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06f8d5-ff91-480b-8b31-f72927773203",
   "metadata": {},
   "source": [
    "## 5. Final Summary\n",
    "\n",
    "### 5.1 Summary of Vocabulary Collection\n",
    "Vocabulary terms were collected from a range of environmental sources, including glossaries, datasets, and controlled vocabularies. Terms were grouped into five entity categories: `taxonomy`, `habitat`, `env_process`, `pollutant`, and `measurement`.\n",
    "\n",
    "Each list was cleaned and standardised to ensure consistency. This included removing duplicates, lowercasing, trimming whitespace, and handling invalid or ambiguous entries. Plural variants were also added to improve matching during annotation. Overlapping terms between categories were resolved manually to keep each entity type distinct.\n",
    "\n",
    "### 5.2 Next Steps\n",
    "The cleaned vocabularies will be saved and used to weakly label the environmental text dataset in the next notebook. Terms will be matched against pre-segmented sentences using string-matching techniques such as Aho-Corasick.\n",
    "\n",
    "After annotation, the dataset will be reviewed manually in Doccano. If relevant entity mentions are found that were not covered by the initial vocabularies, those terms will be added by hand. This final review helps fill in any gaps and ensures the vocabularies reflect real-world usage in the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
