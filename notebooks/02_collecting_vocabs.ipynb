{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de58da9-14df-493c-a46f-fa8233f19526",
   "metadata": {},
   "source": [
    "# Vocabulary Curation for Environmental Science NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165b069-9c47-4081-8f44-bb64c5616813",
   "metadata": {},
   "source": [
    "## 1. Scraping Domain Terms from GEMET\n",
    "We used the GEMET vocabulary via EIONET to retrieve domain-specific terms for categories like pollution, habitats, and climate. The scraper iterates through each themed page, collecting distinct terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4f2ee-4ce4-4306-a06d-9402c2af66b0",
   "metadata": {},
   "source": [
    "The ‚Äúenvironmental_policy‚Äù theme was excluded as it did not align with the NER categories chosen for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4549f9-dab7-4302-b42e-ad247072e4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping theme: pollution\n",
      "  -> Page 1\n",
      "  -> Page 2\n",
      "  -> Page 3\n",
      "  -> Page 4\n",
      "  -> Page 5\n",
      "  -> Page 6\n",
      "  -> Page 7\n",
      "  -> Page 8\n",
      "  -> Page 9\n",
      "  -> Page 10\n",
      "  -> Page 11\n",
      "  -> Page 12\n",
      "  -> Page 13\n",
      "  -> Page 14\n",
      "  -> Page 15\n",
      "  -> Page 16\n",
      "  [!] Failed to load page, stopping.\n",
      "  ‚úÖ Saved 578 terms for pollution.\n",
      "Scraping theme: biology\n",
      "  -> Page 1\n",
      "  -> Page 2\n",
      "  -> Page 3\n",
      "  -> Page 4\n",
      "  -> Page 5\n",
      "  -> Page 6\n",
      "  -> Page 7\n",
      "  -> Page 8\n",
      "  -> Page 9\n",
      "  -> Page 10\n",
      "  -> Page 11\n",
      "  -> Page 12\n",
      "  -> Page 13\n",
      "  -> Page 14\n",
      "  -> Page 15\n",
      "  -> Page 16\n",
      "  -> Page 17\n",
      "  -> Page 18\n",
      "  -> Page 19\n",
      "  [!] Failed to load page, stopping.\n",
      "  ‚úÖ Saved 691 terms for biology.\n",
      "Scraping theme: natural_areas\n",
      "  -> Page 1\n",
      "  -> Page 2\n",
      "  -> Page 3\n",
      "  -> Page 4\n",
      "  -> Page 5\n",
      "  -> Page 6\n",
      "  -> Page 7\n",
      "  -> Page 8\n",
      "  -> Page 9\n",
      "  -> Page 10\n",
      "  -> Page 11\n",
      "  -> Page 12\n",
      "  -> Page 13\n",
      "  [!] Failed to load page, stopping.\n",
      "  ‚úÖ Saved 467 terms for natural_areas.\n",
      "Scraping theme: climate\n",
      "  -> Page 1\n",
      "  -> Page 2\n",
      "  -> Page 3\n",
      "  -> Page 4\n",
      "  -> Page 5\n",
      "  [!] Failed to load page, stopping.\n",
      "  ‚úÖ Saved 128 terms for climate.\n",
      "Scraping theme: environmental_policy\n",
      "  -> Page 1\n",
      "  -> Page 2\n",
      "  -> Page 3\n",
      "  -> Page 4\n",
      "  -> Page 5\n",
      "  -> Page 6\n",
      "  -> Page 7\n",
      "  -> Page 8\n",
      "  -> Page 9\n",
      "  -> Page 10\n",
      "  -> Page 11\n",
      "  -> Page 12\n",
      "  -> Page 13\n",
      "  -> Page 14\n",
      "  [!] Failed to load page, stopping.\n",
      "  ‚úÖ Saved 498 terms for environmental_policy.\n",
      "\n",
      "üéâ All themes scraped and saved in 'gemet_terms/' folder.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Themes mapped to their GEMET IDs\n",
    "themes = {\n",
    "    \"pollution\": 26,\n",
    "    \"biology\": 4,\n",
    "    \"natural_areas\": 23,\n",
    "    \"climate\": 7,\n",
    "    \"environmental_policy\": 11\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://www.eionet.europa.eu/gemet/en/theme/{}/concepts/?page={}&letter=0\"\n",
    "OUTPUT_DIR = \"gemet_terms\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def scrape_theme(theme_id, theme_name):\n",
    "    print(f\"Scraping theme: {theme_name}\")\n",
    "    page = 1\n",
    "    terms = set()\n",
    "\n",
    "    while True:\n",
    "        url = BASE_URL.format(theme_id, page)\n",
    "        print(f\"  -> Page {page}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(\"  [!] Failed to load page, stopping.\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        term_elements = soup.select(\"ul.listing.columns.split-20 li a\")\n",
    "        if not term_elements:\n",
    "            break\n",
    "\n",
    "        new_terms = {el.get_text(strip=True) for el in term_elements}\n",
    "        if new_terms.issubset(terms):  # if no new terms, stop\n",
    "            break\n",
    "\n",
    "        terms.update(new_terms)\n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/{theme_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for term in sorted(terms):\n",
    "            f.write(term + \"\\n\")\n",
    "    print(f\"  ‚úÖ Saved {len(terms)} terms for {theme_name}.\")\n",
    "\n",
    "# Run scraper for all selected themes\n",
    "for name, theme_id in themes.items():\n",
    "    scrape_theme(theme_id, name)\n",
    "\n",
    "print(\"\\nüéâ All themes scraped and saved in 'gemet_terms/' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d4dc3-9af8-4871-a81c-2d4d9f8f2175",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Filtering Terms\n",
    "Lemmatise all GEMET terms.\n",
    "\n",
    "Remove:\n",
    "\n",
    "* Words that are too generic (e.g. \"animal\", \"life\")\n",
    "\n",
    "* Words that are < 3 characters\n",
    "\n",
    "* Anything that doesn't fit your NER context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f88a2-cbf7-49bc-b04c-4ace0c6d23c2",
   "metadata": {},
   "source": [
    "## 3. Lemmatization Script (with spaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf871bb8-1dbe-460a-8e93-071e7af08935",
   "metadata": {},
   "source": [
    "one of the below is correct and should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a63d9-fe25-4544-8ea0-7b1ada414b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "VOCAB_DIR = Path(\"gemet_terms\")\n",
    "CLEAN_DIR = Path(\"../vocabularies\")\n",
    "CLEAN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for vocab_file in VOCAB_DIR.glob(\"*.txt\"):\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "    lemmatised = set()\n",
    "    for term in terms:\n",
    "        doc = nlp(term)\n",
    "        lemma = \" \".join([token.lemma_ for token in doc])\n",
    "        lemmatised.add(lemma.lower())\n",
    "\n",
    "    # Filter out generic or short terms (e.g., less than 3 chars)\n",
    "    filtered = sorted({term for term in lemmatised if len(term) > 2 and not term.isnumeric()})\n",
    "    \n",
    "    out_path = CLEAN_DIR / vocab_file.name\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for term in filtered:\n",
    "            f.write(term + \"\\n\")\n",
    "    \n",
    "    print(f\"{vocab_file.name}: {len(filtered)} terms saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fccfc0-e3a0-4fef-834a-f52f92f5bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398181f6-d185-4809-b4db-3d13a19aabb9",
   "metadata": {},
   "source": [
    "This step uses SpaCy to lemmatise all terms to their base forms (e.g., \"habitats\" ‚Üí \"habitat\"). Generic terms and words shorter than 3 characters were removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb6429-cc2d-4532-b6e1-add0dc0f2c0d",
   "metadata": {},
   "source": [
    "We now have cleaned and curated vocabulary lists for each NER category:\n",
    "\n",
    "- `taxonomy.txt` (from BHL)\n",
    "- `habitat.txt`\n",
    "- `pollutant.txt`\n",
    "- `env_process.txt`\n",
    "- `measurement.txt`\n",
    "\n",
    "These vocabulary files are stored in the `vocabularies/` directory and are ready to be used for Aho-Corasick-based annotation. If additional terms are discovered later, they can be appended and recompiled on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda8bb8-1800-4099-b7d1-3295e9f56de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae5f44-1227-463f-82b3-357f5588c297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ae463-7da8-4ea8-b5ef-b32d3f786b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30e339-b5b9-49f0-a12e-d08840b09319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19feffac-f706-4c8c-bdd8-6b08d8363f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf2ab9-8400-476d-9167-db6176ea89ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538048e-83c5-48b3-98ed-7b06d13afd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bae6fa1f-708e-4e3e-b95e-42fa5bd39114",
   "metadata": {},
   "source": [
    "### I should move this to annotation and maybe say FlashText was too slow compared to Ahocorasick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eeda82-d700-48de-817a-6a4ee7f2c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FlashText Annotation ---\n",
    "def annotate_text_with_vocab(text, vocab_terms, label):\n",
    "    processor = KeywordProcessor()\n",
    "    for term in vocab_terms:\n",
    "        processor.add_keyword(term)\n",
    "\n",
    "    results = processor.extract_keywords(text, span_info=True)\n",
    "\n",
    "    annotations = []\n",
    "    for keyword, start_idx, end_idx in results:\n",
    "        annotations.append([start_idx, end_idx, label])\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6a0bf2-3f92-4134-89e2-5548343588f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚è≥ Processed 563200/564547 texts...\n",
      "  ‚è≥ Processed 563300/564547 texts...\n",
      "  ‚è≥ Processed 563400/564547 texts...\n",
      "  ‚è≥ Processed 563500/564547 texts...\n",
      "  ‚è≥ Processed 563600/564547 texts...\n",
      "  ‚è≥ Processed 563700/564547 texts...\n",
      "  ‚è≥ Processed 563800/564547 texts...\n",
      "  ‚è≥ Processed 563900/564547 texts...\n",
      "  ‚è≥ Processed 564000/564547 texts...\n",
      "  ‚è≥ Processed 564100/564547 texts...\n",
      "  ‚è≥ Processed 564200/564547 texts...\n",
      "  ‚è≥ Processed 564300/564547 texts...\n",
      "  ‚è≥ Processed 564400/564547 texts...\n",
      "  ‚è≥ Processed 564500/564547 texts...\n",
      "‚úÖ Finished POLLUTION: 30593 annotated samples saved to ..\\training_data\\training_data_pollution.jsonl\n",
      "\n",
      "üîç Annotating category: POLLUTANT from pollutants.txt\n",
      "  ‚è≥ Processed 100/564547 texts...\n",
      "  ‚è≥ Processed 200/564547 texts...\n",
      "  ‚è≥ Processed 300/564547 texts...\n",
      "  ‚è≥ Processed 400/564547 texts...\n",
      "  ‚è≥ Processed 500/564547 texts...\n",
      "  ‚è≥ Processed 600/564547 texts...\n",
      "  ‚è≥ Processed 700/564547 texts...\n",
      "  ‚è≥ Processed 800/564547 texts...\n",
      "  ‚è≥ Processed 900/564547 texts...\n",
      "  ‚è≥ Processed 1000/564547 texts...\n",
      "  ‚è≥ Processed 1100/564547 texts...\n",
      "  ‚è≥ Processed 1200/564547 texts...\n",
      "  ‚è≥ Processed 1300/564547 texts...\n",
      "  ‚è≥ Processed 1400/564547 texts...\n",
      "  ‚è≥ Processed 1500/564547 texts...\n",
      "  ‚è≥ Processed 1600/564547 texts...\n",
      "  ‚è≥ Processed 1700/564547 texts...\n",
      "  ‚è≥ Processed 1800/564547 texts...\n",
      "  ‚è≥ Processed 1900/564547 texts...\n",
      "  ‚è≥ Processed 2000/564547 texts...\n",
      "  ‚è≥ Processed 2100/564547 texts...\n",
      "  ‚è≥ Processed 2200/564547 texts...\n",
      "  ‚è≥ Processed 2300/564547 texts...\n",
      "  ‚è≥ Processed 2400/564547 texts...\n",
      "  ‚è≥ Processed 2500/564547 texts...\n",
      "  ‚è≥ Processed 2600/564547 texts...\n",
      "  ‚è≥ Processed 2700/564547 texts...\n",
      "  ‚è≥ Processed 2800/564547 texts...\n",
      "  ‚è≥ Processed 2900/564547 texts...\n",
      "  ‚è≥ Processed 3000/564547 texts...\n",
      "  ‚è≥ Processed 3100/564547 texts...\n",
      "  ‚è≥ Processed 3200/564547 texts...\n",
      "  ‚è≥ Processed 3300/564547 texts...\n",
      "  ‚è≥ Processed 3400/564547 texts...\n",
      "  ‚è≥ Processed 3500/564547 texts...\n",
      "  ‚è≥ Processed 3600/564547 texts...\n",
      "  ‚è≥ Processed 3700/564547 texts...\n",
      "  ‚è≥ Processed 3800/564547 texts...\n",
      "  ‚è≥ Processed 3900/564547 texts...\n",
      "  ‚è≥ Processed 4000/564547 texts...\n",
      "  ‚è≥ Processed 4100/564547 texts...\n",
      "  ‚è≥ Processed 4200/564547 texts...\n",
      "  ‚è≥ Processed 4300/564547 texts...\n",
      "  ‚è≥ Processed 4400/564547 texts...\n",
      "  ‚è≥ Processed 4500/564547 texts...\n",
      "  ‚è≥ Processed 4600/564547 texts...\n",
      "  ‚è≥ Processed 4700/564547 texts...\n",
      "  ‚è≥ Processed 4800/564547 texts...\n",
      "  ‚è≥ Processed 4900/564547 texts...\n",
      "  ‚è≥ Processed 5000/564547 texts...\n",
      "  ‚è≥ Processed 5100/564547 texts...\n",
      "  ‚è≥ Processed 5200/564547 texts...\n",
      "  ‚è≥ Processed 5300/564547 texts...\n",
      "  ‚è≥ Processed 5400/564547 texts...\n",
      "  ‚è≥ Processed 5500/564547 texts...\n",
      "  ‚è≥ Processed 5600/564547 texts...\n",
      "  ‚è≥ Processed 5700/564547 texts...\n",
      "  ‚è≥ Processed 5800/564547 texts...\n",
      "  ‚è≥ Processed 5900/564547 texts...\n",
      "  ‚è≥ Processed 6000/564547 texts...\n",
      "  ‚è≥ Processed 6100/564547 texts...\n",
      "  ‚è≥ Processed 6200/564547 texts...\n",
      "  ‚è≥ Processed 6300/564547 texts...\n",
      "  ‚è≥ Processed 6400/564547 texts...\n",
      "  ‚è≥ Processed 6500/564547 texts...\n",
      "  ‚è≥ Processed 6600/564547 texts...\n",
      "  ‚è≥ Processed 6700/564547 texts...\n",
      "  ‚è≥ Processed 6800/564547 texts...\n",
      "  ‚è≥ Processed 6900/564547 texts...\n",
      "  ‚è≥ Processed 7000/564547 texts...\n",
      "  ‚è≥ Processed 7100/564547 texts...\n",
      "  ‚è≥ Processed 7200/564547 texts...\n",
      "  ‚è≥ Processed 7300/564547 texts...\n",
      "  ‚è≥ Processed 7400/564547 texts...\n",
      "  ‚è≥ Processed 7500/564547 texts...\n",
      "  ‚è≥ Processed 7600/564547 texts...\n",
      "  ‚è≥ Processed 7700/564547 texts...\n",
      "  ‚è≥ Processed 7800/564547 texts...\n",
      "  ‚è≥ Processed 7900/564547 texts...\n",
      "  ‚è≥ Processed 8000/564547 texts...\n",
      "  ‚è≥ Processed 8100/564547 texts...\n",
      "  ‚è≥ Processed 8200/564547 texts...\n",
      "  ‚è≥ Processed 8300/564547 texts...\n",
      "  ‚è≥ Processed 8400/564547 texts...\n",
      "  ‚è≥ Processed 8500/564547 texts...\n",
      "  ‚è≥ Processed 8600/564547 texts...\n",
      "  ‚è≥ Processed 8700/564547 texts...\n",
      "  ‚è≥ Processed 8800/564547 texts...\n",
      "  ‚è≥ Processed 8900/564547 texts...\n",
      "  ‚è≥ Processed 9000/564547 texts...\n",
      "  ‚è≥ Processed 9100/564547 texts...\n",
      "  ‚è≥ Processed 9200/564547 texts...\n",
      "  ‚è≥ Processed 9300/564547 texts...\n",
      "  ‚è≥ Processed 9400/564547 texts...\n",
      "  ‚è≥ Processed 9500/564547 texts...\n",
      "  ‚è≥ Processed 9600/564547 texts...\n",
      "  ‚è≥ Processed 9700/564547 texts...\n",
      "  ‚è≥ Processed 9800/564547 texts...\n",
      "  ‚è≥ Processed 9900/564547 texts...\n",
      "  ‚è≥ Processed 10000/564547 texts...\n",
      "  ‚è≥ Processed 10100/564547 texts...\n",
      "  ‚è≥ Processed 10200/564547 texts...\n",
      "  ‚è≥ Processed 10300/564547 texts...\n",
      "  ‚è≥ Processed 10400/564547 texts...\n",
      "  ‚è≥ Processed 10500/564547 texts...\n",
      "  ‚è≥ Processed 10600/564547 texts...\n",
      "  ‚è≥ Processed 10700/564547 texts...\n",
      "  ‚è≥ Processed 10800/564547 texts...\n",
      "  ‚è≥ Processed 10900/564547 texts...\n",
      "  ‚è≥ Processed 11000/564547 texts...\n",
      "  ‚è≥ Processed 11100/564547 texts...\n",
      "  ‚è≥ Processed 11200/564547 texts...\n",
      "  ‚è≥ Processed 11300/564547 texts...\n",
      "  ‚è≥ Processed 11400/564547 texts...\n",
      "  ‚è≥ Processed 11500/564547 texts...\n",
      "  ‚è≥ Processed 11600/564547 texts...\n",
      "  ‚è≥ Processed 11700/564547 texts...\n",
      "  ‚è≥ Processed 11800/564547 texts...\n",
      "  ‚è≥ Processed 11900/564547 texts...\n",
      "  ‚è≥ Processed 12000/564547 texts...\n",
      "  ‚è≥ Processed 12100/564547 texts...\n",
      "  ‚è≥ Processed 12200/564547 texts...\n",
      "  ‚è≥ Processed 12300/564547 texts...\n",
      "  ‚è≥ Processed 12400/564547 texts...\n",
      "  ‚è≥ Processed 12500/564547 texts...\n",
      "  ‚è≥ Processed 12600/564547 texts...\n",
      "  ‚è≥ Processed 12700/564547 texts...\n",
      "  ‚è≥ Processed 12800/564547 texts...\n",
      "  ‚è≥ Processed 12900/564547 texts...\n",
      "  ‚è≥ Processed 13000/564547 texts...\n",
      "  ‚è≥ Processed 13100/564547 texts...\n",
      "  ‚è≥ Processed 13200/564547 texts...\n",
      "  ‚è≥ Processed 13300/564547 texts...\n",
      "  ‚è≥ Processed 13400/564547 texts...\n",
      "  ‚è≥ Processed 13500/564547 texts...\n",
      "  ‚è≥ Processed 13600/564547 texts...\n",
      "  ‚è≥ Processed 13700/564547 texts...\n",
      "  ‚è≥ Processed 13800/564547 texts...\n",
      "  ‚è≥ Processed 13900/564547 texts...\n",
      "  ‚è≥ Processed 14000/564547 texts...\n",
      "  ‚è≥ Processed 14100/564547 texts...\n",
      "  ‚è≥ Processed 14200/564547 texts...\n",
      "  ‚è≥ Processed 14300/564547 texts...\n",
      "  ‚è≥ Processed 14400/564547 texts...\n",
      "  ‚è≥ Processed 14500/564547 texts...\n",
      "  ‚è≥ Processed 14600/564547 texts...\n",
      "  ‚è≥ Processed 14700/564547 texts...\n",
      "  ‚è≥ Processed 14800/564547 texts...\n",
      "  ‚è≥ Processed 14900/564547 texts...\n",
      "  ‚è≥ Processed 15000/564547 texts...\n",
      "  ‚è≥ Processed 15100/564547 texts...\n",
      "  ‚è≥ Processed 15200/564547 texts...\n",
      "  ‚è≥ Processed 15300/564547 texts...\n",
      "  ‚è≥ Processed 15400/564547 texts...\n",
      "  ‚è≥ Processed 15500/564547 texts...\n",
      "  ‚è≥ Processed 15600/564547 texts...\n",
      "  ‚è≥ Processed 15700/564547 texts...\n",
      "  ‚è≥ Processed 15800/564547 texts...\n",
      "  ‚è≥ Processed 15900/564547 texts...\n",
      "  ‚è≥ Processed 16000/564547 texts...\n",
      "  ‚è≥ Processed 16100/564547 texts...\n",
      "  ‚è≥ Processed 16200/564547 texts...\n",
      "  ‚è≥ Processed 16300/564547 texts...\n",
      "  ‚è≥ Processed 16400/564547 texts...\n",
      "  ‚è≥ Processed 16500/564547 texts...\n",
      "  ‚è≥ Processed 16600/564547 texts...\n",
      "  ‚è≥ Processed 16700/564547 texts...\n",
      "  ‚è≥ Processed 16800/564547 texts...\n",
      "  ‚è≥ Processed 16900/564547 texts...\n",
      "  ‚è≥ Processed 17000/564547 texts...\n",
      "  ‚è≥ Processed 17100/564547 texts...\n",
      "  ‚è≥ Processed 17200/564547 texts...\n",
      "  ‚è≥ Processed 17300/564547 texts...\n",
      "  ‚è≥ Processed 17400/564547 texts...\n",
      "  ‚è≥ Processed 17500/564547 texts...\n",
      "  ‚è≥ Processed 17600/564547 texts...\n",
      "  ‚è≥ Processed 17700/564547 texts...\n",
      "  ‚è≥ Processed 17800/564547 texts...\n",
      "  ‚è≥ Processed 17900/564547 texts...\n",
      "  ‚è≥ Processed 18000/564547 texts...\n",
      "  ‚è≥ Processed 18100/564547 texts...\n",
      "  ‚è≥ Processed 18200/564547 texts...\n",
      "  ‚è≥ Processed 18300/564547 texts...\n",
      "  ‚è≥ Processed 18400/564547 texts...\n",
      "  ‚è≥ Processed 18500/564547 texts...\n",
      "  ‚è≥ Processed 18600/564547 texts...\n",
      "  ‚è≥ Processed 18700/564547 texts...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m annotated_samples = []\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preprocessed_texts):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     annotations = \u001b[43mannotate_text_with_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_terms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m annotations:\n\u001b[32m     24\u001b[39m         annotated_samples.append({\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: annotations})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mannotate_text_with_vocab\u001b[39m\u001b[34m(text, vocab_terms, label)\u001b[39m\n\u001b[32m      3\u001b[39m processor = KeywordProcessor()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m vocab_terms:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_keyword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m results = processor.extract_keywords(text, span_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m annotations = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\env-ner-project\\.venv\\Lib\\site-packages\\flashtext\\keyword.py:225\u001b[39m, in \u001b[36mKeywordProcessor.add_keyword\u001b[39m\u001b[34m(self, keyword, clean_name)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"add a character that will be considered as part of word.\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    221\u001b[39m \n\u001b[32m    222\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.non_word_boundaries.add(character)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_keyword\u001b[39m(\u001b[38;5;28mself\u001b[39m, keyword, clean_name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"To add one or more keywords to the dictionary\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    pass the keyword and the clean name it maps to.\u001b[39;00m\n\u001b[32m    228\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m \u001b[33;03m        >>> # This case 'Big Apple' will return 'Big Apple'\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__setitem__\u001b[39m(keyword, clean_name)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Annotate and Save JSONL ---\n",
    "theme_files = [\"pollution.txt\", \"pollutants.txt\", \"climate.txt\", \"natural_areas.txt\"]\n",
    "theme_labels = {\n",
    "    \"pollution\": \"POLLUTION\",\n",
    "    \"pollutants\": \"POLLUTANT\",\n",
    "    \"climate\": \"CLIMATE\",\n",
    "    \"natural_areas\": \"HABITAT\"\n",
    "}\n",
    "\n",
    "combined_annotations = []\n",
    "\n",
    "for fname in theme_files:\n",
    "    theme_name = fname.replace(\".txt\", \"\")\n",
    "    label = theme_labels[theme_name]\n",
    "    with open(VOCAB_DIR / fname, encoding=\"utf-8\") as f:\n",
    "        vocab_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    for text in preprocessed_texts:\n",
    "        annotations = annotate_text_with_vocab(text.lower(), vocab_terms, label)\n",
    "        if annotations:\n",
    "            combined_annotations.append({\"text\": text, \"label\": annotations})\n",
    "\n",
    "# Save the annotated data\n",
    "annotated_path = OUTPUT_DIR / \"training_data.jsonl\"\n",
    "with open(annotated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in combined_annotations:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Saved {len(combined_annotations)} annotated samples to {annotated_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a39edd-fb8c-447e-9236-c66b2e2d25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean Overlapping Annotations ---\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "def has_overlapping_entities(entities):\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    for i in range(len(sorted_entities) - 1):\n",
    "        current_start, current_end, _ = sorted_entities[i]\n",
    "        next_start, _, _ = sorted_entities[i + 1]\n",
    "        if current_end > next_start:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def resolve_overlaps(entities):\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "    resolved = []\n",
    "    occupied = set()\n",
    "    for start, end, label in entities:\n",
    "        if not any(pos in occupied for pos in range(start, end)):\n",
    "            resolved.append([start, end, label])\n",
    "            occupied.update(range(start, end))\n",
    "    return sorted(resolved, key=lambda x: x[0])\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.max_length = 5_000_000\n",
    "\n",
    "# Load previously saved annotations\n",
    "with open(annotated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "valid_data = []\n",
    "invalid_data = []\n",
    "\n",
    "for i, example in enumerate(raw_data):\n",
    "    text = example[\"text\"]\n",
    "    annotations = example[\"label\"]\n",
    "\n",
    "    if has_overlapping_entities(annotations):\n",
    "        annotations = resolve_overlaps(annotations)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        Example.from_dict(doc, {\"entities\": annotations})\n",
    "        valid_data.append({\"text\": text, \"label\": annotations})\n",
    "    except Exception as e:\n",
    "        invalid_data.append({\n",
    "            \"index\": i,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text,\n",
    "            \"label\": annotations\n",
    "        })\n",
    "\n",
    "# Save cleaned data\n",
    "cleaned_path = OUTPUT_DIR / \"cleaned_training_data.jsonl\"\n",
    "with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in valid_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned: {len(valid_data)} valid / {len(invalid_data)} invalid\")\n",
    "print(f\"üìÅ Saved to: {cleaned_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c7f3a-48c1-4de0-873d-7797f680f73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
