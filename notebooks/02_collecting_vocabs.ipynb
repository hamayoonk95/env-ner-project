{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe5d15a-ad7c-41af-98e7-d39b433fe9d5",
   "metadata": {},
   "source": [
    "# Vocabulary Curation for Environmental Science NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac8cb71-f73d-494e-afa4-06d2b01395f9",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Background and Purpose\n",
    "\n",
    "This notebook focuses on building a vocabulary of environmental terms to support downstream Named Entity Recognition (NER). It is the second step in a pipeline designed to develop a domain-specific NER dataset, following on from the raw text preparation carried out in `01_data_collection.ipynb`.\n",
    "\n",
    "The aim is to identify and organise key domain-specific terms that appear in environmental texts. These include species names, habitat types, ecological processes, pollutants, and scientific measurements. General-purpose NER models often fail to capture such terms accurately, as they are either underrepresented or entirely absent in mainstream corpora.\n",
    "\n",
    "The vocabulary lists created here will be used in the next step to apply rule-based annotation. This pre-annotation will enable the creation of weakly labelled data, which will then be used to train statistical and machine learning models for environmental NER. As a result, this stage is critical in determining the coverage and relevance of named entities in the training set.\n",
    "\n",
    "Vocabulary terms are gathered from a range of publicly available sources, including domain-specific glossaries, classification systems, and structured datasets. Some are extracted using scripts, while others are compiled manually. The final outputs will be cleaned, deduplicated, and stored in a structured format ready for annotation.\n",
    "\n",
    "### 1.2 Objectives\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "- To define a set of named entity categories that reflect the types of concepts commonly found in environmental science\n",
    "- To identify and extract vocabulary terms for each category from trusted sources, including ontologies, glossaries, and environmental datasets\n",
    "- To standardise and clean the collected terms, removing duplicates and formatting them consistently\n",
    "- To save the resulting vocabulary files in a format suitable for rule-based matching and future model training\n",
    "\n",
    "Each category is handled in a separate section, with a clear explanation of sources used, any filtering applied, and a final cleaned list. These outputs will be used to annotate the sentence-level dataset produced earlier in the pipeline.\n",
    "\n",
    "### 1.3 Challenges in Vocabulary Development\n",
    "\n",
    "Developing vocabulary lists for environmental NER presents several challenges that need to be addressed carefully:\n",
    "\n",
    "- **Choosing meaningful categories**  \n",
    "  There is no single standard for environmental entity types. Categories must be defined based on project needs, typical text content, and the structure of existing ontologies. Each category should be well defined and distinct from the others.\n",
    "\n",
    "- **Finding reliable and relevant sources**  \n",
    "  Domain-specific terms are spread across many different resources. These resources vary in structure and accessibility. Some are easy to extract from; others require manual effort or custom scripts. Some of these are easy to extract; others require manual effort or custom scripts.\n",
    "\n",
    "- **Managing ambiguity**  \n",
    "  Certain terms have multiple meanings in different contexts. For example, “lead” may refer to a pollutant or act as a verb. These terms need to be flagged and treated cautiously during annotation.\n",
    "\n",
    "- **Balancing coverage with precision**  \n",
    "  A broad vocabulary may capture more entities but also introduces noise. It is important to focus on terms that are likely to appear in the corpus and are useful for the task. Rare, overly general, or ambiguous entries may be excluded or reviewed manually.\n",
    "\n",
    "- **Handling formatting inconsistencies**  \n",
    "  Terms from different sources may vary in spelling, case, pluralisation, or punctuation. A standard cleaning process is needed to normalise these variations and ensure consistent matching later.\n",
    "\n",
    "These challenges underline the need for a clear and reproducible method for vocabulary collection. Each step must be documented and carefully structured to ensure high-quality results in the next phase of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fb50b-ae17-4a36-b7b7-95d990aec703",
   "metadata": {},
   "source": [
    "## 2. Defining Entity Categories\n",
    "\n",
    "### 2.1 Choosing the Entity Categories\n",
    "\n",
    "Named Entity Recognition in environmental science requires clearly defined entity types that reflect the domain’s specialised vocabulary. General NER categories, such as “Organisation” or “Location,” are not specific enough to capture the kinds of entities that appear in environmental text. Terms such as species names, habitat descriptors, or chemical pollutants need to be grouped meaningfully to support accurate tagging.\n",
    "\n",
    "The categories chosen for this notebook aim to balance three key criteria: specificity, clarity, and practical relevance. Each category should represent a well-understood environmental concept, be distinct from the others, and be useful for annotating terms that frequently appear in environmental documents. This includes both scientific abstracts and semi-structured metadata.\n",
    "\n",
    "The category design also draws on patterns seen in domain vocabularies, classification systems, and earlier review of the text corpus collected earlier. These sources reveal recurring term types such as biological taxa, ecosystem names, pollution indicators, and measurement units. Grouping these into separate categories improves both annotation consistency and future model training.\n",
    "\n",
    "### 2.2 Category Review Process\n",
    "The selection of entity categories was based on a review of environmental materials and the text data collected earlier. The aim was to identify recurring patterns in how key concepts are mentioned and described across different sources.\n",
    "\n",
    "To guide this process, a range of domain-specific materials were considered, including glossaries, classification systems, and datasets. These were not used to define categories directly, but to observe common term types that appear in environmental science. In parallel, the collected corpus was manually reviewed to spot frequent entities and assess their natural groupings.\n",
    "\n",
    "The process was iterative. Some categories, such as species names and pollutants, were clearly distinguishable from the start. Others, like environmental processes or measurements, required refinement to avoid overlapping meanings or inconsistent boundaries. Wherever possible, categories were defined to be specific, distinct, and useful for labelling terms that appear regularly in environmental text.\n",
    "\n",
    "### 2.3 Final Entity Categories\n",
    "\n",
    "Based on the review process above, five named entity categories were selected. Each category is designed to capture a specific type of domain-relevant term found in environmental science text.\n",
    "\n",
    "| Entity Category | Description | Example Terms |\n",
    "|------------------|-------------|----------------|\n",
    "| TAXONOMY         | Names of species, genera, families, or other taxonomic units | *Salmo salar*, *Panthera leo* |\n",
    "| HABITAT          | Names of ecosystems, land cover types, or habitat descriptors | estuary, peatland, saltmarsh |\n",
    "| ENV_PROCESS      | Environmental or ecological processes, including both natural and human-driven events | erosion, eutrophication, acidification |\n",
    "| POLLUTANT        | Chemical or physical substances known to cause pollution or environmental harm | mercury, nitrate, microplastics |\n",
    "| MEASUREMENT      | Units, quantities, or indicators used to measure environmental variables | pH, mg/L, temperature, CO₂ levels |\n",
    "\n",
    "These categories are used to group vocabulary terms in the next sections. Each term list is built and processed separately, then saved in a structured format for annotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b248cc7-ad83-4760-898d-b2c382e1d31e",
   "metadata": {},
   "source": [
    "## 3. Collecting Vocabulary Terms\n",
    "This section focuses on building the vocabulary lists for each of the entity categories defined earlier. For each category, relevant terms are collected from domain-specific resources such as glossaries, classification systems, and downloadable datasets. Where possible, structured sources are processed programmatically. Others are manually reviewed or extracted depending on format and accessibility.\n",
    "\n",
    "The goal is to compile targeted, category-specific lists that reflect the kinds of entities typically found in environmental science texts. Each vocabulary is cleaned, standardised, and saved in a structured format for later use in annotation and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a429c5-af4c-4b5c-b4b7-1bbcd5385843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path(\"../vocabs\")\n",
    "RAW_PATH = BASE_PATH / \"raw\"\n",
    "EXTRACTED_PATH = BASE_PATH / \"extracted\"\n",
    "\n",
    "RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
    "EXTRACTED_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccaf952-52b9-409c-b5cc-5fae26d02965",
   "metadata": {},
   "source": [
    "### 3.1 Taxonomy\n",
    "\n",
    "The taxonomy vocabulary is based on the GBIF Backbone Taxonomy dataset [link](https://www.gbif.org/dataset/d7dddbf4-2cf0-4f39-9b2a-bb099caae36c), which provides a large set of biological names across all major taxonomic groups. The raw file was downloaded as a TSV and includes a wide range of vernacular terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235e85f-eddf-4fce-8038-5a8a184552e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxonID</th>\n",
       "      <th>vernacularName</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>countryCode</th>\n",
       "      <th>sex</th>\n",
       "      <th>lifeStage</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5371864</td>\n",
       "      <td>sels næp</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5371864</td>\n",
       "      <td>spreng-rood</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5371864</td>\n",
       "      <td>syle-næbber</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3034225</td>\n",
       "      <td>angelsrot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3034225</td>\n",
       "      <td>mjølkerot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nordic plant uses from Gunnerus and Høeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   taxonID vernacularName language country countryCode  sex lifeStage  \\\n",
       "0  5371864       sels næp       no     NaN         NaN  NaN       NaN   \n",
       "1  5371864    spreng-rood       no     NaN         NaN  NaN       NaN   \n",
       "2  5371864    syle-næbber       no     NaN         NaN  NaN       NaN   \n",
       "3  3034225      angelsrot      NaN     NaN         NaN  NaN       NaN   \n",
       "4  3034225      mjølkerot      NaN     NaN         NaN  NaN       NaN   \n",
       "\n",
       "                                     source  \n",
       "0  Nordic plant uses from Gunnerus and Høeg  \n",
       "1  Nordic plant uses from Gunnerus and Høeg  \n",
       "2  Nordic plant uses from Gunnerus and Høeg  \n",
       "3  Nordic plant uses from Gunnerus and Høeg  \n",
       "4  Nordic plant uses from Gunnerus and Høeg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RAW_PATH / \"VernacularName.tsv\", sep='\\t', dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1de8e-2c61-4315-a7f9-42cee573a69c",
   "metadata": {},
   "source": [
    "An initial preview of the file revealed that the entries span multiple languages, stored under a `language` column. Since the corpus being annotated is in English, only entries marked as English (`language == 'en'`) are retained for this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a51dce83-e5fa-44a1-9bcb-ce4a15a8fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RAW_PATH / \"VernacularName.tsv\", sep='\\t', usecols=[\"vernacularName\", \"language\"], dtype=str)\n",
    "\n",
    "df = df[df[\"language\"] == \"en\"]\n",
    "df = df.dropna(subset=[\"vernacularName\"])\n",
    "df[\"vernacularName\"] = df[\"vernacularName\"].str.strip().str.lower()\n",
    "df = df[df[\"vernacularName\"] != \"\"]\n",
    "\n",
    "taxonomy_terms = df[\"vernacularName\"].drop_duplicates().sort_values()\n",
    "\n",
    "taxonomy_path = EXTRACTED_PATH / \"taxonomy.txt\"\n",
    "taxonomy_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "taxonomy_terms.to_csv(taxonomy_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf81422e-d4f4-4ea2-866b-fb303f4fcdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229559\n"
     ]
    }
   ],
   "source": [
    "print(len(taxonomy_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fda9ce79-7e69-4901-84d7-f60ebd7fe07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abandoned industrial site\n",
      "accidental release of organisms\n",
      "afforestation\n",
      "afforestations\n",
      "agricultural landscape\n",
      "alluvial plain\n",
      "altitude\n",
      "altitudes\n",
      "amusement park\n",
      "anaerobic lagoon\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539757b-8946-4545-a0ff-84eefa96835f",
   "metadata": {},
   "source": [
    "The extracted list contains approximately 229,000 unique English-language names. From a brief manual inspection, the terms appear accurate, well-formed, and highly relevant to biological taxonomy. Many include common species and group-level names, making this a strong starting point.\n",
    "\n",
    "While the number of terms is large, it is not expected to negatively impact annotation. Any names not found in the corpus will simply not be matched. Basic cleaning will be applied later to remove duplicates, overly generic names, or problematic entries where necessary.\n",
    "\n",
    "Due to the size of the list, full manual review is not feasible. However, spot checks indicate the data quality is high. A standardised version of the vocabulary will be prepared during the cleaning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac70d01-ebc0-4cdd-a8b3-d8a3c1c7a39e",
   "metadata": {},
   "source": [
    "### 3.2 Habitat\n",
    "The habitat vocabulary is based on the GEMET thesaurus, which includes a wide range of environmental and ecological terms. It provides structured terminology for describing natural environments, land cover types, and habitat-related concepts. The terms were downloaded in structured format and filtered for habitat-relevant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "235176f5-e1fe-44b3-8b4b-735cfe348b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gemet_theme(theme_id: int):\n",
    "    \"\"\"Scrapes GEMET terms under a given theme and writes them to a text file\"\"\"\n",
    "    \n",
    "    base_url = f\"https://www.eionet.europa.eu/gemet/en/theme/{theme_id}/concepts/?page={{}}&letter=0\"\n",
    "    \n",
    "    page = 1\n",
    "    terms = set()\n",
    "\n",
    "    print(f\"Scraping theme ID {theme_id}\")\n",
    "    \n",
    "    while True:\n",
    "        url = base_url.format(page)\n",
    "        print(f\"Page {page}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        term_elements = soup.select(\"ul.listing.columns.split-20 li a\")\n",
    "        if not term_elements:\n",
    "            break\n",
    "\n",
    "        new_terms = {el.get_text(strip=True) for el in term_elements}\n",
    "        if new_terms.issubset(terms):\n",
    "            break\n",
    "\n",
    "        terms.update(new_terms)\n",
    "        page += 1\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return sorted(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adce0ced-91cf-422f-aa8f-2061a7c38aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping theme ID 23\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Saved habitat terms to habitat.txt\n"
     ]
    }
   ],
   "source": [
    "habitat_terms = scrape_gemet_theme(theme_id=23)\n",
    "\n",
    "output_path = EXTRACTED_PATH / \"habitat.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in habitat_terms:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Saved habitat terms to {output_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2eeb6517-ea0f-4ded-bbb0-76359c593b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n"
     ]
    }
   ],
   "source": [
    "print(len(habitat_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5acb88f-8111-4561-ae7c-a1b8fc9d777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperate woodland\n",
      "wildlife sanctuary\n",
      "dam draining\n",
      "coast protection\n",
      "rivers\n",
      "periurban space\n",
      "site rehabilitation\n",
      "area under stress\n",
      "resource scarcity\n",
      "bays\n",
      "forest fire\n",
      "cultural ecosystem services\n",
      "coastal environment\n",
      "fens\n",
      "nesting area\n",
      "forest\n",
      "forest reserve\n",
      "integrated environmental assessment\n",
      "bocages\n",
      "abandoned industrial site\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bf8fc-f4c6-4a2f-9a38-2442e3743285",
   "metadata": {},
   "source": [
    "The extracted list for habitat terms includes 467 unique entries from the GEMET thesaurus under the \"natural areas\" theme. While this number is smaller compared to the taxonomy list, it is expected since habitats are a more constrained and well-defined concept, with fewer distinct variations than species names.\n",
    "\n",
    "The list covers a broad set of environments, including terms such as *marshes*, *coastal environment*, *catchment area*, *bog*, *sand dune fixation*, *mountain forest*, and *terraced landscape*. These are well-formed, domain-relevant, and reflect both natural and managed habitats.\n",
    "\n",
    "Initial inspection confirms the quality and relevance of the terms. The vocabulary will be further cleaned and standardised before annotation, but no immediate expansion is needed unless specific gaps are identified later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cfae0b-802d-4efd-aa1d-d51feeb8e71b",
   "metadata": {},
   "source": [
    "### 3.3 Environmental Processes\n",
    "This category includes vocabulary terms that describe natural, physical, or chemical processes occurring within the environment. These may include terms related to climate, hydrology, soil dynamics, pollution cycles, and other system-level interactions commonly found in environmental science texts.\n",
    "\n",
    "The goal is to capture processes that are relevant to ecological modelling, environmental monitoring, and scientific descriptions of system change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fd76f93-8981-4e6e-bd13-9ad2edd9157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping theme ID 7\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Saved env_process terms to env_process.txt\n"
     ]
    }
   ],
   "source": [
    "env_process_path = EXTRACTED_PATH / \"env_process.txt\"\n",
    "\n",
    "climate_terms = scrape_gemet_theme(theme_id=7)\n",
    "\n",
    "with open(env_process_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in climate_terms:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Saved env_process terms to {env_process_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c6971ac-0a7e-4682-9fa5-f1149155aee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(climate_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e1ca632-373a-4a3b-9a66-e028e645b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation enhancement\n",
      "season\n",
      "meteorological disaster\n",
      "oceanic climate\n",
      "flood forecast\n",
      "haze\n",
      "atmospheric structure\n",
      "climate regulation\n",
      "tornado\n",
      "weather modification\n",
      "snow\n",
      "storm damage\n",
      "air conditioning\n",
      "troposphere\n",
      "meteorological research\n",
      "mountain climate\n",
      "ozone layer\n",
      "atmospheric precipitation\n",
      "climate protection\n",
      "atmospheric composition\n"
     ]
    }
   ],
   "source": [
    "with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40ef20-3323-4885-a332-97a0c5c777c7",
   "metadata": {},
   "source": [
    "The environmental process vocabulary was initially sourced from the GEMET thesaurus under the \"climate\" theme, which produced 128 terms. This list includes core concepts such as *global warming*, *ozone layer*, *atmospheric composition*, *feedback loop*, and *water scarcity*.\n",
    "\n",
    "While these are relevant and well-formed, the coverage is narrow and heavily climate-focused. Environmental processes span a wider range of concepts including soil dynamics, chemical pollution cycles, hydrology, and ecosystem-level change. These are not well represented in the current list.\n",
    "\n",
    "Additional sources will be explored to expand this vocabulary, including manually compiled lists and process-focused glossaries or ontologies. The GEMET terms will still be included as part of the final vocabulary after cleaning and standardisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e86504ad-4329-4c26-a6ae-1bb6fa812695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: climate\n",
      "Scraping theme ID 5\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Scraping: pollution\n",
      "Scraping theme ID 26\n",
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Page 13\n",
      "Page 14\n",
      "Page 15\n",
      "Page 16\n",
      "Scraping: natural_dynamics\n",
      "Scraping theme ID 36\n",
      "Page 1\n",
      "Page 2\n"
     ]
    }
   ],
   "source": [
    "theme_ids = {\n",
    "    \"climate\": 5,\n",
    "    \"pollution\": 26,\n",
    "    \"natural_dynamics\": 36,\n",
    "}\n",
    "\n",
    "all_terms = set()\n",
    "\n",
    "for theme, theme_id in theme_ids.items():\n",
    "    print(f\"Scraping: {theme}\")\n",
    "    \n",
    "    terms = scrape_gemet_theme(theme_id)\n",
    "    all_terms.update(term.lower() for term in terms if term.strip())\n",
    "    \n",
    "    with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            term = line.strip().lower()\n",
    "            if term:\n",
    "                all_terms.add(term)\n",
    "\n",
    "with open(env_process_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in sorted(all_terms):\n",
    "        f.write(term + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd96a917-f92e-4772-b3e4-334e06039581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n"
     ]
    }
   ],
   "source": [
    "print(len(all_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ed8b1fff-f21b-47d7-befc-c3f6fcf08e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microclimate\n",
      "land restoration\n",
      "combined cycle-power station\n",
      "tundra\n",
      "bleaching process\n",
      "anaerobic treatment\n",
      "advection\n",
      "building service\n",
      "chemical decontamination\n",
      "environmental impact of agriculture\n",
      "hail\n",
      "salination\n",
      "polluter-pays principle\n",
      "olfactory pollution\n",
      "nursery garden\n",
      "water pollution\n",
      "pollution monitoring\n",
      "building site\n",
      "physical alteration\n",
      "sea level rise\n"
     ]
    }
   ],
   "source": [
    "with open(env_process_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24885b-182d-476c-8511-99cbb70fe2da",
   "metadata": {},
   "source": [
    "The expanded list for environmental processes now includes 1,003 unique terms aggregated from multiple GEMET themes, including climate, pollution, and natural dynamics. This broader coverage captures a wide range of relevant concepts such as microclimate, chemical decontamination, salination, environmental impact of agriculture, and pollution monitoring. While some entries are loosely defined or overlap with other categories, the overall list is diverse and strongly aligned with the types of system-level processes found in environmental science literature. The vocabulary will be reviewed and refined in the next stage to remove ambiguous or overly general terms, but the current coverage is considered sufficient for initial annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ad82e-a726-4adf-a663-970a4ae52a2a",
   "metadata": {},
   "source": [
    "### 3.4 Pollutants\n",
    "The pollutants vocabulary includes chemical substances and other environmental stressors that are commonly referenced in environmental science. These may be individual compounds such as benzene or lead, or broader classes such as pesticides, microplastics, or particulate matter. These terms are often found in regulatory reports, monitoring programmes, and scientific publications that focus on pollution, exposure, and environmental impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5f67e-b06a-4850-9cbe-dfc91ec6d584",
   "metadata": {},
   "source": [
    "To build this vocabulary, the Toxics Release Inventory (TRI) Chemical List was selected as the source. This is a publicly available list of substances that are tracked under the US EPA’s TRI reporting programme, which requires facilities to report on the management of certain toxic chemicals. The list is maintained as part of the EPA’s environmental transparency efforts and includes both individual chemicals and chemical categories.\n",
    "\n",
    "The full list was downloaded in spreadsheet format from the following source:\n",
    "https://www.epa.gov/toxics-release-inventory-tri-program/tri-listed-chemicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28aff3ae-6889-4c1c-884a-6efa6533469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hkhan/Projects/env-ner-project/.venv/lib/python3.12/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASRN</th>\n",
       "      <th>TRI Chemical or Chemical Category Name</th>\n",
       "      <th>Chemical Structure</th>\n",
       "      <th>De Minimis Limit %</th>\n",
       "      <th>M,P/OU Thresholds (lb)</th>\n",
       "      <th>Category Description</th>\n",
       "      <th>Category Member</th>\n",
       "      <th>Additional Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71751-41-2</td>\n",
       "      <td>Abamectin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30560-19-1</td>\n",
       "      <td>Acephate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75-07-0</td>\n",
       "      <td>Acetaldehyde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60-35-5</td>\n",
       "      <td>Acetamide</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75-05-8</td>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>25,000/10,000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CASRN TRI Chemical or Chemical Category Name  Chemical Structure  \\\n",
       "0  71751-41-2                               Abamectin                NaN   \n",
       "1  30560-19-1                                Acephate                NaN   \n",
       "2     75-07-0                            Acetaldehyde                NaN   \n",
       "3     60-35-5                               Acetamide                NaN   \n",
       "4     75-05-8                            Acetonitrile                NaN   \n",
       "\n",
       "  De Minimis Limit % M,P/OU Thresholds (lb) Category Description  \\\n",
       "0                  1          25,000/10,000                  NaN   \n",
       "1                  1          25,000/10,000                  NaN   \n",
       "2                0.1          25,000/10,000                  NaN   \n",
       "3                0.1          25,000/10,000                  NaN   \n",
       "4                  1          25,000/10,000                  NaN   \n",
       "\n",
       "  Category Member Additional Information  \n",
       "0             NaN                    NaN  \n",
       "1             NaN                    NaN  \n",
       "2             NaN                    NaN  \n",
       "3             NaN                    NaN  \n",
       "4             NaN                    NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(RAW_PATH / \"2024_tri_chemical_list.xlsx\", engine=\"openpyxl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e974c0-d897-48cf-a6ed-abd6f0ed29d3",
   "metadata": {},
   "source": [
    "The TRI chemical file includes a structured list of pollutant names, along with metadata such as CAS numbers, thresholds, and classification notes. A preview of the dataset shows chemical entries like Abamectin, Acetaldehyde, and Acetonitrile, which reflect a balance of technical specificity and practical relevance.\n",
    "\n",
    "This initial inspection confirms that the file contains suitable pollutant terms for vocabulary extraction. Further processing will focus on selecting and standardising these names into a plain text list for use in annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07402b9d-3c5f-4c33-9189-9deb44622f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "pollutant_terms = (\n",
    "    df[\"TRI Chemical or Chemical Category Name\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .drop_duplicates()\n",
    "    .sort_values()\n",
    ")\n",
    "\n",
    "pollutant_path = EXTRACTED_PATH / \"pollutant.txt\"\n",
    "pollutant_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "pollutant_terms.to_csv(pollutant_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05d230dd-0c5b-49c8-9064-536b24121276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pollutant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "698ccb56-4803-4460-b424-1bf5ee0a475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dichlorobromomethane\n",
      "nickel\n",
      "paraldehyde\n",
      "acrolein\n",
      "bromine\n",
      "dihydrosafrole\n",
      "cycloate\n",
      "\"3,3'-dimethylbenzidine dihydrofluoride\"\n",
      "\"3,3'-dimethylbenzidine dihydrochloride\"\n",
      "triforine\n",
      "diaminotoluene (mixed isomers) (toluenediamine)\n",
      "c.i. direct blue 218\n",
      "\"hexamethylene-1,6-diisocyanate\"\n",
      "\"1,2-dichloro-1,1,3,3,3-pentafluoropropane (hcfc-225da)\"\n",
      "\"1,2,3,7,8‑pentachlorodibenzo-p-dioxin\"\n",
      "chlorothalonil\n",
      "2-nitrophenol (o-nitrophenol)\n",
      "perchloromethyl mercaptan\n",
      "sodium azide\n",
      "\"1,1,2,2-tetrachloroethane\"\n"
     ]
    }
   ],
   "source": [
    "with open(pollutant_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "sampled = random.sample(terms, 20)\n",
    "for term in sampled:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac192e7-f993-46c5-bb2b-23238fe52dc1",
   "metadata": {},
   "source": [
    "A total of 728 unique pollutant terms were extracted from the TRI list. The sample includes a mix of well-formed names and more complex entries, such as those wrapped in quotation marks or containing grouped chemicals.\n",
    "\n",
    "Basic cleaning will be applied to remove entries with formatting issues or compound categories that are too broad for direct annotation. Since the list is relatively small, this will be done manually.\n",
    "\n",
    "Some common terms like pesticide, ozone, or microplastic are not included in the TRI list. These will be added separately to improve coverage across general environmental texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be16e3-823d-447a-bf4d-a187c3f8e640",
   "metadata": {},
   "source": [
    "### 3.5 Measurements\n",
    "This category includes vocabulary related to units, scales, and quantities commonly used in environmental datasets and reports. These may appear alongside named entities or as standalone references in measurements, thresholds, or limits.\n",
    "\n",
    "Examples include standard units like mg/L, ppm, µg/m³, and domain-specific terms such as emission factor, exceedance, or threshold concentration. These are important for tasks such as span-based annotation, context interpretation, and distinguishing between entity mentions and surrounding descriptors.\n",
    "\n",
    "Relevant terms will be collected from environmental glossaries, standards documentation, and precompiled unit lists. A final cleaned vocabulary will be prepared to support annotation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc6181-0b2e-413f-9618-1a04031a2630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaab8f7-10a9-4028-a8c5-cf9478fae2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753486da-7e7d-4727-a425-f008a7b0feb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554211bf-bc25-4f84-bff6-954540818af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adaf027-5478-4093-b0e1-f0cd12ff6f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad110e-d886-44d1-beb1-34c6f2b9f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3c951-f47e-48d8-8ec3-c7da0a5a5bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f62820-5775-4574-8fee-cfcaca7b183d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319da90c-eeaf-447b-8632-a98fc5548558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe468896-c754-4a4d-8615-c11449c37961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75096a4f-4ebc-49ef-b8b4-3c9f95e8153b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e312c13-d63e-41ea-8716-3858e9aa6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f73d4dc3-9af8-4871-a81c-2d4d9f8f2175",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Filtering Terms\n",
    "Lemmatise all GEMET terms.\n",
    "\n",
    "Remove:\n",
    "\n",
    "* Words that are too generic (e.g. \"animal\", \"life\")\n",
    "\n",
    "* Words that are < 3 characters\n",
    "\n",
    "* Anything that doesn't fit your NER context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f88a2-cbf7-49bc-b04c-4ace0c6d23c2",
   "metadata": {},
   "source": [
    "## 3. Lemmatization Script (with spaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf871bb8-1dbe-460a-8e93-071e7af08935",
   "metadata": {},
   "source": [
    "one of the below is correct and should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a63d9-fe25-4544-8ea0-7b1ada414b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "VOCAB_DIR = Path(\"gemet_terms\")\n",
    "CLEAN_DIR = Path(\"../vocabularies\")\n",
    "CLEAN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "for vocab_file in VOCAB_DIR.glob(\"*.txt\"):\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        terms = {line.strip() for line in f if line.strip()}\n",
    "\n",
    "    lemmatised = set()\n",
    "    for term in terms:\n",
    "        doc = nlp(term)\n",
    "        lemma = \" \".join([token.lemma_ for token in doc])\n",
    "        lemmatised.add(lemma.lower())\n",
    "\n",
    "    # Filter out generic or short terms (e.g., less than 3 chars)\n",
    "    filtered = sorted({term for term in lemmatised if len(term) > 2 and not term.isnumeric()})\n",
    "    \n",
    "    out_path = CLEAN_DIR / vocab_file.name\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for term in filtered:\n",
    "            f.write(term + \"\\n\")\n",
    "    \n",
    "    print(f\"{vocab_file.name}: {len(filtered)} terms saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fccfc0-e3a0-4fef-834a-f52f92f5bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "OUTPUT_DIR = Path(\"..\") / \"data\" / \"processed_data\"\n",
    "VOCAB_DIR = Path(\"..\") / \"vocabularies\"\n",
    "\n",
    "input_path = VOCAB_DIR / \"taxonomy.txt\"\n",
    "output_path = VOCAB_DIR / \"taxonomy_lemmatized.txt\"\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read original taxonomy terms\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "lemmatised_terms = set()\n",
    "\n",
    "for term in terms:\n",
    "    doc = nlp(term)\n",
    "    lemma = \" \".join([token.lemma_ for token in doc])\n",
    "    lemmatised_terms.add(lemma)\n",
    "\n",
    "# Sort and save\n",
    "lemmatised_sorted = sorted(lemmatised_terms)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for term in lemmatised_sorted:\n",
    "        f.write(term + \"\\n\")\n",
    "\n",
    "print(f\"Lemmatized {len(terms)} terms down to {len(lemmatised_terms)} unique ones.\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398181f6-d185-4809-b4db-3d13a19aabb9",
   "metadata": {},
   "source": [
    "This step uses SpaCy to lemmatise all terms to their base forms (e.g., \"habitats\" → \"habitat\"). Generic terms and words shorter than 3 characters were removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb6429-cc2d-4532-b6e1-add0dc0f2c0d",
   "metadata": {},
   "source": [
    "We now have cleaned and curated vocabulary lists for each NER category:\n",
    "\n",
    "- `taxonomy.txt` (from BHL)\n",
    "- `habitat.txt`\n",
    "- `pollutant.txt`\n",
    "- `env_process.txt`\n",
    "- `measurement.txt`\n",
    "\n",
    "These vocabulary files are stored in the `vocabularies/` directory and are ready to be used for Aho-Corasick-based annotation. If additional terms are discovered later, they can be appended and recompiled on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda8bb8-1800-4099-b7d1-3295e9f56de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae5f44-1227-463f-82b3-357f5588c297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ae463-7da8-4ea8-b5ef-b32d3f786b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30e339-b5b9-49f0-a12e-d08840b09319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19feffac-f706-4c8c-bdd8-6b08d8363f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf2ab9-8400-476d-9167-db6176ea89ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a538048e-83c5-48b3-98ed-7b06d13afd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c7f3a-48c1-4de0-873d-7797f680f73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
