{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6eb4dac-d119-4a90-a99c-487561a27a04",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "\n",
    "This notebook focuses on collecting raw text data from a range of environmental science sources. The goal is to gather unprocessed content that will later be used for training and evaluating a Named Entity Recognition (NER) model tailored to the environmental domain.\n",
    "\n",
    "For each source, the notebook extracts available text fields such as titles, descriptions, and full document content where possible. These are written line by line into a unified text file, without applying any preprocessing. This ensures the original structure and language of the data are preserved for later inspection, annotation, and model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fbcd0-961c-4f00-bf55-110a80778789",
   "metadata": {},
   "source": [
    "## UKCEH Data Collection – Setup\n",
    "\n",
    "This notebook is focused on collecting raw text data from the UKCEH data catalogue. The goal is to extract dataset titles, descriptions, and any supporting documents. At this stage, no preprocessing or annotation is applied.\n",
    "\n",
    "The extracted content will be stored in a single text file (`data/raw_data/ukceh/ukceh_data.txt`) with one dataset per line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb92de5-6e6a-4049-9e46-859ffb47c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import tempfile\n",
    "import zipfile\n",
    "import re\n",
    "import unicodedata\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from docx import Document\n",
    "from pdfminer.high_level import extract_text as extract_pdf_text\n",
    "import filetype\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\" / \"ukceh\"\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE = BASE_DIR / \"ukceh_data.txt\"\n",
    "\n",
    "BASE_URL = \"https://catalogue.ceh.ac.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97983970-a1b6-4114-a924-0aaf01510b45",
   "metadata": {},
   "source": [
    "### Define page scraper\n",
    "\n",
    "This function loads a single catalogue page using Selenium and returns all dataset links from it. Each dataset link corresponds to a document detail page that may include downloadable supporting documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e645f791-05df-46ad-81e7-fa6149e09f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(page_num):\n",
    "    url = f\"{BASE_URL}/?page={page_num}\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    results_list = soup.find(\"div\", class_=\"results__list\")\n",
    "    if not results_list:\n",
    "        return []\n",
    "\n",
    "    dataset_links = [\n",
    "        BASE_URL + a['href']\n",
    "        for a in results_list.find_all(\"a\", href=True)\n",
    "        if a['href'].startswith(\"/documents/\")\n",
    "    ]\n",
    "    return dataset_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15735613-1706-4243-98a9-912ab9c0b479",
   "metadata": {},
   "source": [
    "### Get dataset metadata\n",
    "\n",
    "Given a dataset URL, this function extracts the title, description, and supporting document download link. These elements are scraped directly from the HTML structure of the dataset detail page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15c0de1e-430d-4c1a-8ee4-6ee43a2abdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_info(dataset_url):\n",
    "    response = requests.get(dataset_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True)\n",
    "    desc_div = soup.find(\"div\", class_=\"description-text\")\n",
    "    description = desc_div.get_text(strip=True) if desc_div else \"\"\n",
    "\n",
    "    supporting_tag = soup.find(\"a\", class_=\"btn btn-access\")\n",
    "    supporting_link = supporting_tag['href'] if supporting_tag else None\n",
    "\n",
    "    return title, description, supporting_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d10ef0-fd8a-4351-87bd-ed840a2fda9a",
   "metadata": {},
   "source": [
    "### Read text from a single file\n",
    "\n",
    "This function attempts to read content from a file of type PDF, DOCX, or TXT. For PDFs, it first uses a text-based parser and falls back to OCR if needed. Files that do not contain extractable text are skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47f42d29-f12a-445a-b5c6-9c0af33e8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_text(file_path):\n",
    "    try:\n",
    "        kind = filetype.guess(file_path)\n",
    "        ext = kind.extension if kind else os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "        if ext == \"pdf\":\n",
    "            with contextlib.redirect_stderr(io.StringIO()):\n",
    "                text = extract_pdf_text(file_path).replace(\"\\n\", \" \").strip()\n",
    "            \n",
    "            if text:\n",
    "                return text\n",
    "            \n",
    "            text_parts = []\n",
    "            doc = fitz.open(file_path)\n",
    "            for page in doc:\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                ocr_text = pytesseract.image_to_string(img).strip()\n",
    "                if ocr_text:\n",
    "                    text_parts.append(ocr_text.replace(\"\\n\", \" \"))\n",
    "            return \" \".join(text_parts).strip()\n",
    "\n",
    "        elif ext == \"docx\":\n",
    "            doc = Document(file_path)\n",
    "            return \" \".join(p.text for p in doc.paragraphs).strip()\n",
    "\n",
    "        elif ext == \"txt\":\n",
    "            with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                return f.read().replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34335f6d-210c-492b-8e5d-01017b457d08",
   "metadata": {},
   "source": [
    "### Download and extract ZIP content\n",
    "\n",
    "This function downloads a ZIP file containing supporting documents for a dataset, extracts all readable files, and combines their content into a single string. If a file fails to process, it is ignored and an error is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a493aadd-e5b3-4814-81b6-8802fa757634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_zip(zip_url, dataset_id):\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        if not response.ok:\n",
    "            \n",
    "            return \"\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            zip_path = os.path.join(tmpdir, f\"{dataset_id}.zip\")\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdir)\n",
    "\n",
    "            text_chunks = []\n",
    "            for root, _, files in os.walk(tmpdir):\n",
    "                for name in files:\n",
    "                    file_path = os.path.join(root, name)\n",
    "                    try:\n",
    "                        text = read_file_text(file_path)\n",
    "                        if text:\n",
    "                            text_chunks.append(text)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read file {name}: {e}\")\n",
    "\n",
    "            return \" \".join(text_chunks).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while processing zip: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59ce5407-34e6-487b-b6b1-481d216d2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = re.sub(r\"[•▪●‣–—·]\", \" \", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \"...\", text)\n",
    "    text = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \" \", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"[- ]{2,}\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e11a1-578f-4078-bf72-324f96f7e1c1",
   "metadata": {},
   "source": [
    "### Write line to file\n",
    "\n",
    "This writes the dataset title, description, and raw extracted text into a single line in the output file. Each field is separated by a pipe (`|`). Preprocessing such as text cleaning is not applied at this stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b828b2b-081a-411c-97c7-8229bbf00b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(title, url, description, extracted_text, output_file):\n",
    "    cleaned_text = clean_text(extracted_text)\n",
    "    line = f\"{title} | {description} | {cleaned_text}\".strip()\n",
    "    \n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acead6-def8-48e4-a843-aaf404a69b34",
   "metadata": {},
   "source": [
    "### Loop through all catalogue pages\n",
    "\n",
    "This loop scrapes dataset links page-by-page and processes each dataset by extracting its metadata and any available supporting document content. The output is written to `data/raw_data/ukceh/ukceh_data.txt`.\n",
    "\n",
    "The process runs for 114 pages. If a page has no results, the loop stops early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d644c06-6acf-4887-86e9-64aff59c2287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Exception while processing zip: Response ended prematurely\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "Scraping page 40\n",
      "Scraping page 41\n",
      "Scraping page 42\n",
      "Scraping page 43\n",
      "Scraping page 44\n",
      "Scraping page 45\n",
      "Scraping page 46\n",
      "Scraping page 47\n",
      "Scraping page 48\n",
      "Scraping page 49\n",
      "Scraping page 50\n",
      "Scraping page 51\n",
      "Scraping page 52\n",
      "Scraping page 53\n",
      "Scraping page 54\n",
      "Scraping page 55\n",
      "Scraping page 56\n",
      "Scraping page 57\n",
      "Scraping page 58\n",
      "Scraping page 59\n",
      "Scraping page 60\n",
      "Scraping page 61\n",
      "Scraping page 62\n",
      "Scraping page 63\n",
      "Scraping page 64\n",
      "Scraping page 65\n",
      "Scraping page 66\n",
      "Scraping page 67\n",
      "Scraping page 68\n",
      "Scraping page 69\n",
      "Scraping page 70\n",
      "Scraping page 71\n",
      "Scraping page 72\n",
      "Scraping page 73\n",
      "Scraping page 74\n",
      "Scraping page 75\n",
      "Scraping page 76\n",
      "Scraping page 77\n",
      "Scraping page 78\n",
      "Scraping page 79\n",
      "Scraping page 80\n",
      "Scraping page 81\n",
      "Scraping page 82\n",
      "Scraping page 83\n",
      "Scraping page 84\n",
      "Scraping page 85\n",
      "Scraping page 86\n",
      "Scraping page 87\n",
      "Scraping page 88\n",
      "Scraping page 89\n",
      "Scraping page 90\n",
      "Scraping page 91\n",
      "Scraping page 92\n",
      "Scraping page 93\n",
      "Scraping page 94\n",
      "Scraping page 95\n",
      "Scraping page 96\n",
      "Scraping page 97\n",
      "Scraping page 98\n",
      "Scraping page 99\n",
      "Scraping page 100\n",
      "Scraping page 101\n",
      "Scraping page 102\n",
      "Scraping page 103\n",
      "Scraping page 104\n",
      "Scraping page 105\n",
      "Scraping page 106\n",
      "Scraping page 107\n",
      "Scraping page 108\n",
      "Scraping page 109\n",
      "Scraping page 110\n",
      "Scraping page 111\n",
      "Scraping page 112\n",
      "Scraping page 113\n",
      "Scraping page 114\n"
     ]
    }
   ],
   "source": [
    "for page in range(1, 115):\n",
    "    print(f\"Scraping page {page}\")\n",
    "    urls = get_links_from_page(page)\n",
    "\n",
    "    if not urls:\n",
    "        print(f\"No results on page {page}\")\n",
    "        break\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            title, desc, zip_link = get_dataset_info(url)\n",
    "            dataset_id = url.split(\"/\")[-1]\n",
    "            extracted = \"\"\n",
    "\n",
    "            if zip_link:\n",
    "                extracted = extract_text_from_zip(zip_link, dataset_id)\n",
    "\n",
    "            write_to_file(title, url, desc, extracted, OUTPUT_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on dataset: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad602b-117e-41a9-926c-3f568eaf5de2",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Data\n",
    "\n",
    "This section performs lightweight preprocessing on the collected data. The goal is to remove unwanted noise, broken characters, extra whitespace, and junk lines (e.g. \"Table 1\", bullets, control characters). This improves model input quality without aggressively altering the raw data.\n",
    "\n",
    "For PubMed abstracts, a special step merges broken lines within each abstract using double newlines as separators. This ensures that full abstract texts are preserved and not split mid-sentence due to export formatting.\n",
    "\n",
    "### Impact on Sources\n",
    "\n",
    "- **PubMed**: Very positive. This merging step reconstructs the abstract into full coherent text blocks, critical for effective sentence segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "403aa82b-286c-4c3a-bec6-73be3b693eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "RAW_BASE = Path(\"../data/raw_data\")\n",
    "CLEAN_BASE = Path(\"../data/processed\")\n",
    "\n",
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None  # skip empty lines\n",
    "\n",
    "    line = unicodedata.normalize(\"NFKD\", line)  # remove weird Unicode ligatures\n",
    "    line = re.sub(r\"[•▪●‣–—·]\", \" \", line)  # bullet points\n",
    "    line = re.sub(r\"\\.{3,}\", \"...\", line)  # collapse dot chains\n",
    "    line = re.sub(r\"https?://\\S+\", \"\", line)  # remove URLs\n",
    "    line = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \"\", line)  # remove control chars\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)  # extra spaces\n",
    "\n",
    "    if len(line.split()) < 3:  # junk (like \"Table 1\", \"Appendix\", etc)\n",
    "        return None\n",
    "\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d68da6c0-9c1b-4ddb-a7df-1271fd105823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pubmed_lines(input_path):\n",
    "    \"\"\"Merge broken abstract lines using blank lines as separators.\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "\n",
    "    abstracts = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            current.append(line)\n",
    "        else:\n",
    "            if current:\n",
    "                abstracts.append(\" \".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        abstracts.append(\" \".join(current))\n",
    "    return abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7f8da90-ee6e-4992-8050-1439e6536bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(input_path, output_path, is_pubmed=False):\n",
    "    if is_pubmed:\n",
    "        raw_lines = merge_pubmed_lines(input_path)\n",
    "    else:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            raw_lines = infile.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in raw_lines:\n",
    "        cleaned = clean_line(line)\n",
    "        if cleaned:\n",
    "            cleaned_lines.append(cleaned)\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in cleaned_lines:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "    print(f\"Saved cleaned file: {output_path.name} ({len(cleaned_lines)} lines)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9a1c7e3-5c0f-45de-9c44-15126b280e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file: ukceh_data.txt (3948 lines)\n",
      "Saved cleaned file: env_news_data.txt (28669 lines)\n",
      "Saved cleaned file: abstract-habitat.txt (136427 lines)\n",
      "Saved cleaned file: abstract-env_process.txt (66957 lines)\n",
      "Saved cleaned file: abstract-environment.txt (66376 lines)\n",
      "Saved cleaned file: abstract-taxonomy.txt (45741 lines)\n",
      "Saved cleaned file: abstract-pollutants.txt (61937 lines)\n",
      "Saved cleaned file: abstract-measurement.txt (58694 lines)\n"
     ]
    }
   ],
   "source": [
    "for source in [\"ukceh\", \"environment_news\", \"pubmed\"]:\n",
    "    raw_dir = RAW_BASE / source\n",
    "    clean_dir = CLEAN_BASE / source\n",
    "    clean_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for txt_file in raw_dir.glob(\"*.txt\"):\n",
    "        is_pubmed = (source == \"pubmed\")\n",
    "        clean_file(txt_file, clean_dir / txt_file.name, is_pubmed=is_pubmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e88232-b2e3-4612-8c65-c60791bcdce1",
   "metadata": {},
   "source": [
    "## 3. Sentence segmentation\n",
    "\n",
    "After cleaning the raw documents, the next step involves sentence segmentation. This is necessary for two main reasons:\n",
    "\n",
    "1. Named Entity Recognition (NER) models work best when input is divided into grammatically coherent units (sentences).\n",
    "2. Many downstream annotation and evaluation tools expect one sentence per line.\n",
    "\n",
    "We use `spaCy` for this step, which applies robust rule-based and statistical models for accurate sentence boundary detection. The segmented sentences are stored under `../data/sentences/`, preserving the original filenames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e4d3fc5-a42e-4a98-a3d7-e94c2ac30dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ukceh_data.txt\n",
      "Done: env_news_data.txt\n",
      "Done: abstract-habitat.txt\n",
      "Done: abstract-env_process.txt\n",
      "Done: abstract-environment.txt\n",
      "Done: abstract-taxonomy.txt\n",
      "Done: abstract-pollutants.txt\n",
      "Done: abstract-measurement.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model and increase max length (safely)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2_000_000  # You can increase more if needed\n",
    "\n",
    "INPUT_DIR = Path(\"../data/processed\")\n",
    "OUTPUT_DIR = Path(\"../data/sentences\")\n",
    "\n",
    "def segment_sentences_streaming(input_path, output_path):\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for block in infile:\n",
    "            block = block.strip()\n",
    "            if not block:\n",
    "                continue\n",
    "\n",
    "            doc = nlp(block)\n",
    "            for sent in doc.sents:\n",
    "                sentence = sent.text.strip()\n",
    "                if sentence:\n",
    "                    outfile.write(sentence + \"\\n\")\n",
    "\n",
    "    print(f\"Done: {output_path.name}\")\n",
    "\n",
    "# Apply to all files\n",
    "for source in [\"ukceh\", \"environment_news\", \"pubmed\"]:\n",
    "    input_folder = INPUT_DIR / source\n",
    "    output_folder = OUTPUT_DIR / source\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in input_folder.glob(\"*.txt\"):\n",
    "        output_path = output_folder / file_path.name\n",
    "        segment_sentences_streaming(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019732f0-ef5b-4c0f-8a49-c160e0609177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d55458-253d-49dc-905d-2ff3696a4998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b1b84-a86e-4215-933d-51af100a451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94215c28-3f16-4363-a7e9-33c6b4a05764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6405611-1675-4236-b810-f8119ce6df78",
   "metadata": {},
   "source": [
    "# This is for processing news data so include this somewhere in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de1a55-ac46-44a3-91c2-873528ff1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load CSV (with error handling for malformed rows)\n",
    "csv_path = Path(\"data.csv\")\n",
    "df = pd.read_csv(csv_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Clean and combine Title, Authors, and Article Text columns\n",
    "combined_lines = df[[\"Title\", \"Intro Text\", \"Article Text\"]].dropna().astype(str).apply(\n",
    "    lambda row: \" | \".join([\n",
    "        row[\"Title\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip(),\n",
    "        row[\"Intro Text\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip(),\n",
    "        row[\"Article Text\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    ]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "output_path = csv_path.parent / \"data.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in combined_lines:\n",
    "        f.write(line + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
