{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6eb4dac-d119-4a90-a99c-487561a27a04",
   "metadata": {},
   "source": [
    "# Data Collection and Preparation\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook handles the first stage of the pipeline: collecting domain-specific textual data for use in a Named Entity Recognition (NER) system for environmental science. A diverse and well-curated corpus is critical for downstream performance, especially when the goal is to identify entities like species names, habitats, pollutants, processes, and measurements from unstructured text.\n",
    "\n",
    "The dataset must be large enough to cover a variety of sentence structures, vocabulary styles, and topic domains. Since environmental language appears in both scientific and non-scientific formats, the collected data should reflect this range.\n",
    "\n",
    "Data is obtained using a combination of:\n",
    "\n",
    "* **Web scraping**: used where structured access to public datasets exists.\n",
    "* **Manual export**: used for sources with downloadable content (e.g. scientific abstracts).\n",
    "* **Programmatic parsing**: used to extract text from complex formats like PDFs, DOCX files, and zipped document archives.\n",
    "\n",
    "At this stage, no cleaning, annotation, or sentence segmentation is performed. The goal is to preserve the original data as faithfully as possible, and only organise it in a reproducible format for later preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bf57f-0b03-499d-91e5-5d3aca389f31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Data Collection Overview\n",
    "To support robust training of a domain-specific NER model, data was collected from multiple environmental sources. Each source offers different linguistic characteristics and entity coverage, improving the model’s ability to generalise to varied real-world texts. The three core sources are:\n",
    "\n",
    "* **UKCEH Catalogue**: A structured data archive containing environmental metadata and reports in the form of supporting documents (PDFs, DOCX files, etc.). These are scraped directly from the UKCEH online catalogue.\n",
    "* **PubMed Abstracts**: Scientific paper abstracts retrieved from PubMed, grouped by category (e.g. habitat, taxonomy, pollutants). These abstracts reflect formal scientific language and frequently include complex environmental terms.\n",
    "* **Environmental News Articles**: News-style articles exported from a Kaggle dataset, representing more informal, public-facing language and terminology.\n",
    "\n",
    "\n",
    "Each of these sources contributes to one or more of the five key NER categories: TAXONOMY, HABITAT, ENV_PROCESS, POLLUTANT, and MEASUREMENT. By combining these sources, the resulting corpus is better suited to capture both scientific and layperson contexts.\n",
    "\n",
    "The collected data is stored in the `data/raw_data/` directory, with one subfolder per source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8b5ab-eb12-4b55-b78a-4ac7d00f7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set up base directory for raw data\n",
    "BASE_DIR = Path(\"..\") / \"data\" / \"raw_data\"\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fbcd0-961c-4f00-bf55-110a80778789",
   "metadata": {},
   "source": [
    "### 2.1. UKCEH Data Collection\n",
    "The UK Centre for Ecology & Hydrology (UKCEH) provides an online catalogue that publishes environmental datasets across various domains such as land use, biodiversity, and water quality. Each dataset page includes a title, description, and often a ZIP archive containing supporting documents. These documents may include project reports, data collection methodologies, and technical notes.\n",
    "\n",
    "The goal here is to extract both the structured metadata (titles and descriptions) and unstructured text content from supporting files. This data will later be used for vocabulary creation and entity recognition.\n",
    "\n",
    "To collect this data, a web scraping approach is used. For each catalogue page, a script identifies all dataset links, then visits each one to extract available metadata. If a downloadable ZIP is found, it is retrieved and unpacked so that the contents (PDFs, DOCX, TXT files, etc.) can be converted into plain text. This makes it possible to mine large amounts of raw language data for further annotation and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e4f3a-3bce-4daa-b5f3-89d47d22d637",
   "metadata": {},
   "source": [
    "#### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb92de5-6e6a-4049-9e46-859ffb47c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import tempfile\n",
    "import zipfile\n",
    "import re\n",
    "import unicodedata\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from docx import Document\n",
    "from pdfminer.high_level import extract_text as extract_pdf_text\n",
    "import filetype\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Target directory for UKCEH data\n",
    "UKCEH_DIR = BASE_DIR / \"ukceh\"\n",
    "UKCEH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE = UKCEH_DIR / \"ukceh_data.txt\"\n",
    "\n",
    "# Base URL for CEH catalogue\n",
    "BASE_URL = \"https://catalogue.ceh.ac.uk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97983970-a1b6-4114-a924-0aaf01510b45",
   "metadata": {},
   "source": [
    "#### Retrieving Dataset Links\n",
    "This function loads a single page of the UKCEH catalogue and returns all dataset URLs listed on that page. Selenium is used here to dynamically render content that would otherwise be hidden from static scraping tools like requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e645f791-05df-46ad-81e7-fa6149e09f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(page_num):\n",
    "    url = f\"{BASE_URL}/?page={page_num}\"\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    results_list = soup.find(\"div\", class_=\"results__list\")\n",
    "    if not results_list:\n",
    "        return []\n",
    "\n",
    "    dataset_links = [\n",
    "        BASE_URL + a['href']\n",
    "        for a in results_list.find_all(\"a\", href=True)\n",
    "        if a['href'].startswith(\"/documents/\")\n",
    "    ]\n",
    "    return dataset_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15735613-1706-4243-98a9-912ab9c0b479",
   "metadata": {},
   "source": [
    "#### Extracting Dataset Metadata\n",
    "Once the dataset URLs have been collected, the next step is to visit each individual dataset page and extract three core elements: the dataset title, its summary description, and the link to any downloadable supporting documents. These documents often contain the actual scientific content of interest — e.g. technical reports, sampling protocols, or field notes.\n",
    "\n",
    "All of this information is pulled directly from the HTML of the dataset detail page using BeautifulSoup. If a supporting ZIP file is found, its link is returned so it can be downloaded later. This is important because, while the description gives us a short summary, the real value is often inside those attached files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15c0de1e-430d-4c1a-8ee4-6ee43a2abdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_info(dataset_url):\n",
    "    response = requests.get(dataset_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True)\n",
    "    desc_div = soup.find(\"div\", class_=\"description-text\")\n",
    "    description = desc_div.get_text(strip=True) if desc_div else \"\"\n",
    "\n",
    "    supporting_tag = soup.find(\"a\", class_=\"btn btn-access\")\n",
    "    supporting_link = supporting_tag['href'] if supporting_tag else None\n",
    "\n",
    "    return title, description, supporting_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d10ef0-fd8a-4351-87bd-ed840a2fda9a",
   "metadata": {},
   "source": [
    "#### Extracting Text from Individual Files\n",
    "The supporting documents come in different formats, mostly pdf, docx, and txt. The aim here is to read and convert them into plain text wherever possible. For PDFs, we use a two-step fallback system:\n",
    "\n",
    "* First try a text-based extractor (pdfminer).\n",
    "* If that fails (which it does often, because many PDFs are actually scanned images), then use OCR with pytesseract and PyMuPDF to extract image-based text.\n",
    "\n",
    "This fallback was added after early runs of the scraper showed zero content being extracted from several files, which turned out to be scanned PDFs. Without OCR, those would be unusable. DOCX and TXT files are handled more straightforwardly.\n",
    "\n",
    "Files that can’t be parsed or yield no text are silently skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47f42d29-f12a-445a-b5c6-9c0af33e8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_text(file_path):\n",
    "    try:\n",
    "        kind = filetype.guess(file_path)\n",
    "        ext = kind.extension if kind else os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "        if ext == \"pdf\":\n",
    "            with contextlib.redirect_stderr(io.StringIO()):\n",
    "                text = extract_pdf_text(file_path).replace(\"\\n\", \" \").strip()\n",
    "            \n",
    "            if text:\n",
    "                return text\n",
    "            \n",
    "            text_parts = []\n",
    "            doc = fitz.open(file_path)\n",
    "            for page in doc:\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                ocr_text = pytesseract.image_to_string(img).strip()\n",
    "                if ocr_text:\n",
    "                    text_parts.append(ocr_text.replace(\"\\n\", \" \"))\n",
    "            return \" \".join(text_parts).strip()\n",
    "\n",
    "        elif ext == \"docx\":\n",
    "            doc = Document(file_path)\n",
    "            return \" \".join(p.text for p in doc.paragraphs).strip()\n",
    "\n",
    "        elif ext == \"txt\":\n",
    "            with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                return f.read().replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34335f6d-210c-492b-8e5d-01017b457d08",
   "metadata": {},
   "source": [
    "#### Downloading and Reading ZIP Archives\n",
    "Each dataset’s supporting documents are stored inside a downloadable ZIP file. Once a link is found, the ZIP is downloaded into a temporary folder, extracted, and all valid text files are parsed using the logic above.\n",
    "\n",
    "This was necessary because the CEH catalogue doesn't expose individual documents directly — everything is bundled. There were also a few quirks here — like some ZIPs containing metadata files (json, html, rtf) that weren’t useful for this task. These were filtered out during processing, and only useful content was included.\n",
    "\n",
    "In the end, the full dataset (title + description + extracted content from supporting files) is written to a single output file, one line per dataset. This makes it easier to process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a493aadd-e5b3-4814-81b6-8802fa757634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_zip(zip_url, dataset_id):\n",
    "    try:\n",
    "        response = requests.get(zip_url)\n",
    "        if not response.ok:\n",
    "            \n",
    "            return \"\"\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            zip_path = os.path.join(tmpdir, f\"{dataset_id}.zip\")\n",
    "            with open(zip_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdir)\n",
    "\n",
    "            text_chunks = []\n",
    "            for root, _, files in os.walk(tmpdir):\n",
    "                for name in files:\n",
    "                    file_path = os.path.join(root, name)\n",
    "                    try:\n",
    "                        text = read_file_text(file_path)\n",
    "                        if text:\n",
    "                            text_chunks.append(text)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read file {name}: {e}\")\n",
    "\n",
    "            return \" \".join(text_chunks).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while processing zip: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e11a1-578f-4078-bf72-324f96f7e1c1",
   "metadata": {},
   "source": [
    "#### Writing Output to File\n",
    "Each dataset is saved as a single line in a raw text file. This includes the title, description, and any extracted content from supporting documents. The three fields are separated by a pipe (|) character for clarity.\n",
    "\n",
    "At this point, no preprocessing or text cleaning is applied — this is intentional. The idea is to preserve the raw source content in full and defer any normalisation or filtering to a later preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b828b2b-081a-411c-97c7-8229bbf00b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(title, url, description, extracted_text, output_file):\n",
    "    line = f\"{title} | {description} | {extracted_text}\".strip()\n",
    "    \n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acead6-def8-48e4-a843-aaf404a69b34",
   "metadata": {},
   "source": [
    "#### Scraping All Pages\n",
    "This loop goes through every page of the CEH catalogue (1 to 114) and processes each dataset on the page. For each entry, it:\n",
    "\n",
    "1. Extracts metadata (title, description, document link),\n",
    "2. Downloads and extracts any ZIP content,\n",
    "3. Saves everything into a single output file (ukceh_data.txt).\n",
    "\n",
    "If a page has no results, the loop exits early. Any datasets that fail (e.g. broken links, unreadable ZIPs) are skipped with an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d644c06-6acf-4887-86e9-64aff59c2287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "Scraping page 22\n",
      "Scraping page 23\n",
      "Scraping page 24\n",
      "Scraping page 25\n",
      "Scraping page 26\n",
      "Scraping page 27\n",
      "Scraping page 28\n",
      "Scraping page 29\n",
      "Scraping page 30\n",
      "Scraping page 31\n",
      "Scraping page 32\n",
      "Scraping page 33\n",
      "Scraping page 34\n",
      "Scraping page 35\n",
      "Scraping page 36\n",
      "Exception while processing zip: Response ended prematurely\n",
      "Scraping page 37\n",
      "Scraping page 38\n",
      "Scraping page 39\n",
      "Scraping page 40\n",
      "Scraping page 41\n",
      "Scraping page 42\n",
      "Scraping page 43\n",
      "Scraping page 44\n",
      "Scraping page 45\n",
      "Scraping page 46\n",
      "Scraping page 47\n",
      "Scraping page 48\n",
      "Scraping page 49\n",
      "Scraping page 50\n",
      "Scraping page 51\n",
      "Scraping page 52\n",
      "Scraping page 53\n",
      "Scraping page 54\n",
      "Scraping page 55\n",
      "Scraping page 56\n",
      "Scraping page 57\n",
      "Scraping page 58\n",
      "Scraping page 59\n",
      "Scraping page 60\n",
      "Scraping page 61\n",
      "Scraping page 62\n",
      "Scraping page 63\n",
      "Scraping page 64\n",
      "Scraping page 65\n",
      "Scraping page 66\n",
      "Scraping page 67\n",
      "Scraping page 68\n",
      "Scraping page 69\n",
      "Scraping page 70\n",
      "Scraping page 71\n",
      "Scraping page 72\n",
      "Scraping page 73\n",
      "Scraping page 74\n",
      "Scraping page 75\n",
      "Scraping page 76\n",
      "Scraping page 77\n",
      "Scraping page 78\n",
      "Scraping page 79\n",
      "Scraping page 80\n",
      "Scraping page 81\n",
      "Scraping page 82\n",
      "Scraping page 83\n",
      "Scraping page 84\n",
      "Scraping page 85\n",
      "Scraping page 86\n",
      "Scraping page 87\n",
      "Scraping page 88\n",
      "Scraping page 89\n",
      "Scraping page 90\n",
      "Scraping page 91\n",
      "Scraping page 92\n",
      "Scraping page 93\n",
      "Scraping page 94\n",
      "Scraping page 95\n",
      "Scraping page 96\n",
      "Scraping page 97\n",
      "Scraping page 98\n",
      "Scraping page 99\n",
      "Scraping page 100\n",
      "Scraping page 101\n",
      "Scraping page 102\n",
      "Scraping page 103\n",
      "Scraping page 104\n",
      "Scraping page 105\n",
      "Scraping page 106\n",
      "Scraping page 107\n",
      "Scraping page 108\n",
      "Scraping page 109\n",
      "Scraping page 110\n",
      "Scraping page 111\n",
      "Scraping page 112\n",
      "Scraping page 113\n",
      "Scraping page 114\n"
     ]
    }
   ],
   "source": [
    "for page in range(1, 115):\n",
    "    print(f\"Scraping page {page}\")\n",
    "    urls = get_links_from_page(page)\n",
    "\n",
    "    if not urls:\n",
    "        print(f\"No results on page {page}\")\n",
    "        break\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            title, desc, zip_link = get_dataset_info(url)\n",
    "            dataset_id = url.split(\"/\")[-1]\n",
    "            extracted = \"\"\n",
    "\n",
    "            if zip_link:\n",
    "                extracted = extract_text_from_zip(zip_link, dataset_id)\n",
    "\n",
    "            write_to_file(title, url, desc, extracted, OUTPUT_FILE)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on dataset: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205b948-7dcf-4e6a-9f65-07022904442a",
   "metadata": {},
   "source": [
    "### 2.2. Environment News Article (Kaggle)\n",
    "\n",
    "The second source comes from a Kaggle dataset of approximately 30,000 environment-related news articles originally published by The Guardian. The dataset includes metadata and full text for each article. It is publicly available as a CSV file, downloadable from:\n",
    "\n",
    "https://www.kaggle.com/datasets/beridzeg45/guardian-environment-related-news\n",
    "\n",
    "The CSV is manually downloaded and placed inside the working directory. Since this notebook focuses on collecting all raw text into unified .txt files, the CSV is loaded and the relevant columns are concatenated into a single line per article. Each line contains the title, introduction, and article body, separated by a pipe (|).\n",
    "\n",
    "This text format aligns with how other sources are being collected and makes downstream processing (like annotation or splitting) simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de1a55-ac46-44a3-91c2-873528ff1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_NEWS_DIR = BASE_DIR / \"environment_news\"\n",
    "ENV_NEWS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = ENV_NEWS_DIR / \"env_news_data.csv\"\n",
    "output_path = ENV_NEWS_DIR / \"env_news_data.txt\"\n",
    "\n",
    "df = pd.read_csv(csv_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Clean and combine Title, Authors, and Article Text columns\n",
    "combined_lines = df[[\"Title\", \"Intro Text\", \"Article Text\"]].dropna().astype(str).apply(\n",
    "    lambda row: \" | \".join([\n",
    "        row[\"Title\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip(),\n",
    "        row[\"Intro Text\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip(),\n",
    "        row[\"Article Text\"].replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    ]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "output_path = csv_path.parent / \"data.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in combined_lines:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c17aca-7801-412a-aaab-4e2a2b514a22",
   "metadata": {},
   "source": [
    "### 2.3. PubMed Abstracts\n",
    "PubMed provides access to millions of scientific abstracts in medicine and life sciences, many of which intersect with environmental topics. While its full-text articles are often behind paywalls, abstracts are openly accessible and contain rich descriptions of research objectives, findings, and terminology.\n",
    "\n",
    "To supplement our dataset with high-quality scientific language, we downloaded abstracts related to environmental science concepts. For each of our core entity categories (e.g. TAXONOMY, POLLUTANT, HABITAT), we used broad search terms like biodiversity, pollutants, and climate to retrieve results directly through PubMed’s web interface. Up to 10,000 abstracts were downloaded per category using the built-in “Send to File” feature.\n",
    "\n",
    "Each export was saved as a plain .txt file under: `data/raw_data/pubmed`\n",
    "\n",
    "Each line in the file corresponds to a single abstract, and no further metadata (such as title or authors) was included. No code was used for collection — this step was completed manually to comply with PubMed’s terms.\n",
    "\n",
    "While programmatic access via the PubMed API (Entrez) is possible, it comes with rate limits and technical constraints that made bulk collection impractical for this project. In contrast, abstracts are freely accessible for research and can be downloaded in bulk via the UI. As a result, our choice to use abstracts is both practical and legally sound.\n",
    "\n",
    "It is also worth noting that including additional data that may not contain annotated entities is not a concern at this stage. During the annotation phase, sentences that do not contain any known entities will be excluded from the final training data. This allows us to prioritise breadth of coverage and domain relevance during data collection without needing perfect alignment up front."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad602b-117e-41a9-926c-3f568eaf5de2",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Raw Text\n",
    "Raw textual data often contains non-informative lines, broken encodings, formatting artifacts, and low-content text (e.g. bullets or section headers like “Table 1”). While it's important not to over-clean, a lightweight standardisation step improves downstream tokenisation and annotation.\n",
    "\n",
    "This stage does not remove domain-specific content or alter sentence boundaries. Instead, it:\n",
    "\n",
    "* Normalises Unicode characters\n",
    "* Removes broken characters, bullet symbols, and URLs\n",
    "* Filters out junk lines (e.g. lines with fewer than three words)\n",
    "\n",
    "PubMed abstracts require one additional fix: merging broken lines caused by formatting errors during export. We identify blocks separated by blank lines and join them into single coherent abstracts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30319d-0c30-4d26-9579-c170c8a6bc1d",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning and Merging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "403aa82b-286c-4c3a-bec6-73be3b693eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "RAW_BASE = Path(\"../data/raw_data\")\n",
    "CLEAN_BASE = Path(\"../data/processed\")\n",
    "\n",
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None  # skip empty lines\n",
    "\n",
    "    line = unicodedata.normalize(\"NFKD\", line)  # remove weird Unicode ligatures\n",
    "    line = re.sub(r\"[•▪●‣–—·]\", \" \", line)  # bullet points\n",
    "    line = re.sub(r\"\\.{3,}\", \"...\", line)  # collapse dot chains\n",
    "    line = re.sub(r\"https?://\\S+\", \"\", line)  # remove URLs\n",
    "    line = re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \"\", line)  # remove control chars\n",
    "    line = re.sub(r\"\\s{2,}\", \" \", line)  # extra spaces\n",
    "\n",
    "    if len(line.split()) < 3:  # junk (like \"Table 1\", \"Appendix\", etc)\n",
    "        return None\n",
    "\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d68da6c0-9c1b-4ddb-a7df-1271fd105823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pubmed_lines(input_path):\n",
    "    \"\"\"Merge broken abstract lines using blank lines as separators.\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "\n",
    "    abstracts = []\n",
    "    current = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            current.append(line)\n",
    "        else:\n",
    "            if current:\n",
    "                abstracts.append(\" \".join(current))\n",
    "                current = []\n",
    "    if current:\n",
    "        abstracts.append(\" \".join(current))\n",
    "    return abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7f8da90-ee6e-4992-8050-1439e6536bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(input_path, output_path, is_pubmed=False):\n",
    "    if is_pubmed:\n",
    "        raw_lines = merge_pubmed_lines(input_path)\n",
    "    else:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "            raw_lines = infile.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in raw_lines:\n",
    "        cleaned = clean_line(line)\n",
    "        if cleaned:\n",
    "            cleaned_lines.append(cleaned)\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in cleaned_lines:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "    print(f\"Saved cleaned file: {output_path.name} ({len(cleaned_lines)} lines)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9a1c7e3-5c0f-45de-9c44-15126b280e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file: ukceh_data.txt (3948 lines)\n",
      "Saved cleaned file: env_news_data.txt (28669 lines)\n",
      "Saved cleaned file: abstract-habitat.txt (136427 lines)\n",
      "Saved cleaned file: abstract-env_process.txt (66957 lines)\n",
      "Saved cleaned file: abstract-environment.txt (66376 lines)\n",
      "Saved cleaned file: abstract-taxonomy.txt (45741 lines)\n",
      "Saved cleaned file: abstract-pollutants.txt (61937 lines)\n",
      "Saved cleaned file: abstract-measurement.txt (58694 lines)\n"
     ]
    }
   ],
   "source": [
    "for source in [\"ukceh\", \"environment_news\", \"pubmed\"]:\n",
    "    raw_dir = RAW_BASE / source\n",
    "    clean_dir = CLEAN_BASE / source\n",
    "    clean_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for txt_file in raw_dir.glob(\"*.txt\"):\n",
    "        is_pubmed = (source == \"pubmed\")\n",
    "        clean_file(txt_file, clean_dir / txt_file.name, is_pubmed=is_pubmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e88232-b2e3-4612-8c65-c60791bcdce1",
   "metadata": {},
   "source": [
    "## 3. Sentence segmentation\n",
    "\n",
    "After cleaning the raw documents, the next step involves sentence segmentation. This is necessary for two main reasons:\n",
    "\n",
    "1. Named Entity Recognition (NER) models work best when input is divided into grammatically coherent units (sentences).\n",
    "2. Many downstream annotation and evaluation tools expect one sentence per line.\n",
    "\n",
    "We use `spaCy` for this step, which applies robust rule-based and statistical models for accurate sentence boundary detection. The segmented sentences are stored under `../data/sentences/`, preserving the original filenames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e4d3fc5-a42e-4a98-a3d7-e94c2ac30dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: ukceh_data.txt\n",
      "Done: env_news_data.txt\n",
      "Done: abstract-habitat.txt\n",
      "Done: abstract-env_process.txt\n",
      "Done: abstract-environment.txt\n",
      "Done: abstract-taxonomy.txt\n",
      "Done: abstract-pollutants.txt\n",
      "Done: abstract-measurement.txt\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Load model and increase max length (safely)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2_000_000  # You can increase more if needed\n",
    "\n",
    "INPUT_DIR = Path(\"../data/processed\")\n",
    "OUTPUT_DIR = Path(\"../data/sentences\")\n",
    "\n",
    "def segment_sentences_streaming(input_path, output_path):\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for block in infile:\n",
    "            block = block.strip()\n",
    "            if not block:\n",
    "                continue\n",
    "\n",
    "            doc = nlp(block)\n",
    "            for sent in doc.sents:\n",
    "                sentence = sent.text.strip()\n",
    "                if sentence:\n",
    "                    outfile.write(sentence + \"\\n\")\n",
    "\n",
    "    print(f\"Done: {output_path.name}\")\n",
    "\n",
    "# Apply to all files\n",
    "for source in [\"ukceh\", \"environment_news\", \"pubmed\"]:\n",
    "    input_folder = INPUT_DIR / source\n",
    "    output_folder = OUTPUT_DIR / source\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in input_folder.glob(\"*.txt\"):\n",
    "        output_path = output_folder / file_path.name\n",
    "        segment_sentences_streaming(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019732f0-ef5b-4c0f-8a49-c160e0609177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d55458-253d-49dc-905d-2ff3696a4998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b1b84-a86e-4215-933d-51af100a451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94215c28-3f16-4363-a7e9-33c6b4a05764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6405611-1675-4236-b810-f8119ce6df78",
   "metadata": {},
   "source": [
    "# This is for processing news data so include this somewhere in the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
